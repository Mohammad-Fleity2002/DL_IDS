{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8089266,"sourceType":"datasetVersion","datasetId":4775518},{"sourceId":8089281,"sourceType":"datasetVersion","datasetId":4775527}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader,Dataset, TensorDataset, WeightedRandomSampler, RandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nimport wandb\nfrom omegaconf import OmegaConf\nimport os\nimport warnings\nfrom kaggle_secrets import UserSecretsClient\n\nwarnings.filterwarnings('ignore')\n\n# Optimized Configuration\nconfig = OmegaConf.create({\n    \"wandb\": {\n        \"project\": \"DL-NIDS-2--cic-ids-2017\",\n        # \"project\": \"DL-NIDS-2--cic-ton-iot\",\n        \"entity\": \"mohammad-fleity-lebanese-university\",\n        # \"tags\": [\"BILSTM\", \"CIC-TON-IOT\", \"PyTorch\"],\n        \"tags\": [\"BILSTM\", \"CIC-IDS-2017\", \"PyTorch\"],\n        \"notes\": \"BILSTM for network intrusion detection\"\n    },\n    \"model\": {\n        # \"hidden_size\": 80,          # Increased capacity\n        \"hidden_size\": 128,          # Increased capacity\n        \"num_layers\": 2,             # Deeper network\n        \"dropout\": 0.4,              # Stronger regularization\n        # \"dense_units\": [100, 50],    # Better feature extraction\n        \"dense_units\": [128, 64],    # Better feature extraction\n        \"learning_rate\": 0.0001,     # Slower learning\n        \"weight_decay\": 1e-4         # Stronger L2 regularization\n    },\n    \"training\": {\n        # \"sequence_length\": 3,        # Longer temporal context\n        \"sequence_length\": 5,        # Longer temporal context\n        # \"batch_size\": 64,           # Larger batches\n        \"batch_size\": 128,           # Larger batches\n        \"max_epochs\": 7,            # More training time\n        # \"max_epochs\": 25,            # More training time\n        \"early_stopping_patience\": 7,# More patience\n        \"oversample\": True,          # Class balancing\n        \"gpus\": 1 if torch.cuda.is_available() else 0,\n        \"train_size\": 0.7,           # Proper train/val split\n        \"val_size\": 0.15             # 70/15/15 split\n    },\n    \"data\": {\n        \"raw\": \"cic_ids_2017.parquet\",\n        \"num_workers\": 4\n        # \"raw\": \"cic_ton_iot.parquet\",\n        # \"num_workers\": 3\n    }\n})\n\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, data, sequence_length, target_idx=None):\n        \"\"\"\n        Dataset for time series data that creates sequences on-the-fly.\n \n        Args:\n            data: Input data tensor of shape (n_samples, n_features)\n            sequence_length: Length of sequences to create\n            target_idx: Optional tensor of target indices. If None, uses the last position in sequence\n        \"\"\"\n        self.data = data\n        self.sequence_length = sequence_length\n        self.target_idx = target_idx\n \n    def __len__(self):\n        return len(self.data)\n \n    def __getitem__(self, idx):\n        # Calculate start index for the sequence\n        start_idx = max(0, idx - self.sequence_length + 1)\n        \n        # Get the sequence\n        sequence = self.data[start_idx:idx + 1]\n        \n        # Ensure sequence is 2D [seq_len, features]\n        if sequence.dim() == 1:\n            sequence = sequence.unsqueeze(0)  # Add sequence length dimension if missing\n        \n        # Pad the beginning if needed\n        if len(sequence) < self.sequence_length:\n            padding = torch.zeros(self.sequence_length - len(sequence), sequence.shape[1])\n            sequence = torch.cat([padding, sequence], dim=0)\n        \n        # Get target\n        target = self.target_idx[idx] if self.target_idx is not None else -1\n        \n        return sequence, target\n\n\n\nclass BiLSTMModel(pl.LightningModule):\n    def __init__(self, input_size, num_classes, config):\n        super().__init__()\n        self.save_hyperparameters()\n        hidden_size = config.model.hidden_size\n        dropout = config.model.dropout\n\n        # Bidirectional LSTM\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=config.model.num_layers,\n            batch_first=True,\n            dropout=dropout if config.model.num_layers > 1 else 0,\n            bidirectional=True  # <--- Bidirectional LSTM\n        )\n\n        # Double output size due to bidirectional\n        self.lstm_ln = nn.LayerNorm(hidden_size * 2)\n\n        self.dense = nn.Sequential(\n            nn.Linear(hidden_size * 2, config.model.dense_units[0]),\n            nn.LayerNorm(config.model.dense_units[0]),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(config.model.dense_units[0], config.model.dense_units[1]),\n            nn.LayerNorm(config.model.dense_units[1]),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n\n        self.output = nn.Linear(config.model.dense_units[1], num_classes)\n        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        lstm_out = lstm_out[:, -1, :]  # Take last time step (already bidirectional)\n        lstm_out = self.lstm_ln(lstm_out)\n        features = self.dense(lstm_out)\n        return self.output(features)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('train_loss_epoch', loss, prog_bar=True)\n        self.log('train_acc_epoch', acc * 100, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', acc * 100, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('test_loss', loss)\n        self.log('test_acc', acc * 100)\n\n    def configure_optimizers(self):\n        return optim.AdamW(\n            self.parameters(),\n            lr=self.hparams.config.model.learning_rate,\n            weight_decay=self.hparams.config.model.weight_decay\n        )\n       #    nn.LayerNorm(config.model.dense_units[0]),\n       # self.log('train_acc_epoch', acc*100, prog_bar=True)\n\nclass NIDSDataModule(pl.LightningDataModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.batch_size = config.training.batch_size\n        self.sequence_length = config.training.sequence_length\n        self.num_workers = config.data.num_workers\n        self.oversample = config.training.oversample\n        self.alpha = 0.5\n\n    def prepare_data(self):\n        # df = pd.read_parquet(os.path.join('/kaggle/input/cic-ton-iot-parquet', self.config.data.raw))\n        df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n    \n        # Clean data\n        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df.dropna(inplace=True)\n        df.drop_duplicates(inplace=True)\n    \n        # Identify non-numeric columns\n        self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', \n                                 'Src Port', 'Attack', 'Dst IP', 'Dst Port', 'Protocol']\n        self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n    \n        # Encode labels\n        self.label_encoder = LabelEncoder()\n        df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n        self.classes = self.label_encoder.classes_\n    \n        # Initialize scaler\n        self.scaler = StandardScaler()\n    \n        # Train/Val/Test split\n        train_df, test_df = train_test_split(\n            df,\n            # test_size=1 - self.config.training.train_size,\n            test_size=0.3,\n            random_state=42,\n            stratify=df['Label_Num']\n        )\n        val_df, test_df = train_test_split(\n            test_df,\n            test_size=0.4,\n            # test_size=0.5,\n            random_state=42,\n            stratify=test_df['Label_Num']\n        )\n        print(len(train_df))\n        print(len(test_df))\n        # train_max_rows = 300000\n        # if len(train_df) > train_max_rows:\n        #     train_df = train_df.sample(n=train_max_rows, random_state=42)\n        # val_max_rows = 45000\n        # if len(val_df) > val_max_rows:\n        #     val_df = val_df.sample(n=val_max_rows, random_state=42)\n\n        # Use `fit=True` only for training data\n        self.X_train, self.y_train = self._prepare_features(train_df, fit=True)\n        self.X_val, self.y_val = self._prepare_features(val_df, fit=False)\n        self.X_test, self.y_test = self._prepare_features(test_df, fit=False)\n\n        \n    def _prepare_features(self, df, fit=False):\n        X = df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n        y = df['Label_Num']\n        if fit:\n            X = self.scaler.fit_transform(X)\n        else:\n            X = self.scaler.transform(X)\n        # return self.create_sequences(X, y)\n        return X, y\n\n    def create_sequences(self, X, y):\n        sequences = []\n        labels = []\n        for i in range(len(X) - self.sequence_length):\n            sequences.append(X[i:i+self.sequence_length])\n            labels.append(y.iloc[i+self.sequence_length-1])\n        return np.array(sequences), np.array(labels)\n    \n    # def setup(self, stage=None):\n    #     # self.scaler = StandardScaler()\n    #     # self.scaler.fit(self.X_train.reshape(-1, self.X_train.shape[-1]))\n        \n    #     # self.X_train = self.scaler.transform(self.X_train.reshape(-1, self.X_train.shape[-1])).reshape(self.X_train.shape)\n    #     # self.X_val = self.scaler.transform(self.X_val.reshape(-1, self.X_val.shape[-1])).reshape(self.X_val.shape)\n    #     # self.X_test = self.scaler.transform(self.X_test.reshape(-1, self.X_test.shape[-1])).reshape(self.X_test.shape)\n        \n    #     self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n    #     self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n    #     self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n    #     print(\"the model will be trained on: \",len(self.train_dataset),\" samples.\")\n    #     print(\"the model will be validated on: \",len(self.val_dataset),\" samples.\")\n    #     print(\"the model will be tested on: \",len(self.test_dataset),\" samples.\")\n\n    def setup(self, stage=None):\n        self.scaler = StandardScaler()\n        self.scaler.fit(self.X_train.reshape(-1, self.X_train.shape[-1]))\n        \n        self.X_train = self.scaler.transform(self.X_train.reshape(-1, self.X_train.shape[-1])).reshape(self.X_train.shape)\n        self.X_val = self.scaler.transform(self.X_val.reshape(-1, self.X_val.shape[-1])).reshape(self.X_val.shape)\n        self.X_test = self.scaler.transform(self.X_test.reshape(-1, self.X_test.shape[-1])).reshape(self.X_test.shape)\n        \n        # self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n        # self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n        # self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n\n        self.train_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_train),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_train.values)\n        )\n        \n        self.val_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_val),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_val.values)\n        )\n        \n        self.test_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_test),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_test.values)\n        )\n\n        print(\"the model will be trained on: \",len(self.train_dataset),\" samples.\")\n        print(\"the model will be validated on: \",len(self.val_dataset),\" samples.\")\n        print(\"the model will be tested on: \",len(self.test_dataset),\" samples.\")\n    \n    def train_dataloader(self):\n        if self.oversample:\n            class_counts = np.bincount(self.y_train)\n            weights = 1. / class_counts[self.y_train]\n            sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n        else:\n            sampler = RandomSampler(self.train_dataset)\n            \n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            sampler=sampler,\n            num_workers=self.num_workers,\n            persistent_workers=True,\n            pin_memory=True\n        )\n    \n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n\n\ndef init_wandb():\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"mohammad_wandb_secret\")\n    wandb.login(key=wandb_api_key)\n    \n    # Initialize the run first\n    run = wandb.init(\n        project=config.wandb.project,\n        entity=config.wandb.entity,\n        tags=config.wandb.tags,\n        notes=config.wandb.notes,\n        config={\n            \"input_size\": None,  # Will be updated later\n            \"num_classes\": None,\n            \"sequence_length\": config.training.sequence_length,\n            \"train_samples\": None,\n            \"test_samples\": None,\n            \"model_config\": dict(config.model),\n            \"training_config\": dict(config.training)\n        }\n    )\n    \n    # Then create the logger\n    wandb_logger = WandbLogger(\n        experiment=run,\n        log_model='all'\n    )\n    \n    return wandb_logger, run\n\ndef main():\n    # Initialize wandb - now returns both logger and run\n    wandb_logger, run = init_wandb()\n    \n    # Initialize data module\n    data_module = NIDSDataModule(config)\n    data_module.prepare_data()\n    data_module.setup()\n    \n    # Get input size from data\n    sample_x, _ = next(iter(data_module.train_dataloader()))\n    input_size = sample_x.shape[2]\n    num_classes = len(data_module.classes)\n    \n    # Update the config with actual values\n    run.config.update({\n        \"input_size\": input_size,\n        \"num_classes\": num_classes,\n        \"train_samples\": len(data_module.train_dataset),\n        \"test_samples\": len(data_module.test_dataset)\n    })\n    \n    # model = LSTMModel(input_size, num_classes, config)\n    \n    model = BiLSTMModel(input_size, num_classes, config)\n    \n    # Callbacks\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=config.training.early_stopping_patience,\n        mode='min'\n    )\n    \n    # checkpoint_callback = ModelCheckpoint(\n    #     monitor='val_f1',  # Now monitoring F1 score\n    #     mode='max',\n    #     save_top_k=1,\n    #     dirpath='checkpoints',\n    #     filename='best_model'\n    # )\n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_acc',\n        mode='max',\n        save_top_k=1,\n        dirpath='checkpoints',\n        filename='best_model'\n    )\n \n    # Initialize trainer\n    trainer = pl.Trainer(\n        precision=16,\n        logger=wandb_logger,\n        max_epochs=config.training.max_epochs,\n        callbacks=[early_stopping, checkpoint_callback],\n        deterministic=True,\n        gradient_clip_val=1.0,\n        enable_progress_bar=True,\n        log_every_n_steps=1000\n    )\n    \n    # Train model\n    trainer.fit(model, datamodule=data_module)\n    \n    # Test model\n    test_results = trainer.test(model, datamodule=data_module)\n    \n    # Collect all predictions and targets for final evaluation\n    test_loader = data_module.test_dataloader()\n    all_preds = []\n    all_targets = []\n    \n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            x, y = batch\n            y_hat = model(x)\n            preds = torch.argmax(y_hat, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(y.cpu().numpy())\n    \n    # Final metrics calculation\n    test_acc = accuracy_score(all_targets, all_preds)\n    test_f1 = f1_score(all_targets, all_preds, average='weighted')\n    \n    # Log final test metrics\n    wandb.log({\n        'test_acc': test_acc,\n        'test_f1': test_f1,\n        'test_loss': test_results[0]['test_loss']\n    })\n    \n    # Confusion matrix and classification report\n    class_names = data_module.classes.tolist()\n    \n    # Confusion Matrix\n    wandb.log({\n        \"confusion_matrix\": wandb.plot.confusion_matrix(\n            y_true=all_targets,\n            preds=all_preds,\n            class_names=class_names,\n            title=\"Confusion Matrix\"\n        )\n    })\n    \n    # Classification Report\n    report = classification_report(\n        all_targets, all_preds, \n        target_names=class_names,\n        output_dict=True\n    )\n    \n    # Create a wandb Table for the classification report\n    report_table = wandb.Table(columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n    for class_name in class_names:\n        report_table.add_data(\n            class_name,\n            report[class_name][\"precision\"],\n            report[class_name][\"recall\"],\n            report[class_name][\"f1-score\"],\n            report[class_name][\"support\"]\n        )\n    \n    # Add weighted averages\n    report_table.add_data(\n        \"Weighted Avg\",\n        report[\"weighted avg\"][\"precision\"],\n        report[\"weighted avg\"][\"recall\"],\n        report[\"weighted avg\"][\"f1-score\"],\n        report[\"weighted avg\"][\"support\"]\n    )\n    \n    wandb.log({\"classification_report\": report_table})\n    \n    # Finish wandb run\n    wandb.finish()\n    \nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T15:32:37.499664Z","iopub.execute_input":"2025-06-27T15:32:37.499969Z","iopub.status.idle":"2025-06-27T16:06:34.865355Z","shell.execute_reply.started":"2025-06-27T15:32:37.499938Z","shell.execute_reply":"2025-06-27T16:06:34.863873Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohammad-fleity\u001b[0m (\u001b[33mmohammad-fleity-lebanese-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250627_153312-88xrolwk</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/88xrolwk' target=\"_blank\">celestial-universe-82</a></strong> to <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/88xrolwk' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/88xrolwk</a>"},"metadata":{}},{"name":"stdout","text":"1979373\n339322\nthe model will be trained on:  1979373  samples.\nthe model will be validated on:  508982  samples.\nthe model will be tested on:  339322  samples.\n1979373\n339322\nthe model will be trained on:  1979373  samples.\nthe model will be validated on:  508982  samples.\nthe model will be tested on:  339322  samples.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba61212dc81949679f2b84091cb2f5f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba2683a1101b44b6a868b55121fe1174"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cca14fe3c334c6398293ca4744549ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"1979373\n339322\nthe model will be trained on:  1979373  samples.\nthe model will be validated on:  508982  samples.\nthe model will be tested on:  339322  samples.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc523c633dc84f19aab1a3d902c74212"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    96.4989013671875     \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.24974116683006287   \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     96.4989013671875      </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.24974116683006287    </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/441955991.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/441955991.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;31m# Add weighted averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m     report_table.add_data(\n\u001b[0m\u001b[1;32m    496\u001b[0m         \u001b[0;34m\"Weighted Avg\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0mreport\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weighted avg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"precision\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/table.py\u001b[0m in \u001b[0;36madd_data\u001b[0;34m(self, *data)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;31m# Update the table's column types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mresult_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_updated_result_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_column_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/table.py\u001b[0m in \u001b[0;36m_get_updated_result_type\u001b[0;34m(self, row)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0mresult_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincoming_row_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    471\u001b[0m                 \"Data row contained incompatible types:\\n{}\".format(\n\u001b[1;32m    472\u001b[0m                     \u001b[0mcurrent_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincoming_row_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Data row contained incompatible types:\n{'Class': 'Weighted Avg', 'Precision': 0.9691997206973322, 'Recall': 0.9649890074914094, 'F1-Score': 0.9658966198751564, 'Support': 339322} of type {'Class': String, 'Precision': Number, 'Recall': Number, 'F1-Score': Number, 'Support': Number} is not assignable to {'Class': None or Number, 'Precision': None or Number, 'Recall': None or Number, 'F1-Score': None or Number, 'Support': None or Number}\nKey 'Class':\n\tString not assignable to None or Number\n\t\tString not assignable to None\n\tand\n\t\tString not assignable to Number"],"ename":"TypeError","evalue":"Data row contained incompatible types:\n{'Class': 'Weighted Avg', 'Precision': 0.9691997206973322, 'Recall': 0.9649890074914094, 'F1-Score': 0.9658966198751564, 'Support': 339322} of type {'Class': String, 'Precision': Number, 'Recall': Number, 'F1-Score': Number, 'Support': Number} is not assignable to {'Class': None or Number, 'Precision': None or Number, 'Recall': None or Number, 'F1-Score': None or Number, 'Support': None or Number}\nKey 'Class':\n\tString not assignable to None or Number\n\t\tString not assignable to None\n\tand\n\t\tString not assignable to Number","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"<h2>Second Dataset</h2>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader,Dataset ,TensorDataset, WeightedRandomSampler, RandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nimport wandb\nfrom omegaconf import OmegaConf\nimport os\nimport warnings\nfrom kaggle_secrets import UserSecretsClient\n\nwarnings.filterwarnings('ignore')\n\n# Optimized Configuration\nconfig = OmegaConf.create({\n    \"wandb\": {\n        # \"project\": \"DL-NIDS-2--cic-ids-2017\",\n        \"project\": \"DL-NIDS-2--cic-ton-iot\",\n        \"entity\": \"mohammad-fleity-lebanese-university\",\n        \"tags\": [\"BILSTM\", \"CIC-TON-IOT\", \"PyTorch\"],\n        # \"tags\": [\"BILSTM\", \"CIC-IDS-2017\", \"PyTorch\"],\n        \"notes\": \"BILSTM for network intrusion detection\"\n    },\n    \"model\": {\n        # \"hidden_size\": 80,          # Increased capacity\n        \"hidden_size\": 128,          # Increased capacity\n        \"num_layers\": 2,             # Deeper network\n        \"dropout\": 0.4,              # Stronger regularization\n        # \"dense_units\": [100, 50],    # Better feature extraction\n        \"dense_units\": [128, 64],    # Better feature extraction\n        \"learning_rate\": 0.0001,     # Slower learning\n        \"weight_decay\": 1e-4         # Stronger L2 regularization\n    },\n    \"training\": {\n        # \"sequence_length\": 3,        # Longer temporal context\n        \"sequence_length\": 5,        # Longer temporal context\n        # \"batch_size\": 64,           # Larger batches\n        \"batch_size\": 128,           # Larger batches\n        \"max_epochs\": 10,            # More training time\n        # \"max_epochs\": 25,            # More training time\n        \"early_stopping_patience\": 7,# More patience\n        \"oversample\": True,          # Class balancing\n        \"gpus\": 1 if torch.cuda.is_available() else 0,\n        \"train_size\": 0.7,           # Proper train/val split\n        \"val_size\": 0.15             # 70/15/15 split\n    },\n    \"data\": {\n        # \"raw\": \"cic_ids_2017.parquet\",\n        # \"num_workers\": 4\n        \"raw\": \"cic_ton_iot.parquet\",\n        \"num_workers\": 3\n    }\n})\n\n\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, data, sequence_length, target_idx=None):\n        \"\"\"\n        Dataset for time series data that creates sequences on-the-fly.\n \n        Args:\n            data: Input data tensor of shape (n_samples, n_features)\n            sequence_length: Length of sequences to create\n            target_idx: Optional tensor of target indices. If None, uses the last position in sequence\n        \"\"\"\n        self.data = data\n        self.sequence_length = sequence_length\n        self.target_idx = target_idx\n \n    def __len__(self):\n        return len(self.data)\n \n    def __getitem__(self, idx):\n        # Calculate start index for the sequence\n        start_idx = max(0, idx - self.sequence_length + 1)\n        \n        # Get the sequence\n        sequence = self.data[start_idx:idx + 1]\n        \n        # Ensure sequence is 2D [seq_len, features]\n        if sequence.dim() == 1:\n            sequence = sequence.unsqueeze(0)  # Add sequence length dimension if missing\n        \n        # Pad the beginning if needed\n        if len(sequence) < self.sequence_length:\n            padding = torch.zeros(self.sequence_length - len(sequence), sequence.shape[1])\n            sequence = torch.cat([padding, sequence], dim=0)\n        \n        # Get target\n        target = self.target_idx[idx] if self.target_idx is not None else -1\n        \n        return sequence, target\n\nclass BiLSTMModel(pl.LightningModule):\n    def __init__(self, input_size, num_classes, config):\n        super().__init__()\n        self.save_hyperparameters()\n        hidden_size = config.model.hidden_size\n        dropout = config.model.dropout\n\n        # Bidirectional LSTM\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=config.model.num_layers,\n            batch_first=True,\n            dropout=dropout if config.model.num_layers > 1 else 0,\n            bidirectional=True  # <--- Bidirectional LSTM\n        )\n\n        # Double output size due to bidirectional\n        self.lstm_ln = nn.LayerNorm(hidden_size * 2)\n\n        self.dense = nn.Sequential(\n            nn.Linear(hidden_size * 2, config.model.dense_units[0]),\n            nn.LayerNorm(config.model.dense_units[0]),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(config.model.dense_units[0], config.model.dense_units[1]),\n            nn.LayerNorm(config.model.dense_units[1]),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n\n        self.output = nn.Linear(config.model.dense_units[1], num_classes)\n        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        lstm_out = lstm_out[:, -1, :]  # Take last time step (already bidirectional)\n        lstm_out = self.lstm_ln(lstm_out)\n        features = self.dense(lstm_out)\n        return self.output(features)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('train_loss_epoch', loss, prog_bar=True)\n        self.log('train_acc_epoch', acc * 100, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', acc * 100, prog_bar=True)\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('test_loss', loss)\n        self.log('test_acc', acc * 100)\n\n    def configure_optimizers(self):\n        return optim.AdamW(\n            self.parameters(),\n            lr=self.hparams.config.model.learning_rate,\n            weight_decay=self.hparams.config.model.weight_decay\n        )\n\n\n\nclass NIDSDataModule(pl.LightningDataModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.batch_size = config.training.batch_size\n        self.sequence_length = config.training.sequence_length\n        self.num_workers = config.data.num_workers\n        self.oversample = config.training.oversample\n        self.alpha = 0.5\n\n    def prepare_data(self):\n        df = pd.read_parquet(os.path.join('/kaggle/input/cic-ton-iot-parquet', self.config.data.raw))\n        # df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n    \n        # Clean data\n        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df.dropna(inplace=True)\n        df.drop_duplicates(inplace=True)\n    \n        # Identify non-numeric columns\n        self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', \n                                 'Src Port', 'Attack', 'Dst IP', 'Dst Port', 'Protocol']\n        self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n    \n        # Encode labels\n        self.label_encoder = LabelEncoder()\n        df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n        self.classes = self.label_encoder.classes_\n    \n        # Initialize scaler\n        self.scaler = StandardScaler()\n    \n        # Train/Val/Test split\n        train_df, test_df = train_test_split(\n            df,\n            # test_size=1 - self.config.training.train_size,\n            test_size=0.3,\n            random_state=42,\n            stratify=df['Label_Num']\n        )\n        val_df, test_df = train_test_split(\n            test_df,\n            test_size=0.4,\n            # test_size=0.5,\n            random_state=42,\n            stratify=test_df['Label_Num']\n        )\n        # print(\"\"len(train_df))\n        # print(\"\"len(test_df))\n        # train_max_rows = 300000\n        # if len(train_df) > train_max_rows:\n        #     train_df = train_df.sample(n=train_max_rows, random_state=42)\n        # val_max_rows = 45000\n        # if len(val_df) > val_max_rows:\n        #     val_df = val_df.sample(n=val_max_rows, random_state=42)\n\n        # Use `fit=True` only for training data\n        self.X_train, self.y_train = self._prepare_features(train_df, fit=True)\n        self.X_val, self.y_val = self._prepare_features(val_df, fit=False)\n        self.X_test, self.y_test = self._prepare_features(test_df, fit=False)\n\n    \n    \n    def _prepare_features(self, df, fit=False):\n        X = df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n        y = df['Label_Num']\n        if fit:\n            X = self.scaler.fit_transform(X)\n        else:\n            X = self.scaler.transform(X)\n        # return self.create_sequences(X, y)\n        return X, y\n\n    def create_sequences(self, X, y):\n        sequences = []\n        labels = []\n        for i in range(len(X) - self.sequence_length):\n            sequences.append(X[i:i+self.sequence_length])\n            labels.append(y.iloc[i+self.sequence_length-1])\n        return np.array(sequences), np.array(labels)\n    \n    # def setup(self, stage=None):\n    #     # self.scaler = StandardScaler()\n    #     # self.scaler.fit(self.X_train.reshape(-1, self.X_train.shape[-1]))\n        \n    #     # self.X_train = self.scaler.transform(self.X_train.reshape(-1, self.X_train.shape[-1])).reshape(self.X_train.shape)\n    #     # self.X_val = self.scaler.transform(self.X_val.reshape(-1, self.X_val.shape[-1])).reshape(self.X_val.shape)\n    #     # self.X_test = self.scaler.transform(self.X_test.reshape(-1, self.X_test.shape[-1])).reshape(self.X_test.shape)\n        \n    #     self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n    #     self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n    #     self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n    #     print(\"the model will be trained on: \",len(self.train_dataset),\" samples.\")\n    #     print(\"the model will be validated on: \",len(self.val_dataset),\" samples.\")\n    #     print(\"the model will be tested on: \",len(self.test_dataset),\" samples.\")\n    def setup(self, stage=None):\n        self.scaler = StandardScaler()\n        self.scaler.fit(self.X_train.reshape(-1, self.X_train.shape[-1]))\n        \n        self.X_train = self.scaler.transform(self.X_train.reshape(-1, self.X_train.shape[-1])).reshape(self.X_train.shape)\n        self.X_val = self.scaler.transform(self.X_val.reshape(-1, self.X_val.shape[-1])).reshape(self.X_val.shape)\n        self.X_test = self.scaler.transform(self.X_test.reshape(-1, self.X_test.shape[-1])).reshape(self.X_test.shape)\n        \n        # self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n        # self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n        # self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n\n        self.train_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_train),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_train.values)\n        )\n        \n        self.val_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_val),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_val.values)\n        )\n        \n        self.test_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_test),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_test.values)\n        )\n\n        print(\"the model will be trained on: \",len(self.train_dataset),\" samples.\")\n        print(\"the model will be validated on: \",len(self.val_dataset),\" samples.\")\n        print(\"the model will be tested on: \",len(self.test_dataset),\" samples.\")\n    \n\n    def train_dataloader(self):\n        if self.oversample:\n            class_counts = np.bincount(self.y_train)\n            weights = 1. / class_counts[self.y_train]\n            sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n        else:\n            sampler = RandomSampler(self.train_dataset)\n            \n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            sampler=sampler,\n            num_workers=self.num_workers,\n            persistent_workers=True,\n            pin_memory=True\n        )\n    \n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n\n\ndef init_wandb():\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"mohammad_wandb_secret\")\n    wandb.login(key=wandb_api_key)\n    \n    # Initialize the run first\n    run = wandb.init(\n        project=config.wandb.project,\n        entity=config.wandb.entity,\n        tags=config.wandb.tags,\n        notes=config.wandb.notes,\n        config={\n            \"input_size\": None,  # Will be updated later\n            \"num_classes\": None,\n            \"sequence_length\": config.training.sequence_length,\n            \"train_samples\": None,\n            \"test_samples\": None,\n            \"model_config\": dict(config.model),\n            \"training_config\": dict(config.training)\n        }\n    )\n    \n    # Then create the logger\n    wandb_logger = WandbLogger(\n        experiment=run,\n        log_model='all'\n    )\n    \n    return wandb_logger, run\n\ndef main():\n    # Initialize wandb - now returns both logger and run\n    wandb_logger, run = init_wandb()\n    \n    # Initialize data module\n    data_module = NIDSDataModule(config)\n    data_module.prepare_data()\n    data_module.setup()\n    \n    # Get input size from data\n    sample_x, _ = next(iter(data_module.train_dataloader()))\n    input_size = sample_x.shape[2]\n    num_classes = len(data_module.classes)\n    \n    # Update the config with actual values\n    run.config.update({\n        \"input_size\": input_size,\n        \"num_classes\": num_classes,\n        \"train_samples\": len(data_module.train_dataset),\n        \"test_samples\": len(data_module.test_dataset)\n    })\n    \n    # model = LSTMModel(input_size, num_classes, config)\n    \n    model = BiLSTMModel(input_size, num_classes, config)\n    \n    # Callbacks\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=config.training.early_stopping_patience,\n        mode='min'\n    )\n    \n    # checkpoint_callback = ModelCheckpoint(\n    #     monitor='val_f1',  # Now monitoring F1 score\n    #     mode='max',\n    #     save_top_k=1,\n    #     dirpath='checkpoints',\n    #     filename='best_model'\n    # )\n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_acc',\n        mode='max',\n        save_top_k=1,\n        dirpath='checkpoints',\n        filename='best_model'\n    )\n \n    # Initialize trainer\n    trainer = pl.Trainer(\n        precision=16,\n        logger=wandb_logger,\n        max_epochs=config.training.max_epochs,\n        callbacks=[early_stopping, checkpoint_callback],\n        deterministic=True,\n        gradient_clip_val=1.0,\n        enable_progress_bar=True,\n        log_every_n_steps=1000\n    )\n    \n    # Train model\n    trainer.fit(model, datamodule=data_module)\n    \n    # Test model\n    test_results = trainer.test(model, datamodule=data_module)\n    \n    # Collect all predictions and targets for final evaluation\n    test_loader = data_module.test_dataloader()\n    all_preds = []\n    all_targets = []\n    \n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            x, y = batch\n            y_hat = model(x)\n            preds = torch.argmax(y_hat, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(y.cpu().numpy())\n    \n    # Final metrics calculation\n    test_acc = accuracy_score(all_targets, all_preds)\n    test_f1 = f1_score(all_targets, all_preds, average='weighted')\n    \n    # Log final test metrics\n    wandb.log({\n        'test_acc': test_acc,\n        'test_f1': test_f1,\n        'test_loss': test_results[0]['test_loss']\n    })\n    \n    # Confusion matrix and classification report\n    class_names = data_module.classes.tolist()\n    \n    # Confusion Matrix\n    wandb.log({\n        \"confusion_matrix\": wandb.plot.confusion_matrix(\n            y_true=all_targets,\n            preds=all_preds,\n            class_names=class_names,\n            title=\"Confusion Matrix\"\n        )\n    })\n    \n    # Classification Report\n    report = classification_report(\n        all_targets, all_preds, \n        target_names=class_names,\n        output_dict=True\n    )\n    \n    # Create a wandb Table for the classification report\n    report_table = wandb.Table(columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n    for class_name in class_names:\n        report_table.add_data(\n            class_name,\n            report[class_name][\"precision\"],\n            report[class_name][\"recall\"],\n            report[class_name][\"f1-score\"],\n            report[class_name][\"support\"]\n        )\n    \n    # Add weighted averages\n    report_table.add_data(\n        \"Weighted Avg\",\n        report[\"weighted avg\"][\"precision\"],\n        report[\"weighted avg\"][\"recall\"],\n        report[\"weighted avg\"][\"f1-score\"],\n        report[\"weighted avg\"][\"support\"]\n    )\n    \n    wandb.log({\"classification_report\": report_table})\n    \n    # Finish wandb run\n    wandb.finish()\n    \nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T16:08:08.227969Z","iopub.execute_input":"2025-06-27T16:08:08.228283Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>test_acc</td><td>█▁</td></tr><tr><td>test_f1</td><td>▁</td></tr><tr><td>test_loss</td><td>▁▁</td></tr><tr><td>train_acc_epoch</td><td>▁▁▃▃▇▃▆▁▄▆▆▇▆▆▆▅▆▅▄▅▃▃▆▃▆▃▆▃▇▃▆▇▃█▆▃▄▆▅▄</td></tr><tr><td>train_loss_epoch</td><td>▇▇█▄▄▃▄▅▄▆█▃▃▂▂▃▂▄▂▃▂▅▂▂▃▃▂▄▂▂▂▁▂▄▃▂▂▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇████</td></tr><tr><td>val_acc</td><td>▁▃▅▇▅█▇</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>test_acc</td><td>0.96499</td></tr><tr><td>test_f1</td><td>0.9659</td></tr><tr><td>test_loss</td><td>0.24974</td></tr><tr><td>train_acc_epoch</td><td>96.09375</td></tr><tr><td>train_loss_epoch</td><td>0.24217</td></tr><tr><td>trainer/global_step</td><td>108248</td></tr><tr><td>val_acc</td><td>96.46864</td></tr><tr><td>val_loss</td><td>0.25012</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">celestial-universe-82</strong> at: <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/88xrolwk' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/88xrolwk</a><br> View project at: <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017</a><br>Synced 5 W&B file(s), 1 media file(s), 12 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250627_153312-88xrolwk/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250627_160808-y4gm9w0p</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot/runs/y4gm9w0p' target=\"_blank\">charmed-durian-69</a></strong> to <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot/runs/y4gm9w0p' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot/runs/y4gm9w0p</a>"},"metadata":{}},{"name":"stdout","text":"the model will be trained on:  3745408  samples.\nthe model will be validated on:  963105  samples.\nthe model will be tested on:  642070  samples.\nthe model will be trained on:  3745408  samples.\nthe model will be validated on:  963105  samples.\nthe model will be tested on:  642070  samples.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6ad6a2136d34d019f2664ec5f026892"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}