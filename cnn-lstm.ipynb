{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "execution_failed": "2025-06-29T10:43:03.344Z",
     "iopub.execute_input": "2025-06-29T09:55:38.425340Z",
     "iopub.status.busy": "2025-06-29T09:55:38.425108Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohammad-fleity\u001b[0m (\u001b[33mmohammad-fleity-lebanese-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250629_095601-4gou7vre</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/4gou7vre' target=\"_blank\">gentle-dream-90</a></strong> to <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/4gou7vre' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/4gou7vre</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model will be trained on:  1979373  samples.\n",
      "the model will be validated on:  424152  samples.\n",
      "the model will be tested on:  424152  samples.\n",
      "the model will be trained on:  1979373  samples.\n",
      "the model will be validated on:  424152  samples.\n",
      "the model will be tested on:  424152  samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2362baad990c49ecbea146edfa56a7e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561b5eb8dc394f85acfa7e8b53cef860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset ,TensorDataset, WeightedRandomSampler, RandomSampler\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import wandb\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "import torchmetrics\n",
    "import warnings\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "config = OmegaConf.create({\n",
    "    \"wandb\": {\n",
    "        \"project\": \"DL-NIDS-2--cic-ids-2017\",\n",
    "        \"entity\": \"mohammad-fleity-lebanese-university\",\n",
    "        \"tags\": [\"CNN_LSTM_AE\", \"cic-ids-2017\", \"PyTorch\"],\n",
    "        \"notes\": \"CNN_LSTM_AE for network intrusion detection with limited samples\"\n",
    "    },\n",
    "    # \"model\": {\n",
    "    #     \"hidden_size\": 128,\n",
    "    #     \"num_layers\": 2,\n",
    "    #     \"dropout\": 0.4,\n",
    "    #     \"dense_units\": [128, 64],\n",
    "    #     \"learning_rate\": 0.0001,\n",
    "    #     \"weight_decay\": 1e-4\n",
    "    # },\n",
    "    \"model\": {\n",
    "    \"cnn_channels\": 64,        # for Conv1D output channels\n",
    "    \"lstm_units\": 128,         # for both LSTM layers\n",
    "    \"dropout\": 0.4,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"weight_decay\": 1e-4\n",
    "    },\n",
    "\n",
    "    \"training\": {\n",
    "        \"sequence_length\": 5,\n",
    "        \"batch_size\": 128,\n",
    "        \"max_epochs\": 7,            \n",
    "        \"early_stopping_patience\": 7,\n",
    "        \"oversample\": True,\n",
    "        \"gpus\": 1 if torch.cuda.is_available() else 0,\n",
    "        \"max_train_samples\": 200000,  # Maximum training samples ---------------- unused\n",
    "        \"max_val_samples\": 30000,     # Maximum validation samples -------------- unused\n",
    "        \"max_test_samples\": 30000     # Maximum test samples -------------------- unused\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"raw\": \"cic_ids_2017.parquet\",\n",
    "        \"num_workers\": 4\n",
    "    }\n",
    "})\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length, target_idx=None):\n",
    "        \"\"\"\n",
    "        Dataset for time series data that creates sequences on-the-fly.\n",
    " \n",
    "        Args:\n",
    "            data: Input data tensor of shape (n_samples, n_features)\n",
    "            sequence_length: Length of sequences to create\n",
    "            target_idx: Optional tensor of target indices. If None, uses the last position in sequence\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "        self.target_idx = target_idx\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        # Calculate start index for the sequence\n",
    "        start_idx = max(0, idx - self.sequence_length + 1)\n",
    "        \n",
    "        # Get the sequence\n",
    "        sequence = self.data[start_idx:idx + 1]\n",
    "        \n",
    "        # Ensure sequence is 2D [seq_len, features]\n",
    "        if sequence.dim() == 1:\n",
    "            sequence = sequence.unsqueeze(0)  # Add sequence length dimension if missing\n",
    "        \n",
    "        # Pad the beginning if needed\n",
    "        if len(sequence) < self.sequence_length:\n",
    "            padding = torch.zeros(self.sequence_length - len(sequence), sequence.shape[1])\n",
    "            sequence = torch.cat([padding, sequence], dim=0)\n",
    "        \n",
    "        # Get target\n",
    "        target = self.target_idx[idx] if self.target_idx is not None else -1\n",
    "        \n",
    "        return sequence, target\n",
    "\n",
    "class CNN_LSTM_Classifier(pl.LightningModule):\n",
    "    def __init__(self, input_channels, num_classes, config):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters({'config': config})\n",
    "        self.config = config\n",
    "\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=input_channels,\n",
    "            out_channels=config.model.cnn_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=config.model.cnn_channels,\n",
    "            hidden_size=config.model.lstm_units,\n",
    "            batch_first=True,\n",
    "            dropout=config.model.dropout,\n",
    "            num_layers=1,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=config.model.lstm_units,\n",
    "            hidden_size=config.model.lstm_units,\n",
    "            batch_first=True,\n",
    "            dropout=config.model.dropout,\n",
    "            num_layers=1,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(config.model.dropout)\n",
    "        self.fc = nn.Linear(config.model.lstm_units, num_classes)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.train_acc = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_acc = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, features = x.shape  # (B, T, F)\n",
    "        x = x.permute(0, 2, 1)                   # (B, F, T) for Conv1D\n",
    "        x = F.relu(self.conv1(x))               # (B, C, T)\n",
    "        x = self.pool(x)                         # (B, C, T//2)\n",
    "        x = x.permute(0, 2, 1)                   # (B, T//2, C)\n",
    "\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = x[:, -1, :]                          # take last time step\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc.update(preds, y)\n",
    "        self.log(\"train_loss_epoch\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc_epoch\", self.train_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        # self.log(\"train_acc\", self.train_acc.compute() * 100, prog_bar=True)\n",
    "        return loss\n",
    "    def on_train_epoch_end(self):\n",
    "        self.train_acc.reset()\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        self.val_acc.reset()\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        self.test_acc.reset()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc.update(preds, y)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", self.val_acc.compute() * 100, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.test_acc.update(preds, y)\n",
    "\n",
    "        self.log(\"test_loss\", loss,prog_bar=True)\n",
    "        self.log(\"test_acc\", self.test_acc.compute() * 100, prog_bar=True)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.config.model.learning_rate,\n",
    "            weight_decay=self.config.model.weight_decay\n",
    "        )\n",
    "\n",
    "class NIDSDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.batch_size = config.training.batch_size\n",
    "        self.sequence_length = config.training.sequence_length\n",
    "        self.num_workers = config.data.num_workers\n",
    "        self.oversample = config.training.oversample\n",
    "        self.max_train_samples = config.training.max_train_samples\n",
    "        self.max_val_samples = config.training.max_val_samples\n",
    "        self.max_test_samples = config.training.max_test_samples\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # /kaggle/input/cic-ton-iot-parquet\n",
    "        # cic_ids_2017\n",
    "        # /kaggle/input/cic-ids-2017-parquet\n",
    "        df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n",
    "    \n",
    "        # Clean data\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        df.drop_duplicates(inplace=True)\n",
    "    \n",
    "        # Identify non-numeric columns\n",
    "        self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', \n",
    "                               'Src Port', 'Attack', 'Dst IP', 'Dst Port', 'Protocol']\n",
    "        self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n",
    "    \n",
    "        # Encode labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n",
    "        self.classes = self.label_encoder.classes_\n",
    "    \n",
    "        # Initialize scaler\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "        # Stratified split with sample limits\n",
    "        train_df, test_df = train_test_split(\n",
    "            df,\n",
    "            test_size=0.3,  # 70% train, 30% test+val\n",
    "            random_state=42,\n",
    "            stratify=df['Label_Num']\n",
    "        )\n",
    "        \n",
    "        # Further split test into val and test\n",
    "        val_df, test_df = train_test_split(\n",
    "            test_df,\n",
    "            test_size=0.5,  # 15% val, 15% test\n",
    "            random_state=42,\n",
    "            stratify=test_df['Label_Num']\n",
    "        )\n",
    "    \n",
    "        # Process each split with sample limits\n",
    "        self.X_train, self.y_train = self._prepare_features(train_df, fit=True)\n",
    "        self.X_val, self.y_val = self._prepare_features(val_df, fit=False)\n",
    "        self.X_test, self.y_test = self._prepare_features(test_df, fit=False)\n",
    "        \n",
    "        # Apply sample limits\n",
    "        self._limit_samples()\n",
    "\n",
    "    def _limit_samples(self):\n",
    "        \"\"\"Limit samples according to configuration\"\"\" # --------------- unused\n",
    "        # # Training data\n",
    "        # if len(self.X_train) > self.max_train_samples:\n",
    "        #     indices = np.random.choice(len(self.X_train), self.max_train_samples, replace=False)\n",
    "        #     self.X_train = self.X_train[indices]\n",
    "        #     self.y_train = self.y_train[indices]\n",
    "        \n",
    "        # # Validation data\n",
    "        # if len(self.X_val) > self.max_val_samples:\n",
    "        #     indices = np.random.choice(len(self.X_val), self.max_val_samples, replace=False)\n",
    "        #     self.X_val = self.X_val[indices]\n",
    "        #     self.y_val = self.y_val[indices]\n",
    "        \n",
    "        # # Test data\n",
    "        # if len(self.X_test) > self.max_test_samples:\n",
    "        #     indices = np.random.choice(len(self.X_test), self.max_test_samples, replace=False)\n",
    "        #     self.X_test = self.X_test[indices]\n",
    "        #     self.y_test = self.y_test[indices]\n",
    "        pass\n",
    "    \n",
    "    def _prepare_features(self, df, fit=False):\n",
    "        X = df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n",
    "        y = df['Label_Num']\n",
    "        if fit:\n",
    "            X = self.scaler.fit_transform(X)\n",
    "        else:\n",
    "            X = self.scaler.transform(X)\n",
    "        # return self.create_sequences(X, y)\n",
    "        return X, y\n",
    "\n",
    "    def create_sequences(self, X, y):\n",
    "        \"\"\"Create sequence\"\"\" # ---------------- unused\n",
    "        sequences = []\n",
    "        labels = []\n",
    "        for i in range(len(X) - self.sequence_length):\n",
    "            sequences.append(X[i:i+self.sequence_length])\n",
    "            labels.append(y.iloc[i+self.sequence_length-1])\n",
    "        return np.array(sequences), np.array(labels)\n",
    "    \n",
    "    # def setup(self, stage=None):\n",
    "    #     self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n",
    "    #     self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n",
    "    #     self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n",
    "        \n",
    "    #     print(f\"Training samples: {len(self.train_dataset)} (limited to {self.max_train_samples})\")\n",
    "    #     print(f\"Validation samples: {len(self.val_dataset)} (limited to {self.max_val_samples})\")\n",
    "    #     print(f\"Test samples: {len(self.test_dataset)} (limited to {self.max_test_samples})\")\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(self.X_train.reshape(-1, self.X_train.shape[-1]))\n",
    "        \n",
    "        self.X_train = self.scaler.transform(self.X_train.reshape(-1, self.X_train.shape[-1])).reshape(self.X_train.shape)\n",
    "        self.X_val = self.scaler.transform(self.X_val.reshape(-1, self.X_val.shape[-1])).reshape(self.X_val.shape)\n",
    "        self.X_test = self.scaler.transform(self.X_test.reshape(-1, self.X_test.shape[-1])).reshape(self.X_test.shape)\n",
    "        \n",
    "        # self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n",
    "        # self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n",
    "        # self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n",
    "\n",
    "        self.train_dataset = TimeSeriesDataset(\n",
    "            data=torch.FloatTensor(self.X_train),\n",
    "            sequence_length=self.sequence_length,\n",
    "            target_idx=torch.LongTensor(self.y_train.values)\n",
    "        )\n",
    "        \n",
    "        self.val_dataset = TimeSeriesDataset(\n",
    "            data=torch.FloatTensor(self.X_val),\n",
    "            sequence_length=self.sequence_length,\n",
    "            target_idx=torch.LongTensor(self.y_val.values)\n",
    "        )\n",
    "        \n",
    "        self.test_dataset = TimeSeriesDataset(\n",
    "            data=torch.FloatTensor(self.X_test),\n",
    "            sequence_length=self.sequence_length,\n",
    "            target_idx=torch.LongTensor(self.y_test.values)\n",
    "        )\n",
    "\n",
    "        print(\"the model will be trained on: \",len(self.train_dataset),\" samples.\")\n",
    "        print(\"the model will be validated on: \",len(self.val_dataset),\" samples.\")\n",
    "        print(\"the model will be tested on: \",len(self.test_dataset),\" samples.\")\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        if self.oversample:\n",
    "            class_counts = np.bincount(self.y_train)\n",
    "            weights = 1. / class_counts[self.y_train]\n",
    "            sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "        else:\n",
    "            sampler = RandomSampler(self.train_dataset)\n",
    "            \n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=sampler,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "def init_wandb():\n",
    "    user_secrets = UserSecretsClient()\n",
    "    wandb_api_key = user_secrets.get_secret(\"mohammad_wandb_secret\")\n",
    "    wandb.login(key=wandb_api_key)\n",
    "    \n",
    "    run = wandb.init(\n",
    "        project=config.wandb.project,\n",
    "        entity=config.wandb.entity,\n",
    "        tags=config.wandb.tags,\n",
    "        notes=config.wandb.notes,\n",
    "        config={\n",
    "            \"input_size\": None,\n",
    "            \"num_classes\": None,\n",
    "            \"sequence_length\": config.training.sequence_length,\n",
    "            # \"train_samples\": config.training.max_train_samples,\n",
    "            # \"val_samples\": config.training.max_val_samples,\n",
    "            # \"test_samples\": config.training.max_test_samples,\n",
    "            \"model_config\": dict(config.model),\n",
    "            \"training_config\": dict(config.training)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    wandb_logger = WandbLogger(\n",
    "        experiment=run,\n",
    "        log_model='all'\n",
    "    )\n",
    "    \n",
    "    return wandb_logger, run\n",
    "\n",
    "def main():\n",
    "    wandb_logger, run = init_wandb()\n",
    "    \n",
    "    data_module = NIDSDataModule(config)\n",
    "    data_module.prepare_data()\n",
    "    data_module.setup()\n",
    "    \n",
    "    sample_x, _ = next(iter(data_module.train_dataloader()))\n",
    "    input_size_per_timestep = sample_x.shape[2]  # Features per timestep\n",
    "    total_input_size = input_size_per_timestep * config.training.sequence_length\n",
    "    num_classes = len(data_module.classes)\n",
    "    \n",
    "    run.config.update({\n",
    "        \"input_size_per_timestep\": input_size_per_timestep,\n",
    "        \"total_input_size\": total_input_size,\n",
    "        \"num_classes\": num_classes\n",
    "    })\n",
    "    \n",
    "    # model = AutoEncoderModel(total_input_size, num_classes, config)\n",
    "    model = CNN_LSTM_Classifier(input_channels=input_size_per_timestep, num_classes=num_classes, config=config)\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=config.training.early_stopping_patience,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_top_k=1,\n",
    "        dirpath='checkpoints',\n",
    "        filename='best_model'\n",
    "    )\n",
    " \n",
    "    trainer = pl.Trainer(\n",
    "        precision=16,\n",
    "        logger=wandb_logger,\n",
    "        max_epochs=config.training.max_epochs,\n",
    "        callbacks=[early_stopping, checkpoint_callback],\n",
    "        deterministic=True,\n",
    "        gradient_clip_val=1.0,\n",
    "        enable_progress_bar=True,\n",
    "        log_every_n_steps=1000\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "    \n",
    "    test_results = trainer.test(model, datamodule=data_module)\n",
    "    \n",
    "    # Collect all predictions and targets\n",
    "    test_loader = data_module.test_dataloader()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x, y = batch\n",
    "            y_hat = model(x)\n",
    "            preds = torch.argmax(y_hat, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_acc = accuracy_score(all_targets, all_preds)\n",
    "    test_f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    \n",
    "    # Log final metrics\n",
    "    wandb.log({\n",
    "        'test_acc': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'test_loss': test_results[0]['test_loss']\n",
    "    })\n",
    "    \n",
    "    # Enhanced multiclass confusion matrix\n",
    "    class_names = data_module.classes.tolist()\n",
    "    conf_mat = confusion_matrix(all_targets, all_preds)\n",
    "    \n",
    "    # Create a custom confusion matrix plot\n",
    "    data = []\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            data.append([class_names[i], class_names[j], conf_mat[i, j]])\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap=\"Blues\")\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    wandb.log({\"conf_matrix_heatmap\": wandb.Image(plt)})\n",
    "    plt.close()\n",
    "    fields = {\n",
    "        \"Actual\": \"Actual\",\n",
    "        \"Predicted\": \"Predicted\",\n",
    "        \"n\": \"Count\"\n",
    "    }\n",
    "    \n",
    "    wandb.log({\n",
    "        \"multiclass_confusion_matrix\": wandb.plot_table(\n",
    "            \"wandb/confusion_matrix/v1\",\n",
    "            wandb.Table(columns=[\"Actual\", \"Predicted\", \"Count\"], data=data),\n",
    "            fields,\n",
    "            {\"title\": \"Multiclass Confusion Matrix\"}\n",
    "        )\n",
    "    })\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(\n",
    "        all_targets, all_preds, \n",
    "        target_names=class_names,\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    report_table = wandb.Table(columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n",
    "    for class_name in class_names:\n",
    "        report_table.add_data(\n",
    "            class_name,\n",
    "            report[class_name][\"precision\"],\n",
    "            report[class_name][\"recall\"],\n",
    "            report[class_name][\"f1-score\"],\n",
    "            report[class_name][\"support\"]\n",
    "        )\n",
    "    \n",
    "    report_table.add_data(\n",
    "        \"Weighted Avg\",\n",
    "        report[\"weighted avg\"][\"precision\"],\n",
    "        report[\"weighted avg\"][\"recall\"],\n",
    "        report[\"weighted avg\"][\"f1-score\"],\n",
    "        report[\"weighted avg\"][\"support\"]\n",
    "    )\n",
    "    \n",
    "    wandb.log({\"classification_report\": report_table})\n",
    "    \n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>SECOND Dataset</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-29T16:15:44.849Z",
     "iopub.execute_input": "2025-06-29T15:12:29.617875Z",
     "iopub.status.busy": "2025-06-29T15:12:29.617594Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohammad-fleity\u001b[0m (\u001b[33mmohammad-fleity-lebanese-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250629_151253-rncnr65q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot/runs/rncnr65q' target=\"_blank\">magic-sky-74</a></strong> to <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot/runs/rncnr65q' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot/runs/rncnr65q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the model will be trained on:  3745408  samples.\n",
      "the model will be validated on:  802587  samples.\n",
      "the model will be tested on:  802588  samples.\n",
      "the model will be trained on:  3745408  samples.\n",
      "the model will be validated on:  802587  samples.\n",
      "the model will be tested on:  802588  samples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd941a0ccf54b74a1d163be1fadd0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader,Dataset ,TensorDataset, WeightedRandomSampler, RandomSampler\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import wandb\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "import torchmetrics\n",
    "import warnings\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "config = OmegaConf.create({\n",
    "    \"wandb\": {\n",
    "        # \"project\": \"DL-NIDS-2--cic-ids-2017\",\n",
    "        \"project\": \"DL-NIDS-2--cic-ton-iot\",\n",
    "        \"entity\": \"mohammad-fleity-lebanese-university\",\n",
    "        \"tags\": [\"CNN_LSTM_AE\", \"cic-ton-iot\", \"PyTorch\"],\n",
    "        \"notes\": \"CNN_LSTM_AE for network intrusion detection\"\n",
    "    },\n",
    "    \"model\": {\n",
    "    \"cnn_channels\": 64,        # for Conv1D output channels\n",
    "    \"lstm_units\": 128,         # for both LSTM layers\n",
    "    \"dropout\": 0.4,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"weight_decay\": 1e-4\n",
    "    },\n",
    "\n",
    "    \"training\": {\n",
    "        \"sequence_length\": 5,\n",
    "        \"batch_size\": 128,\n",
    "        \"max_epochs\": 7,            \n",
    "        \"early_stopping_patience\": 7,\n",
    "        \"oversample\": True,\n",
    "        \"gpus\": 1 if torch.cuda.is_available() else 0,\n",
    "        \"max_train_samples\": 200000,  # Maximum training samples ---------------- unused\n",
    "        \"max_val_samples\": 30000,     # Maximum validation samples -------------- unused\n",
    "        \"max_test_samples\": 30000     # Maximum test samples -------------------- unused\n",
    "    },\n",
    "    \"data\": {\n",
    "        # \"raw\": \"cic_ids_2017.parquet\",\n",
    "        \"raw\": \"cic_ton_iot.parquet\",\n",
    "        \"num_workers\": 4\n",
    "    }\n",
    "})\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length, target_idx=None):\n",
    "        \"\"\"\n",
    "        Dataset for time series data that creates sequences on-the-fly.\n",
    " \n",
    "        Args:\n",
    "            data: Input data tensor of shape (n_samples, n_features)\n",
    "            sequence_length: Length of sequences to create\n",
    "            target_idx: Optional tensor of target indices. If None, uses the last position in sequence\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "        self.target_idx = target_idx\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        # Calculate start index for the sequence\n",
    "        start_idx = max(0, idx - self.sequence_length + 1)\n",
    "        \n",
    "        # Get the sequence\n",
    "        sequence = self.data[start_idx:idx + 1]\n",
    "        \n",
    "        # Ensure sequence is 2D [seq_len, features]\n",
    "        if sequence.dim() == 1:\n",
    "            sequence = sequence.unsqueeze(0)  # Add sequence length dimension if missing\n",
    "        \n",
    "        # Pad the beginning if needed\n",
    "        if len(sequence) < self.sequence_length:\n",
    "            padding = torch.zeros(self.sequence_length - len(sequence), sequence.shape[1])\n",
    "            sequence = torch.cat([padding, sequence], dim=0)\n",
    "        \n",
    "        # Get target\n",
    "        target = self.target_idx[idx] if self.target_idx is not None else -1\n",
    "        \n",
    "        return sequence, target\n",
    "\n",
    "\n",
    "class CNN_LSTM_Classifier(pl.LightningModule):\n",
    "    def __init__(self, input_channels, num_classes, config):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters({'config': config})\n",
    "        self.config = config\n",
    "\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=input_channels,\n",
    "            out_channels=config.model.cnn_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=config.model.cnn_channels,\n",
    "            hidden_size=config.model.lstm_units,\n",
    "            batch_first=True,\n",
    "            dropout=config.model.dropout,\n",
    "            num_layers=1,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=config.model.lstm_units,\n",
    "            hidden_size=config.model.lstm_units,\n",
    "            batch_first=True,\n",
    "            dropout=config.model.dropout,\n",
    "            num_layers=1,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(config.model.dropout)\n",
    "        self.fc = nn.Linear(config.model.lstm_units, num_classes)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.train_acc = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_acc = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, features = x.shape  # (B, T, F)\n",
    "        x = x.permute(0, 2, 1)                   # (B, F, T) for Conv1D\n",
    "        x = F.relu(self.conv1(x))               # (B, C, T)\n",
    "        x = self.pool(x)                         # (B, C, T//2)\n",
    "        x = x.permute(0, 2, 1)                   # (B, T//2, C)\n",
    "\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = x[:, -1, :]                          # take last time step\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc.update(preds, y)\n",
    "        self.log(\"train_loss_epoch\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc_epoch\", self.train_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        # self.log(\"train_acc\", self.train_acc.compute() * 100, prog_bar=True)\n",
    "        return loss\n",
    "    def on_train_epoch_end(self):\n",
    "        self.train_acc.reset()\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        self.val_acc.reset()\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        self.test_acc.reset()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc.update(preds, y)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", self.val_acc.compute() * 100, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.test_acc.update(preds, y)\n",
    "\n",
    "        self.log(\"test_loss\", loss,prog_bar=True)\n",
    "        self.log(\"test_acc\", self.test_acc.compute() * 100, prog_bar=True)\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.config.model.learning_rate,\n",
    "            weight_decay=self.config.model.weight_decay\n",
    "        )\n",
    "\n",
    "class NIDSDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.batch_size = config.training.batch_size\n",
    "        self.sequence_length = config.training.sequence_length\n",
    "        self.num_workers = config.data.num_workers\n",
    "        self.oversample = config.training.oversample\n",
    "        self.max_train_samples = config.training.max_train_samples\n",
    "        self.max_val_samples = config.training.max_val_samples\n",
    "        self.max_test_samples = config.training.max_test_samples\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # /kaggle/input/cic-ton-iot-parquet\n",
    "        # cic_ids_2017\n",
    "        # /kaggle/input/cic-ids-2017-parquet\n",
    "        df = pd.read_parquet(os.path.join('/kaggle/input/cic-ton-iot-parquet', self.config.data.raw))\n",
    "        # df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n",
    "    \n",
    "        # Clean data\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        df.drop_duplicates(inplace=True)\n",
    "    \n",
    "        # Identify non-numeric columns\n",
    "        self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', \n",
    "                               'Src Port', 'Attack', 'Dst IP', 'Dst Port', 'Protocol']\n",
    "        self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n",
    "    \n",
    "        # Encode labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n",
    "        self.classes = self.label_encoder.classes_\n",
    "    \n",
    "        # Initialize scaler\n",
    "        self.scaler = StandardScaler()\n",
    "    \n",
    "        # Stratified split with sample limits\n",
    "        train_df, test_df = train_test_split(\n",
    "            df,\n",
    "            test_size=0.3,  # 70% train, 30% test+val\n",
    "            random_state=42,\n",
    "            stratify=df['Label_Num']\n",
    "        )\n",
    "        \n",
    "        # Further split test into val and test\n",
    "        val_df, test_df = train_test_split(\n",
    "            test_df,\n",
    "            test_size=0.5,  # 15% val, 15% test\n",
    "            random_state=42,\n",
    "            stratify=test_df['Label_Num']\n",
    "        )\n",
    "    \n",
    "        # Process each split with sample limits\n",
    "        self.X_train, self.y_train = self._prepare_features(train_df, fit=True)\n",
    "        self.X_val, self.y_val = self._prepare_features(val_df, fit=False)\n",
    "        self.X_test, self.y_test = self._prepare_features(test_df, fit=False)\n",
    "        \n",
    "        # Apply sample limits\n",
    "        self._limit_samples()\n",
    "\n",
    "    def _limit_samples(self):\n",
    "        \"\"\"Limit samples according to configuration\"\"\" # --------------- unused\n",
    "        # # Training data\n",
    "        # if len(self.X_train) > self.max_train_samples:\n",
    "        #     indices = np.random.choice(len(self.X_train), self.max_train_samples, replace=False)\n",
    "        #     self.X_train = self.X_train[indices]\n",
    "        #     self.y_train = self.y_train[indices]\n",
    "        \n",
    "        # # Validation data\n",
    "        # if len(self.X_val) > self.max_val_samples:\n",
    "        #     indices = np.random.choice(len(self.X_val), self.max_val_samples, replace=False)\n",
    "        #     self.X_val = self.X_val[indices]\n",
    "        #     self.y_val = self.y_val[indices]\n",
    "        \n",
    "        # # Test data\n",
    "        # if len(self.X_test) > self.max_test_samples:\n",
    "        #     indices = np.random.choice(len(self.X_test), self.max_test_samples, replace=False)\n",
    "        #     self.X_test = self.X_test[indices]\n",
    "        #     self.y_test = self.y_test[indices]\n",
    "        pass\n",
    "    \n",
    "    def _prepare_features(self, df, fit=False):\n",
    "        X = df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n",
    "        y = df['Label_Num']\n",
    "        if fit:\n",
    "            X = self.scaler.fit_transform(X)\n",
    "        else:\n",
    "            X = self.scaler.transform(X)\n",
    "        # return self.create_sequences(X, y)\n",
    "        return X, y\n",
    "\n",
    "    def create_sequences(self, X, y):\n",
    "        \"\"\"Create sequence\"\"\" # ---------------- unused\n",
    "        sequences = []\n",
    "        labels = []\n",
    "        for i in range(len(X) - self.sequence_length):\n",
    "            sequences.append(X[i:i+self.sequence_length])\n",
    "            labels.append(y.iloc[i+self.sequence_length-1])\n",
    "        return np.array(sequences), np.array(labels)\n",
    "    \n",
    "    # def setup(self, stage=None):\n",
    "    #     self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n",
    "    #     self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n",
    "    #     self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n",
    "        \n",
    "    #     print(f\"Training samples: {len(self.train_dataset)} (limited to {self.max_train_samples})\")\n",
    "    #     print(f\"Validation samples: {len(self.val_dataset)} (limited to {self.max_val_samples})\")\n",
    "    #     print(f\"Test samples: {len(self.test_dataset)} (limited to {self.max_test_samples})\")\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(self.X_train.reshape(-1, self.X_train.shape[-1]))\n",
    "        \n",
    "        self.X_train = self.scaler.transform(self.X_train.reshape(-1, self.X_train.shape[-1])).reshape(self.X_train.shape)\n",
    "        self.X_val = self.scaler.transform(self.X_val.reshape(-1, self.X_val.shape[-1])).reshape(self.X_val.shape)\n",
    "        self.X_test = self.scaler.transform(self.X_test.reshape(-1, self.X_test.shape[-1])).reshape(self.X_test.shape)\n",
    "        \n",
    "        # self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n",
    "        # self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n",
    "        # self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n",
    "\n",
    "        self.train_dataset = TimeSeriesDataset(\n",
    "            data=torch.FloatTensor(self.X_train),\n",
    "            sequence_length=self.sequence_length,\n",
    "            target_idx=torch.LongTensor(self.y_train.values)\n",
    "        )\n",
    "        \n",
    "        self.val_dataset = TimeSeriesDataset(\n",
    "            data=torch.FloatTensor(self.X_val),\n",
    "            sequence_length=self.sequence_length,\n",
    "            target_idx=torch.LongTensor(self.y_val.values)\n",
    "        )\n",
    "        \n",
    "        self.test_dataset = TimeSeriesDataset(\n",
    "            data=torch.FloatTensor(self.X_test),\n",
    "            sequence_length=self.sequence_length,\n",
    "            target_idx=torch.LongTensor(self.y_test.values)\n",
    "        )\n",
    "\n",
    "        print(\"the model will be trained on: \",len(self.train_dataset),\" samples.\")\n",
    "        print(\"the model will be validated on: \",len(self.val_dataset),\" samples.\")\n",
    "        print(\"the model will be tested on: \",len(self.test_dataset),\" samples.\")\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        if self.oversample:\n",
    "            class_counts = np.bincount(self.y_train)\n",
    "            weights = 1. / class_counts[self.y_train]\n",
    "            sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n",
    "        else:\n",
    "            sampler = RandomSampler(self.train_dataset)\n",
    "            \n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            sampler=sampler,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "def init_wandb():\n",
    "    user_secrets = UserSecretsClient()\n",
    "    wandb_api_key = user_secrets.get_secret(\"mohammad_wandb_secret\")\n",
    "    wandb.login(key=wandb_api_key)\n",
    "    \n",
    "    run = wandb.init(\n",
    "        project=config.wandb.project,\n",
    "        entity=config.wandb.entity,\n",
    "        tags=config.wandb.tags,\n",
    "        notes=config.wandb.notes,\n",
    "        config={\n",
    "            \"input_size\": None,\n",
    "            \"num_classes\": None,\n",
    "            \"sequence_length\": config.training.sequence_length,\n",
    "            # \"train_samples\": config.training.max_train_samples,\n",
    "            # \"val_samples\": config.training.max_val_samples,\n",
    "            # \"test_samples\": config.training.max_test_samples,\n",
    "            \"model_config\": dict(config.model),\n",
    "            \"training_config\": dict(config.training)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    wandb_logger = WandbLogger(\n",
    "        experiment=run,\n",
    "        log_model='all'\n",
    "    )\n",
    "    \n",
    "    return wandb_logger, run\n",
    "\n",
    "def main():\n",
    "    wandb_logger, run = init_wandb()\n",
    "    \n",
    "    data_module = NIDSDataModule(config)\n",
    "    data_module.prepare_data()\n",
    "    data_module.setup()\n",
    "    \n",
    "    sample_x, _ = next(iter(data_module.train_dataloader()))\n",
    "    input_size_per_timestep = sample_x.shape[2]  # Features per timestep\n",
    "    total_input_size = input_size_per_timestep * config.training.sequence_length\n",
    "    num_classes = len(data_module.classes)\n",
    "    \n",
    "    run.config.update({\n",
    "        \"input_size_per_timestep\": input_size_per_timestep,\n",
    "        \"total_input_size\": total_input_size,\n",
    "        \"num_classes\": num_classes\n",
    "    })\n",
    "    \n",
    "    # model = AutoEncoderModel(total_input_size, num_classes, config)\n",
    "    model = CNN_LSTM_Classifier(input_channels=input_size_per_timestep, num_classes=num_classes, config=config)\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=config.training.early_stopping_patience,\n",
    "        mode='min'\n",
    "    )\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_top_k=1,\n",
    "        dirpath='checkpoints',\n",
    "        filename='best_model'\n",
    "    )\n",
    " \n",
    "    trainer = pl.Trainer(\n",
    "        precision=16,\n",
    "        logger=wandb_logger,\n",
    "        max_epochs=config.training.max_epochs,\n",
    "        callbacks=[early_stopping, checkpoint_callback],\n",
    "        deterministic=True,\n",
    "        gradient_clip_val=1.0,\n",
    "        enable_progress_bar=True,\n",
    "        log_every_n_steps=1000\n",
    "    )\n",
    "    \n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "    \n",
    "    test_results = trainer.test(model, datamodule=data_module)\n",
    "    \n",
    "    # Collect all predictions and targets\n",
    "    test_loader = data_module.test_dataloader()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x, y = batch\n",
    "            y_hat = model(x)\n",
    "            preds = torch.argmax(y_hat, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_targets.extend(y.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_acc = accuracy_score(all_targets, all_preds)\n",
    "    test_f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    \n",
    "    # Log final metrics\n",
    "    wandb.log({\n",
    "        'test_acc': test_acc,\n",
    "        'test_f1': test_f1,\n",
    "        'test_loss': test_results[0]['test_loss']\n",
    "    })\n",
    "    \n",
    "    # Enhanced multiclass confusion matrix\n",
    "    class_names = data_module.classes.tolist()\n",
    "    conf_mat = confusion_matrix(all_targets, all_preds)\n",
    "    \n",
    "    # Create a custom confusion matrix plot\n",
    "    data = []\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            data.append([class_names[i], class_names[j], conf_mat[i, j]])\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(conf_mat, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap=\"Blues\")\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    wandb.log({\"conf_matrix_heatmap\": wandb.Image(plt)})\n",
    "    plt.close()\n",
    "    fields = {\n",
    "        \"Actual\": \"Actual\",\n",
    "        \"Predicted\": \"Predicted\",\n",
    "        \"n\": \"Count\"\n",
    "    }\n",
    "    \n",
    "    wandb.log({\n",
    "        \"multiclass_confusion_matrix\": wandb.plot_table(\n",
    "            \"wandb/confusion_matrix/v1\",\n",
    "            wandb.Table(columns=[\"Actual\", \"Predicted\", \"Count\"], data=data),\n",
    "            fields,\n",
    "            {\"title\": \"Multiclass Confusion Matrix\"}\n",
    "        )\n",
    "    })\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(\n",
    "        all_targets, all_preds, \n",
    "        target_names=class_names,\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    report_table = wandb.Table(columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n",
    "    for class_name in class_names:\n",
    "        report_table.add_data(\n",
    "            class_name,\n",
    "            report[class_name][\"precision\"],\n",
    "            report[class_name][\"recall\"],\n",
    "            report[class_name][\"f1-score\"],\n",
    "            report[class_name][\"support\"]\n",
    "        )\n",
    "    \n",
    "    report_table.add_data(\n",
    "        \"Weighted Avg\",\n",
    "        report[\"weighted avg\"][\"precision\"],\n",
    "        report[\"weighted avg\"][\"recall\"],\n",
    "        report[\"weighted avg\"][\"f1-score\"],\n",
    "        report[\"weighted avg\"][\"support\"]\n",
    "    )\n",
    "    \n",
    "    wandb.log({\"classification_report\": report_table})\n",
    "    \n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# DL-NIDS-2--cic-ids-2017\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"DL-NIDS-2--cic-ids-2017\")\n",
    "\n",
    "table = wandb.Table(data=[\n",
    "    [\"Bilstm\", 0.249741166830062],\n",
    "    [\"LSTM\", 0.252381503582],\n",
    "    [\"cnn_lstm_new\", 0.151507258415222]\n",
    "], columns=[\"Model\", \"Test Loss\"])\n",
    "\n",
    "wandb.log({\"Final Test Loss Comparison\": wandb.plot.bar(table, \"Model\", \"Test Loss\", title=\"Test Loss\")})\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4775527,
     "sourceId": 8089281,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4775518,
     "sourceId": 8089266,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
