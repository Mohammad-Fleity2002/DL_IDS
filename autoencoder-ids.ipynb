{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8089266,"sourceType":"datasetVersion","datasetId":4775518},{"sourceId":8089281,"sourceType":"datasetVersion","datasetId":4775527}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h2>First Dataset</h2>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader,Dataset ,TensorDataset, WeightedRandomSampler, RandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nimport wandb\nfrom omegaconf import OmegaConf\nimport os\nimport torchmetrics\nimport warnings\nfrom kaggle_secrets import UserSecretsClient\n\nwarnings.filterwarnings('ignore')\n\nconfig = OmegaConf.create({\n    \"wandb\": {\n        \"project\": \"DL-NIDS-2--cic-ids-2017\",\n        \"entity\": \"mohammad-fleity-lebanese-university\",\n        \"tags\": [\"AutoEncoderDecoder\", \"cic-ids-2017\", \"PyTorch\"],\n        \"notes\": \"Optimized AutoEncoderDecoder for network intrusion detection with limited samples\"\n    },\n    \"model\": {\n        \"hidden_size\": 128,\n        \"num_layers\": 2,\n        \"dropout\": 0.4,\n        \"dense_units\": [128, 64],\n        \"learning_rate\": 0.0001,\n        \"weight_decay\": 1e-4\n    },\n    \"training\": {\n        \"sequence_length\": 5,\n        \"batch_size\": 128,\n        \"max_epochs\": 7,            \n        \"early_stopping_patience\": 7,\n        \"oversample\": True,\n        \"gpus\": 1 if torch.cuda.is_available() else 0,\n        \"max_train_samples\": 200000,  # Maximum training samples ---------------- unused\n        \"max_val_samples\": 30000,     # Maximum validation samples -------------- unused\n        \"max_test_samples\": 30000     # Maximum test samples -------------------- unused\n    },\n    \"data\": {\n        \"raw\": \"cic_ids_2017.parquet\",\n        \"num_workers\": 4\n    }\n})\n\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, data, sequence_length, target_idx=None):\n        \"\"\"\n        Dataset for time series data that creates sequences on-the-fly.\n \n        Args:\n            data: Input data tensor of shape (n_samples, n_features)\n            sequence_length: Length of sequences to create\n            target_idx: Optional tensor of target indices. If None, uses the last position in sequence\n        \"\"\"\n        self.data = data\n        self.sequence_length = sequence_length\n        self.target_idx = target_idx\n \n    def __len__(self):\n        return len(self.data)\n \n    def __getitem__(self, idx):\n        # Calculate start index for the sequence\n        start_idx = max(0, idx - self.sequence_length + 1)\n        \n        # Get the sequence\n        sequence = self.data[start_idx:idx + 1]\n        \n        # Ensure sequence is 2D [seq_len, features]\n        if sequence.dim() == 1:\n            sequence = sequence.unsqueeze(0)  # Add sequence length dimension if missing\n        \n        # Pad the beginning if needed\n        if len(sequence) < self.sequence_length:\n            padding = torch.zeros(self.sequence_length - len(sequence), sequence.shape[1])\n            sequence = torch.cat([padding, sequence], dim=0)\n        \n        # Get target\n        target = self.target_idx[idx] if self.target_idx is not None else -1\n        \n        return sequence, target\n\nclass AutoEncoderModel(pl.LightningModule):\n    def __init__(self, input_size, num_classes, config):\n        super().__init__()\n        self.save_hyperparameters({'config': config})\n        self.config = config\n        \n        self.encoder = nn.Sequential(\n            nn.Linear(input_size, 128),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(64, 32)\n        )\n\n        self.decoder = nn.Sequential(\n            nn.Linear(32, 64),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(64, 128),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(128, input_size)  # match input size for reconstruction\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(32, 64),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(64, num_classes)\n        )\n\n        self.recon_loss = nn.MSELoss()\n        self.class_loss = nn.CrossEntropyLoss()\n\n        # Metrics\n        self.train_acc = torchmetrics.classification.Accuracy(task='multiclass', num_classes=num_classes)\n        self.val_acc = torchmetrics.classification.Accuracy(task='multiclass', num_classes=num_classes)\n        self.test_acc = torchmetrics.classification.Accuracy(task='multiclass', num_classes=num_classes)\n\n    def forward(self, x):\n        batch_size, seq_len, features = x.shape\n        x_flat = x.view(batch_size, -1)\n        z = self.encoder(x_flat)\n\n        x_hat = self.decoder(z).view(batch_size, seq_len, features)\n        logits = self.classifier(z)\n        return x_hat, logits\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x_hat, logits = self(x)\n        loss_recon = self.recon_loss(x_hat, x)\n        loss_cls = self.class_loss(logits, y)\n        loss = loss_recon + loss_cls\n\n        preds = torch.argmax(logits, dim=1)\n        self.train_acc.update(preds, y)\n\n        self.log(\"train_loss_epoch\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"train_acc_epoch\", self.train_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n        # self.log(\"train_acc_epoch\", self.train_acc.compute()*100, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        x_hat, logits = self(x)\n        loss_recon = self.recon_loss(x_hat, x)\n        loss_cls = self.class_loss(logits, y)\n        loss = loss_recon + loss_cls\n\n        preds = torch.argmax(logits, dim=1)\n        self.val_acc.update(preds, y)\n\n        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        # self.log(\"val_acc\", self.val_acc*100, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"val_acc\", self.val_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        x_hat, logits = self(x)\n        loss_recon = self.recon_loss(x_hat, x)\n        # loss_cls = self.class_loss(logits, y)\n        loss = loss_recon \n        # loss = loss_recon + loss_cls\n\n        preds = torch.argmax(logits, dim=1)\n        self.test_acc.update(preds, y)\n\n        self.log(\"test_loss\", loss, on_step=False, on_epoch=True)\n        # print(\"loss: \",loss ,\".\")\n        # self.log(\"test_acc\", self.test_acc, on_step=False, on_epoch=True)\n        self.log(\"test_acc\", self.test_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n        return {\"loss\": loss}\n\n    def configure_optimizers(self):\n        return optim.AdamW(\n            self.parameters(),\n            lr=self.hparams.config.model.learning_rate,\n            weight_decay=self.hparams.config.model.weight_decay\n        )\n\n\nclass NIDSDataModule(pl.LightningDataModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.batch_size = config.training.batch_size\n        self.sequence_length = config.training.sequence_length\n        self.num_workers = config.data.num_workers\n        self.oversample = config.training.oversample\n        self.max_train_samples = config.training.max_train_samples\n        self.max_val_samples = config.training.max_val_samples\n        self.max_test_samples = config.training.max_test_samples\n\n    def prepare_data(self):\n        # /kaggle/input/cic-ton-iot-parquet\n        # cic_ids_2017\n        # /kaggle/input/cic-ids-2017-parquet\n        df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n    \n        # Clean data\n        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df.dropna(inplace=True)\n        df.drop_duplicates(inplace=True)\n    \n        # Identify non-numeric columns\n        self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', \n                               'Src Port', 'Attack', 'Dst IP', 'Dst Port', 'Protocol']\n        self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n    \n        # Encode labels\n        self.label_encoder = LabelEncoder()\n        df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n        self.classes = self.label_encoder.classes_\n    \n        # Initialize scaler\n        self.scaler = StandardScaler()\n    \n        # Stratified split with sample limits\n        train_df, test_df = train_test_split(\n            df,\n            test_size=0.3,  # 70% train, 30% test+val\n            random_state=42,\n            stratify=df['Label_Num']\n        )\n        \n        # Further split test into val and test\n        val_df, test_df = train_test_split(\n            test_df,\n            test_size=0.5,  # 15% val, 15% test\n            random_state=42,\n            stratify=test_df['Label_Num']\n        )\n    \n        # Process each split with sample limits\n        self.X_train, self.y_train = self._prepare_features(train_df, fit=True)\n        self.X_val, self.y_val = self._prepare_features(val_df, fit=False)\n        self.X_test, self.y_test = self._prepare_features(test_df, fit=False)\n        \n        # Apply sample limits\n        self._limit_samples()\n\n    def _limit_samples(self):\n        \"\"\"Limit samples according to configuration\"\"\" # --------------- unused\n        # # Training data\n        # if len(self.X_train) > self.max_train_samples:\n        #     indices = np.random.choice(len(self.X_train), self.max_train_samples, replace=False)\n        #     self.X_train = self.X_train[indices]\n        #     self.y_train = self.y_train[indices]\n        \n        # # Validation data\n        # if len(self.X_val) > self.max_val_samples:\n        #     indices = np.random.choice(len(self.X_val), self.max_val_samples, replace=False)\n        #     self.X_val = self.X_val[indices]\n        #     self.y_val = self.y_val[indices]\n        \n        # # Test data\n        # if len(self.X_test) > self.max_test_samples:\n        #     indices = np.random.choice(len(self.X_test), self.max_test_samples, replace=False)\n        #     self.X_test = self.X_test[indices]\n        #     self.y_test = self.y_test[indices]\n        pass\n    \n    def _prepare_features(self, df, fit=False):\n        X = df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n        y = df['Label_Num']\n        if fit:\n            X = self.scaler.fit_transform(X)\n        else:\n            X = self.scaler.transform(X)\n        # return self.create_sequences(X, y)\n        return X, y\n\n    def create_sequences(self, X, y):\n        \"\"\"Create sequence\"\"\" # ---------------- unused\n        sequences = []\n        labels = []\n        for i in range(len(X) - self.sequence_length):\n            sequences.append(X[i:i+self.sequence_length])\n            labels.append(y.iloc[i+self.sequence_length-1])\n        return np.array(sequences), np.array(labels)\n    \n    # def setup(self, stage=None):\n    #     self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n    #     self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n    #     self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n        \n    #     print(f\"Training samples: {len(self.train_dataset)} (limited to {self.max_train_samples})\")\n    #     print(f\"Validation samples: {len(self.val_dataset)} (limited to {self.max_val_samples})\")\n    #     print(f\"Test samples: {len(self.test_dataset)} (limited to {self.max_test_samples})\")\n\n    def setup(self, stage=None):\n        self.scaler = StandardScaler()\n        self.scaler.fit(self.X_train.reshape(-1, self.X_train.shape[-1]))\n        \n        self.X_train = self.scaler.transform(self.X_train.reshape(-1, self.X_train.shape[-1])).reshape(self.X_train.shape)\n        self.X_val = self.scaler.transform(self.X_val.reshape(-1, self.X_val.shape[-1])).reshape(self.X_val.shape)\n        self.X_test = self.scaler.transform(self.X_test.reshape(-1, self.X_test.shape[-1])).reshape(self.X_test.shape)\n        \n        # self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n        # self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n        # self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n\n        self.train_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_train),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_train.values)\n        )\n        \n        self.val_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_val),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_val.values)\n        )\n        \n        self.test_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_test),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_test.values)\n        )\n\n        print(\"the model will be trained on: \",len(self.train_dataset),\" samples.\")\n        print(\"the model will be validated on: \",len(self.val_dataset),\" samples.\")\n        print(\"the model will be tested on: \",len(self.test_dataset),\" samples.\")\n        \n    def train_dataloader(self):\n        if self.oversample:\n            class_counts = np.bincount(self.y_train)\n            weights = 1. / class_counts[self.y_train]\n            sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n        else:\n            sampler = RandomSampler(self.train_dataset)\n            \n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            sampler=sampler,\n            num_workers=self.num_workers,\n            persistent_workers=True,\n            pin_memory=True\n        )\n    \n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n\ndef init_wandb():\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"mohammad_wandb_secret\")\n    wandb.login(key=wandb_api_key)\n    \n    run = wandb.init(\n        project=config.wandb.project,\n        entity=config.wandb.entity,\n        tags=config.wandb.tags,\n        notes=config.wandb.notes,\n        config={\n            \"input_size\": None,\n            \"num_classes\": None,\n            \"sequence_length\": config.training.sequence_length,\n            # \"train_samples\": config.training.max_train_samples,\n            # \"val_samples\": config.training.max_val_samples,\n            # \"test_samples\": config.training.max_test_samples,\n            \"model_config\": dict(config.model),\n            \"training_config\": dict(config.training)\n        }\n    )\n    \n    wandb_logger = WandbLogger(\n        experiment=run,\n        log_model='all'\n    )\n    \n    return wandb_logger, run\n\ndef main():\n    wandb_logger, run = init_wandb()\n    \n    data_module = NIDSDataModule(config)\n    data_module.prepare_data()\n    data_module.setup()\n    \n    sample_x, _ = next(iter(data_module.train_dataloader()))\n    input_size_per_timestep = sample_x.shape[2]  # Features per timestep\n    total_input_size = input_size_per_timestep * config.training.sequence_length\n    num_classes = len(data_module.classes)\n    \n    run.config.update({\n        \"input_size_per_timestep\": input_size_per_timestep,\n        \"total_input_size\": total_input_size,\n        \"num_classes\": num_classes\n    })\n    \n    model = AutoEncoderModel(total_input_size, num_classes, config)\n    \n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=config.training.early_stopping_patience,\n        mode='min'\n    )\n    \n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_loss',\n        mode='min',\n        save_top_k=1,\n        dirpath='checkpoints',\n        filename='best_model'\n    )\n \n    trainer = pl.Trainer(\n        precision=16,\n        logger=wandb_logger,\n        max_epochs=config.training.max_epochs,\n        callbacks=[early_stopping, checkpoint_callback],\n        deterministic=True,\n        gradient_clip_val=1.0,\n        enable_progress_bar=True,\n        log_every_n_steps=1000\n    )\n    \n    trainer.fit(model, datamodule=data_module)\n    \n    test_results = trainer.test(model, datamodule=data_module)\n    \n    # Collect all predictions and targets\n    test_loader = data_module.test_dataloader()\n    all_preds = []\n    all_targets = []\n    \n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            x, y = batch\n            y_hat = model(x)\n            preds = torch.argmax(y_hat, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(y.cpu().numpy())\n    \n    # Calculate metrics\n    test_acc = accuracy_score(all_targets, all_preds)\n    test_f1 = f1_score(all_targets, all_preds, average='weighted')\n    \n    # Log final metrics\n    wandb.log({\n        'test_acc': test_acc,\n        'test_f1': test_f1,\n        'test_loss': test_results[0]['test_loss']\n    })\n    \n    # Enhanced multiclass confusion matrix\n    class_names = data_module.classes.tolist()\n    conf_mat = confusion_matrix(all_targets, all_preds)\n    \n    # Create a custom confusion matrix plot\n    data = []\n    for i in range(len(class_names)):\n        for j in range(len(class_names)):\n            data.append([class_names[i], class_names[j], conf_mat[i, j]])\n    \n    fields = {\n        \"Actual\": \"Actual\",\n        \"Predicted\": \"Predicted\",\n        \"n\": \"Count\"\n    }\n    \n    wandb.log({\n        \"multiclass_confusion_matrix\": wandb.plot_table(\n            \"wandb/confusion_matrix/v1\",\n            wandb.Table(columns=[\"Actual\", \"Predicted\", \"Count\"], data=data),\n            fields,\n            {\"title\": \"Multiclass Confusion Matrix\"}\n        )\n    })\n    \n    # Classification Report\n    report = classification_report(\n        all_targets, all_preds, \n        target_names=class_names,\n        output_dict=True\n    )\n    \n    report_table = wandb.Table(columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n    for class_name in class_names:\n        report_table.add_data(\n            class_name,\n            report[class_name][\"precision\"],\n            report[class_name][\"recall\"],\n            report[class_name][\"f1-score\"],\n            report[class_name][\"support\"]\n        )\n    \n    report_table.add_data(\n        \"Weighted Avg\",\n        report[\"weighted avg\"][\"precision\"],\n        report[\"weighted avg\"][\"recall\"],\n        report[\"weighted avg\"][\"f1-score\"],\n        report[\"weighted avg\"][\"support\"]\n    )\n    \n    wandb.log({\"classification_report\": report_table})\n    \n    wandb.finish()\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T09:08:31.420270Z","iopub.execute_input":"2025-06-28T09:08:31.420558Z","execution_failed":"2025-06-28T10:12:35.438Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">deft-bee-84</strong> at: <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/kzf2p4jc' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/kzf2p4jc</a><br> View project at: <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250628_090701-kzf2p4jc/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250628_090831-dsy0r7a2</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/dsy0r7a2' target=\"_blank\">rose-wildflower-85</a></strong> to <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/dsy0r7a2' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/dsy0r7a2</a>"},"metadata":{}},{"name":"stdout","text":"the model will be trained on:  1979373  samples.\nthe model will be validated on:  424152  samples.\nthe model will be tested on:  424152  samples.\nthe model will be trained on:  1979373  samples.\nthe model will be validated on:  424152  samples.\nthe model will be tested on:  424152  samples.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ba140a4236f443eb0c437a5ccd307fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"<h2>Second Dataset</h2>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader,Dataset ,TensorDataset, WeightedRandomSampler, RandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nimport wandb\nfrom omegaconf import OmegaConf\nimport os\nimport torchmetrics\nimport warnings\nfrom kaggle_secrets import UserSecretsClient\n\nwarnings.filterwarnings('ignore')\n\nconfig = OmegaConf.create({\n    \"wandb\": {\n        # \"project\": \"DL-NIDS-2--cic-ids-2017\",\n        # \"entity\": \"mohammad-fleity-lebanese-university\",\n        \"project\": \"DL-NIDS-2--cic-ton-iot\",\n        \"entity\": \"mohammad-fleity-lebanese-university\",\n        \"tags\": [\"AutoEncoderDecoder\", \"cic-ton-iot\", \"PyTorch\"],\n        \"notes\": \"Optimized AutoEncoderDecoder for network intrusion detection with limited samples\"\n    },\n    \"model\": {\n        \"hidden_size\": 128,\n        \"num_layers\": 2,\n        \"dropout\": 0.4,\n        \"dense_units\": [128, 64],\n        \"learning_rate\": 0.0001,\n        \"weight_decay\": 1e-4\n    },\n    \"training\": {\n        \"sequence_length\": 5,\n        \"batch_size\": 128,\n        \"max_epochs\": 10,            \n        \"early_stopping_patience\": 7,\n        \"oversample\": True,\n        \"gpus\": 1 if torch.cuda.is_available() else 0,\n        \"max_train_samples\": 200000,  # Maximum training samples ---------------- unused\n        \"max_val_samples\": 30000,     # Maximum validation samples -------------- unused\n        \"max_test_samples\": 30000     # Maximum test samples -------------------- unused\n    },\n    \"data\": {\n        # \"raw\": \"cic_ids_2017.parquet\",\n        # \"num_workers\": 4\n        \"raw\": \"cic_ton_iot.parquet\",\n        \"num_workers\": 4\n    }\n})\n\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, data, sequence_length, target_idx=None):\n        \"\"\"\n        Dataset for time series data that creates sequences on-the-fly.\n \n        Args:\n            data: Input data tensor of shape (n_samples, n_features)\n            sequence_length: Length of sequences to create\n            target_idx: Optional tensor of target indices. If None, uses the last position in sequence\n        \"\"\"\n        self.data = data\n        self.sequence_length = sequence_length\n        self.target_idx = target_idx\n \n    def __len__(self):\n        return len(self.data)\n \n    def __getitem__(self, idx):\n        # Calculate start index for the sequence\n        start_idx = max(0, idx - self.sequence_length + 1)\n        \n        # Get the sequence\n        sequence = self.data[start_idx:idx + 1]\n        \n        # Ensure sequence is 2D [seq_len, features]\n        if sequence.dim() == 1:\n            sequence = sequence.unsqueeze(0)  # Add sequence length dimension if missing\n        \n        # Pad the beginning if needed\n        if len(sequence) < self.sequence_length:\n            padding = torch.zeros(self.sequence_length - len(sequence), sequence.shape[1])\n            sequence = torch.cat([padding, sequence], dim=0)\n        \n        # Get target\n        target = self.target_idx[idx] if self.target_idx is not None else -1\n        \n        return sequence, target\n\nclass AutoEncoderModel(pl.LightningModule):\n    def __init__(self, input_size, num_classes, config):\n        super().__init__()\n        self.save_hyperparameters({'config': config})\n        self.config = config\n        \n        self.encoder = nn.Sequential(\n            nn.Linear(input_size, 128),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(64, 32)\n        )\n\n        self.decoder = nn.Sequential(\n            nn.Linear(32, 64),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(64, 128),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(128, input_size)  # match input size for reconstruction\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(32, 64),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(64, num_classes)\n        )\n\n        self.recon_loss = nn.MSELoss()\n        self.class_loss = nn.CrossEntropyLoss()\n\n        # Metrics\n        self.train_acc = torchmetrics.classification.Accuracy(task='multiclass', num_classes=num_classes)\n        self.val_acc = torchmetrics.classification.Accuracy(task='multiclass', num_classes=num_classes)\n        self.test_acc = torchmetrics.classification.Accuracy(task='multiclass', num_classes=num_classes)\n\n    def forward(self, x):\n        batch_size, seq_len, features = x.shape\n        x_flat = x.view(batch_size, -1)\n        z = self.encoder(x_flat)\n\n        x_hat = self.decoder(z).view(batch_size, seq_len, features)\n        logits = self.classifier(z)\n        return x_hat, logits\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x_hat, logits = self(x)\n        loss_recon = self.recon_loss(x_hat, x)\n        loss_cls = self.class_loss(logits, y)\n        loss = loss_recon + loss_cls\n\n        preds = torch.argmax(logits, dim=1)\n        self.train_acc.update(preds, y)\n\n        self.log(\"train_loss_epoch\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"train_acc_epoch\", self.train_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n        # self.log(\"train_acc_epoch\", self.train_acc.compute()*100, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        x_hat, logits = self(x)\n        loss_recon = self.recon_loss(x_hat, x)\n        loss_cls = self.class_loss(logits, y)\n        loss = loss_recon + loss_cls\n\n        preds = torch.argmax(logits, dim=1)\n        self.val_acc.update(preds, y)\n\n        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        # self.log(\"val_acc\", self.val_acc*100, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"val_acc\", self.val_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        x_hat, logits = self(x)\n        loss_recon = self.recon_loss(x_hat, x)\n        # loss_cls = self.class_loss(logits, y)\n        loss = loss_recon \n        # loss = loss_recon + loss_cls\n\n        preds = torch.argmax(logits, dim=1)\n        self.test_acc.update(preds, y)\n\n        self.log(\"test_loss\", loss, on_step=False, on_epoch=True)\n        # print(\"loss: \",loss ,\".\")\n        # self.log(\"test_acc\", self.test_acc, on_step=False, on_epoch=True)\n        self.log(\"test_acc\", self.test_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n        return {\"loss\": loss}\n\n    def configure_optimizers(self):\n        return optim.AdamW(\n            self.parameters(),\n            lr=self.hparams.config.model.learning_rate,\n            weight_decay=self.hparams.config.model.weight_decay\n        )\n\n\nclass NIDSDataModule(pl.LightningDataModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.batch_size = config.training.batch_size\n        self.sequence_length = config.training.sequence_length\n        self.num_workers = config.data.num_workers\n        self.oversample = config.training.oversample\n        self.max_train_samples = config.training.max_train_samples\n        self.max_val_samples = config.training.max_val_samples\n        self.max_test_samples = config.training.max_test_samples\n\n    def prepare_data(self):\n        # /kaggle/input/cic-ton-iot-parquet\n        # cic_ids_2017\n        # /kaggle/input/cic-ids-2017-parquet\n        # df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n        df = pd.read_parquet(os.path.join('/kaggle/input/cic-ton-iot-parquet', self.config.data.raw))\n    \n        # Clean data\n        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df.dropna(inplace=True)\n        df.drop_duplicates(inplace=True)\n    \n        # Identify non-numeric columns\n        self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', \n                               'Src Port', 'Attack', 'Dst IP', 'Dst Port', 'Protocol']\n        self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n    \n        # Encode labels\n        self.label_encoder = LabelEncoder()\n        df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n        self.classes = self.label_encoder.classes_\n    \n        # Initialize scaler\n        self.scaler = StandardScaler()\n    \n        # Stratified split with sample limits\n        train_df, test_df = train_test_split(\n            df,\n            test_size=0.3,  # 70% train, 30% test+val\n            random_state=42,\n            stratify=df['Label_Num']\n        )\n        \n        # Further split test into val and test\n        val_df, test_df = train_test_split(\n            test_df,\n            test_size=0.5,  # 15% val, 15% test\n            random_state=42,\n            stratify=test_df['Label_Num']\n        )\n    \n        # Process each split with sample limits\n        self.X_train, self.y_train = self._prepare_features(train_df, fit=True)\n        self.X_val, self.y_val = self._prepare_features(val_df, fit=False)\n        self.X_test, self.y_test = self._prepare_features(test_df, fit=False)\n        \n        # Apply sample limits\n        self._limit_samples()\n\n    def _limit_samples(self):\n        \"\"\"Limit samples according to configuration\"\"\" # --------------- unused\n        # # Training data\n        # if len(self.X_train) > self.max_train_samples:\n        #     indices = np.random.choice(len(self.X_train), self.max_train_samples, replace=False)\n        #     self.X_train = self.X_train[indices]\n        #     self.y_train = self.y_train[indices]\n        \n        # # Validation data\n        # if len(self.X_val) > self.max_val_samples:\n        #     indices = np.random.choice(len(self.X_val), self.max_val_samples, replace=False)\n        #     self.X_val = self.X_val[indices]\n        #     self.y_val = self.y_val[indices]\n        \n        # # Test data\n        # if len(self.X_test) > self.max_test_samples:\n        #     indices = np.random.choice(len(self.X_test), self.max_test_samples, replace=False)\n        #     self.X_test = self.X_test[indices]\n        #     self.y_test = self.y_test[indices]\n        pass\n    \n    def _prepare_features(self, df, fit=False):\n        X = df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n        y = df['Label_Num']\n        if fit:\n            X = self.scaler.fit_transform(X)\n        else:\n            X = self.scaler.transform(X)\n        # return self.create_sequences(X, y)\n        return X, y\n\n    def create_sequences(self, X, y):\n        \"\"\"Create sequence\"\"\" # ---------------- unused\n        sequences = []\n        labels = []\n        for i in range(len(X) - self.sequence_length):\n            sequences.append(X[i:i+self.sequence_length])\n            labels.append(y.iloc[i+self.sequence_length-1])\n        return np.array(sequences), np.array(labels)\n    \n    # def setup(self, stage=None):\n    #     self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n    #     self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n    #     self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n        \n    #     print(f\"Training samples: {len(self.train_dataset)} (limited to {self.max_train_samples})\")\n    #     print(f\"Validation samples: {len(self.val_dataset)} (limited to {self.max_val_samples})\")\n    #     print(f\"Test samples: {len(self.test_dataset)} (limited to {self.max_test_samples})\")\n\n    def setup(self, stage=None):\n        self.scaler = StandardScaler()\n        self.scaler.fit(self.X_train.reshape(-1, self.X_train.shape[-1]))\n        \n        self.X_train = self.scaler.transform(self.X_train.reshape(-1, self.X_train.shape[-1])).reshape(self.X_train.shape)\n        self.X_val = self.scaler.transform(self.X_val.reshape(-1, self.X_val.shape[-1])).reshape(self.X_val.shape)\n        self.X_test = self.scaler.transform(self.X_test.reshape(-1, self.X_test.shape[-1])).reshape(self.X_test.shape)\n        \n        # self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n        # self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n        # self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n\n        self.train_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_train),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_train.values)\n        )\n        \n        self.val_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_val),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_val.values)\n        )\n        \n        self.test_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_test),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_test.values)\n        )\n\n        print(\"the model will be trained on: \",len(self.train_dataset),\" samples.\")\n        print(\"the model will be validated on: \",len(self.val_dataset),\" samples.\")\n        print(\"the model will be tested on: \",len(self.test_dataset),\" samples.\")\n        \n    def train_dataloader(self):\n        if self.oversample:\n            class_counts = np.bincount(self.y_train)\n            weights = 1. / class_counts[self.y_train]\n            sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n        else:\n            sampler = RandomSampler(self.train_dataset)\n            \n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            sampler=sampler,\n            num_workers=self.num_workers,\n            persistent_workers=True,\n            pin_memory=True\n        )\n    \n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n\ndef init_wandb():\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"mohammad_wandb_secret\")\n    wandb.login(key=wandb_api_key)\n    \n    run = wandb.init(\n        project=config.wandb.project,\n        entity=config.wandb.entity,\n        tags=config.wandb.tags,\n        notes=config.wandb.notes,\n        config={\n            \"input_size\": None,\n            \"num_classes\": None,\n            \"sequence_length\": config.training.sequence_length,\n            # \"train_samples\": config.training.max_train_samples,\n            # \"val_samples\": config.training.max_val_samples,\n            # \"test_samples\": config.training.max_test_samples,\n            \"model_config\": dict(config.model),\n            \"training_config\": dict(config.training)\n        }\n    )\n    \n    wandb_logger = WandbLogger(\n        experiment=run,\n        log_model='all'\n    )\n    \n    return wandb_logger, run\n\ndef main():\n    wandb_logger, run = init_wandb()\n    \n    data_module = NIDSDataModule(config)\n    data_module.prepare_data()\n    data_module.setup()\n    \n    sample_x, _ = next(iter(data_module.train_dataloader()))\n    input_size_per_timestep = sample_x.shape[2]  # Features per timestep\n    total_input_size = input_size_per_timestep * config.training.sequence_length\n    num_classes = len(data_module.classes)\n    \n    run.config.update({\n        \"input_size_per_timestep\": input_size_per_timestep,\n        \"total_input_size\": total_input_size,\n        \"num_classes\": num_classes\n    })\n    \n    model = AutoEncoderModel(total_input_size, num_classes, config)\n    \n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=config.training.early_stopping_patience,\n        mode='min'\n    )\n    \n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_loss',\n        mode='min',\n        save_top_k=1,\n        dirpath='checkpoints',\n        filename='best_model'\n    )\n \n    trainer = pl.Trainer(\n        precision=16,\n        logger=wandb_logger,\n        max_epochs=config.training.max_epochs,\n        callbacks=[early_stopping, checkpoint_callback],\n        deterministic=True,\n        gradient_clip_val=1.0,\n        enable_progress_bar=True,\n        log_every_n_steps=1000\n    )\n    \n    trainer.fit(model, datamodule=data_module)\n    \n    test_results = trainer.test(model, datamodule=data_module)\n    \n    # Collect all predictions and targets\n    test_loader = data_module.test_dataloader()\n    all_preds = []\n    all_targets = []\n    \n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            x, y = batch\n            y_hat = model(x)\n            preds = torch.argmax(y_hat, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(y.cpu().numpy())\n    \n    # Calculate metrics\n    test_acc = accuracy_score(all_targets, all_preds)\n    test_f1 = f1_score(all_targets, all_preds, average='weighted')\n    \n    # Log final metrics\n    wandb.log({\n        'test_acc': test_acc,\n        'test_f1': test_f1,\n        'test_loss': test_results[0]['test_loss']\n    })\n    \n    # Enhanced multiclass confusion matrix\n    class_names = data_module.classes.tolist()\n    conf_mat = confusion_matrix(all_targets, all_preds)\n    \n    # Create a custom confusion matrix plot\n    data = []\n    for i in range(len(class_names)):\n        for j in range(len(class_names)):\n            data.append([class_names[i], class_names[j], conf_mat[i, j]])\n    \n    fields = {\n        \"Actual\": \"Actual\",\n        \"Predicted\": \"Predicted\",\n        \"n\": \"Count\"\n    }\n    \n    wandb.log({\n        \"multiclass_confusion_matrix\": wandb.plot_table(\n            \"wandb/confusion_matrix/v1\",\n            wandb.Table(columns=[\"Actual\", \"Predicted\", \"Count\"], data=data),\n            fields,\n            {\"title\": \"Multiclass Confusion Matrix\"}\n        )\n    })\n    \n    # Classification Report\n    report = classification_report(\n        all_targets, all_preds, \n        target_names=class_names,\n        output_dict=True\n    )\n    \n    report_table = wandb.Table(columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n    for class_name in class_names:\n        report_table.add_data(\n            class_name,\n            report[class_name][\"precision\"],\n            report[class_name][\"recall\"],\n            report[class_name][\"f1-score\"],\n            report[class_name][\"support\"]\n        )\n    \n    report_table.add_data(\n        \"Weighted Avg\",\n        report[\"weighted avg\"][\"precision\"],\n        report[\"weighted avg\"][\"recall\"],\n        report[\"weighted avg\"][\"f1-score\"],\n        report[\"weighted avg\"][\"support\"]\n    )\n    \n    wandb.log({\"classification_report\": report_table})\n    \n    wandb.finish()\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T12:13:05.556212Z","iopub.execute_input":"2025-06-28T12:13:05.556753Z","execution_failed":"2025-06-28T13:41:23.652Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohammad-fleity\u001b[0m (\u001b[33mmohammad-fleity-lebanese-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250628_121329-6e1bv1eh</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot/runs/6e1bv1eh' target=\"_blank\">rosy-shape-72</a></strong> to <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot/runs/6e1bv1eh' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot/runs/6e1bv1eh</a>"},"metadata":{}},{"name":"stdout","text":"the model will be trained on:  3745408  samples.\nthe model will be validated on:  802587  samples.\nthe model will be tested on:  802588  samples.\nthe model will be trained on:  3745408  samples.\nthe model will be validated on:  802587  samples.\nthe model will be tested on:  802588  samples.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e042520b6fff49eaa31cbff6d1717e9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"<h1>New Model</h1>","metadata":{}},{"cell_type":"markdown","source":"<h2>First Dataset</h2>","metadata":{}},{"cell_type":"code","source":"# -----------------------------    THIS MODEL WASN'T SUCCESSFULL    ---------------------------- #\n\n\n# import numpy as np\n# import pandas as pd\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# from torch.utils.data import DataLoader,Dataset ,TensorDataset, WeightedRandomSampler, RandomSampler\n# from sklearn.preprocessing import StandardScaler, LabelEncoder\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n# import pytorch_lightning as pl\n# from pytorch_lightning.loggers import WandbLogger\n# from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n# import wandb\n# from omegaconf import OmegaConf\n# import os\n# import torchmetrics\n# import warnings\n# from kaggle_secrets import UserSecretsClient\n\n# warnings.filterwarnings('ignore')\n\n# config = OmegaConf.create({\n#     \"wandb\": {\n#         \"project\": \"DL-NIDS-2--cic-ids-2017\",\n#         \"entity\": \"mohammad-fleity-lebanese-university\",\n#         \"tags\": [\"AutoEncoderDecoder\", \"cic-ids-2017\", \"PyTorch\"],\n#         # \"project\": \"DL-NIDS-2--cic-ton-iot\",\n#         # \"entity\": \"mohammad-fleity-lebanese-university\",\n#         # \"tags\": [\"AutoEncoderDecoder\", \"cic-ton-iot\", \"PyTorch\"],\n#         \"notes\": \"AutoEncoderDecoder for network intrusion detection\"\n#     },\n#     \"model\": {\n#         \"hidden_size\": 128,\n#         \"num_layers\": 2,\n#         \"dropout\": 0.4,\n#         \"dense_units\": [128, 64],\n#         \"learning_rate\": 0.0001,\n#         \"weight_decay\": 1e-4\n#     },\n#     \"training\": {\n#         \"sequence_length\": 5,\n#         \"batch_size\": 128,\n#         \"max_epochs\": 10,            \n#         \"early_stopping_patience\": 7,\n#         \"oversample\": True,\n#         \"gpus\": 1 if torch.cuda.is_available() else 0,\n#         \"max_train_samples\": 200000,  # Maximum training samples ---------------- unused\n#         \"max_val_samples\": 30000,     # Maximum validation samples -------------- unused\n#         \"max_test_samples\": 30000     # Maximum test samples -------------------- unused\n#     },\n#     \"data\": {\n#         \"raw\": \"cic_ids_2017.parquet\",\n#         \"num_workers\": 4\n#         # \"raw\": \"cic_ton_iot.parquet\",\n#         # \"num_workers\": 4\n#     }\n# })\n\n# class TimeSeriesDataset(Dataset):\n#     def __init__(self, data, sequence_length, target_idx=None):\n#         \"\"\"\n#         Dataset for time series data that creates sequences on-the-fly.\n \n#         Args:\n#             data: Input data tensor of shape (n_samples, n_features)\n#             sequence_length: Length of sequences to create\n#             target_idx: Optional tensor of target indices. If None, uses the last position in sequence\n#         \"\"\"\n#         self.data = data\n#         self.sequence_length = sequence_length\n#         self.target_idx = target_idx\n \n#     def __len__(self):\n#         return len(self.data)\n \n#     def __getitem__(self, idx):\n#         # Calculate start index for the sequence\n#         start_idx = max(0, idx - self.sequence_length + 1)\n        \n#         # Get the sequence\n#         sequence = self.data[start_idx:idx + 1]\n        \n#         # Ensure sequence is 2D [seq_len, features]\n#         if sequence.dim() == 1:\n#             sequence = sequence.unsqueeze(0)  # Add sequence length dimension if missing\n        \n#         # Pad the beginning if needed\n#         if len(sequence) < self.sequence_length:\n#             padding = torch.zeros(self.sequence_length - len(sequence), sequence.shape[1])\n#             sequence = torch.cat([padding, sequence], dim=0)\n        \n#         # Get target\n#         target = self.target_idx[idx] if self.target_idx is not None else -1\n        \n#         return sequence, target\n\n# # class AutoEncoderModel(pl.LightningModule):\n# #     def __init__(self, input_size, num_classes, config):\n# #         super().__init__()\n# #         self.save_hyperparameters({'config': config})\n# #         self.config = config\n        \n# #         self.encoder = nn.Sequential(\n# #             nn.Linear(input_size, 128),\n# #             nn.ReLU(),\n# #             nn.Dropout(config.model.dropout),\n# #             nn.Linear(128, 64),\n# #             nn.ReLU(),\n# #             nn.Dropout(config.model.dropout),\n# #             nn.Linear(64, 32)\n# #         )\n\n# #         self.decoder = nn.Sequential(\n# #             nn.Linear(32, 64),\n# #             nn.ReLU(),\n# #             nn.Dropout(config.model.dropout),\n# #             nn.Linear(64, 128),\n# #             nn.ReLU(),\n# #             nn.Dropout(config.model.dropout),\n# #             nn.Linear(128, input_size)  # match input size for reconstruction\n# #         )\n\n# #         self.classifier = nn.Sequential(\n# #             nn.Linear(32, 64),\n# #             nn.ReLU(),\n# #             nn.Dropout(config.model.dropout),\n# #             nn.Linear(64, num_classes)\n# #         )\n\n# #         self.recon_loss = nn.MSELoss()\n# #         self.class_loss = nn.CrossEntropyLoss()\n\n# #         # Metrics\n# #         self.train_acc = torchmetrics.classification.Accuracy(task='multiclass', num_classes=num_classes)\n# #         self.val_acc = torchmetrics.classification.Accuracy(task='multiclass', num_classes=num_classes)\n# #         self.test_acc = torchmetrics.classification.Accuracy(task='multiclass', num_classes=num_classes)\n\n# #     def forward(self, x):\n# #         batch_size, seq_len, features = x.shape\n# #         x_flat = x.view(batch_size, -1)\n# #         z = self.encoder(x_flat)\n\n# #         x_hat = self.decoder(z).view(batch_size, seq_len, features)\n# #         logits = self.classifier(z)\n# #         return x_hat, logits\n\n# #     def training_step(self, batch, batch_idx):\n# #         x, y = batch\n# #         x_hat, logits = self(x)\n# #         loss_recon = self.recon_loss(x_hat, x)\n# #         loss_cls = self.class_loss(logits, y)\n# #         loss = loss_recon + loss_cls\n\n# #         preds = torch.argmax(logits, dim=1)\n# #         self.train_acc.update(preds, y)\n\n# #         self.log(\"train_loss_epoch\", loss, on_step=False, on_epoch=True, prog_bar=True)\n# #         self.log(\"train_acc_epoch\", self.train_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n# #         # self.log(\"train_acc_epoch\", self.train_acc.compute()*100, on_step=False, on_epoch=True, prog_bar=True)\n# #         return loss\n\n# #     def validation_step(self, batch, batch_idx):\n# #         x, y = batch\n# #         x_hat, logits = self(x)\n# #         loss_recon = self.recon_loss(x_hat, x)\n# #         loss_cls = self.class_loss(logits, y)\n# #         loss = loss_recon + loss_cls\n\n# #         preds = torch.argmax(logits, dim=1)\n# #         self.val_acc.update(preds, y)\n\n# #         self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n# #         # self.log(\"val_acc\", self.val_acc*100, on_step=False, on_epoch=True, prog_bar=True)\n# #         self.log(\"val_acc\", self.val_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n# #         return loss\n\n\n# #     def test_step(self, batch, batch_idx):\n# #         x, y = batch\n# #         x_hat, logits = self(x)\n# #         loss_recon = self.recon_loss(x_hat, x)\n# #         # loss_cls = self.class_loss(logits, y)\n# #         loss = loss_recon \n# #         # loss = loss_recon + loss_cls\n\n# #         preds = torch.argmax(logits, dim=1)\n# #         self.test_acc.update(preds, y)\n\n# #         self.log(\"test_loss\", loss, on_step=False, on_epoch=True)\n# #         # print(\"loss: \",loss ,\".\")\n# #         # self.log(\"test_acc\", self.test_acc, on_step=False, on_epoch=True)\n# #         self.log(\"test_acc\", self.test_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n# #         return {\"loss\": loss}\n\n# #     def configure_optimizers(self):\n# #         return optim.AdamW(\n# #             self.parameters(),\n# #             lr=self.hparams.config.model.learning_rate,\n# #             weight_decay=self.hparams.config.model.weight_decay\n# #         )\n\n\n\n# class AutoEncoderModel(pl.LightningModule):\n#     def __init__(self, input_size, num_classes, config):\n#         super().__init__()\n#         self.save_hyperparameters({'config': config})\n#         self.config = config\n\n#         # ----- Encoder -----\n#         self.encoder = nn.Sequential(\n#             nn.Linear(input_size, 128),\n#             nn.BatchNorm1d(128),\n#             nn.ReLU(),\n#             nn.Dropout(config.model.dropout),\n\n#             nn.Linear(128, 64),\n#             nn.BatchNorm1d(64),\n#             nn.ReLU(),\n#             nn.Dropout(config.model.dropout),\n\n#             nn.Linear(64, 32),\n#             nn.BatchNorm1d(32),  # Optional: normalize latent space\n#         )\n\n#         # ----- Decoder -----\n#         self.decoder = nn.Sequential(\n#             nn.Linear(32, 64),\n#             nn.BatchNorm1d(64),\n#             nn.ReLU(),\n#             nn.Dropout(config.model.dropout),\n\n#             nn.Linear(64, 128),\n#             nn.BatchNorm1d(128),\n#             nn.ReLU(),\n#             nn.Dropout(config.model.dropout),\n\n#             nn.Linear(128, input_size)\n#         )\n\n#         # ----- Classifier -----\n#         self.classifier = nn.Sequential(\n#             nn.Linear(32, 64),\n#             nn.BatchNorm1d(64),\n#             nn.ReLU(),\n#             nn.Dropout(config.model.dropout),\n\n#             nn.Linear(64, num_classes)\n#         )\n\n#         # Loss functions\n#         self.recon_loss = nn.MSELoss()\n#         self.class_loss = nn.CrossEntropyLoss()\n\n#         # Metrics\n#         self.train_acc = torchmetrics.classification.Accuracy(task='multiclass', num_classes=num_classes)\n#         self.val_acc = torchmetrics.classification.Accuracy(task='multiclass', num_classes=num_classes)\n#         self.test_acc = torchmetrics.classification.Accuracy(task='multiclass', num_classes=num_classes)\n\n#     def forward(self, x):\n#         batch_size, seq_len, features = x.shape\n#         x_flat = x.view(batch_size, -1)\n#         z = self.encoder(x_flat)\n\n#         x_hat = self.decoder(z).view(batch_size, seq_len, features)\n#         logits = self.classifier(z)\n#         return x_hat, logits\n\n#     def training_step(self, batch, batch_idx):\n#         x, y = batch\n#         x_hat, logits = self(x)\n#         loss_recon = self.recon_loss(x_hat, x)\n#         loss_cls = self.class_loss(logits, y)\n#         loss = loss_recon + loss_cls\n\n#         preds = torch.argmax(logits, dim=1)\n#         self.train_acc.update(preds, y)\n\n#         self.log(\"train_loss_epoch\", loss, on_step=False, on_epoch=True, prog_bar=True)\n#         self.log(\"train_acc_epoch\", self.train_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n#         return loss\n\n#     def validation_step(self, batch, batch_idx):\n#         x, y = batch\n#         x_hat, logits = self(x)\n#         loss_recon = self.recon_loss(x_hat, x)\n#         loss_cls = self.class_loss(logits, y)\n#         loss = loss_recon + loss_cls\n\n#         preds = torch.argmax(logits, dim=1)\n#         self.val_acc.update(preds, y)\n\n#         self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n#         self.log(\"val_acc\", self.val_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n#         return loss\n\n#     def test_step(self, batch, batch_idx):\n#         x, y = batch\n#         x_hat, logits = self(x)\n#         loss = self.recon_loss(x_hat, x)  # classification loss optional in test\n\n#         preds = torch.argmax(logits, dim=1)\n#         self.test_acc.update(preds, y)\n\n#         self.log(\"test_loss\", loss, on_step=False, on_epoch=True)\n#         self.log(\"test_acc\", self.test_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n#         return {\"loss\": loss}\n\n#     def configure_optimizers(self):\n#         return optim.AdamW(\n#             self.parameters(),\n#             lr=self.hparams.config.model.learning_rate,\n#             weight_decay=self.hparams.config.model.weight_decay\n#         )\n\n# class NIDSDataModule(pl.LightningDataModule):\n#     def __init__(self, config):\n#         super().__init__()\n#         self.config = config\n#         self.batch_size = config.training.batch_size\n#         self.sequence_length = config.training.sequence_length\n#         self.num_workers = config.data.num_workers\n#         self.oversample = config.training.oversample\n#         self.max_train_samples = config.training.max_train_samples\n#         self.max_val_samples = config.training.max_val_samples\n#         self.max_test_samples = config.training.max_test_samples\n\n#     def prepare_data(self):\n#         # /kaggle/input/cic-ton-iot-parquet\n#         # cic_ids_2017\n#         # /kaggle/input/cic-ids-2017-parquet\n#         df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n#         # df = pd.read_parquet(os.path.join('/kaggle/input/cic-ton-iot-parquet', self.config.data.raw))\n    \n#         # Clean data\n#         df.replace([np.inf, -np.inf], np.nan, inplace=True)\n#         df.dropna(inplace=True)\n#         df.drop_duplicates(inplace=True)\n    \n#         # Identify non-numeric columns\n#         self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', \n#                                'Src Port', 'Attack', 'Dst IP', 'Dst Port', 'Protocol']\n#         self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n    \n#         # Encode labels\n#         self.label_encoder = LabelEncoder()\n#         df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n#         self.classes = self.label_encoder.classes_\n    \n#         # Initialize scaler\n#         self.scaler = StandardScaler()\n    \n#         # Stratified split with sample limits\n#         train_df, test_df = train_test_split(\n#             df,\n#             test_size=0.3,  # 70% train, 30% test+val\n#             random_state=42,\n#             stratify=df['Label_Num']\n#         )\n        \n#         # Further split test into val and test\n#         val_df, test_df = train_test_split(\n#             test_df,\n#             test_size=0.5,  # 15% val, 15% test\n#             random_state=42,\n#             stratify=test_df['Label_Num']\n#         )\n    \n#         # Process each split with sample limits\n#         self.X_train, self.y_train = self._prepare_features(train_df, fit=True)\n#         self.X_val, self.y_val = self._prepare_features(val_df, fit=False)\n#         self.X_test, self.y_test = self._prepare_features(test_df, fit=False)\n        \n#         # Apply sample limits\n#         self._limit_samples()\n\n#     def _limit_samples(self):\n#         \"\"\"Limit samples according to configuration\"\"\" # --------------- unused\n#         # # Training data\n#         # if len(self.X_train) > self.max_train_samples:\n#         #     indices = np.random.choice(len(self.X_train), self.max_train_samples, replace=False)\n#         #     self.X_train = self.X_train[indices]\n#         #     self.y_train = self.y_train[indices]\n        \n#         # # Validation data\n#         # if len(self.X_val) > self.max_val_samples:\n#         #     indices = np.random.choice(len(self.X_val), self.max_val_samples, replace=False)\n#         #     self.X_val = self.X_val[indices]\n#         #     self.y_val = self.y_val[indices]\n        \n#         # # Test data\n#         # if len(self.X_test) > self.max_test_samples:\n#         #     indices = np.random.choice(len(self.X_test), self.max_test_samples, replace=False)\n#         #     self.X_test = self.X_test[indices]\n#         #     self.y_test = self.y_test[indices]\n#         pass\n    \n#     def _prepare_features(self, df, fit=False):\n#         X = df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n#         y = df['Label_Num']\n#         if fit:\n#             X = self.scaler.fit_transform(X)\n#         else:\n#             X = self.scaler.transform(X)\n#         # return self.create_sequences(X, y)\n#         return X, y\n\n#     def create_sequences(self, X, y):\n#         \"\"\"Create sequence\"\"\" # ---------------- unused\n#         sequences = []\n#         labels = []\n#         for i in range(len(X) - self.sequence_length):\n#             sequences.append(X[i:i+self.sequence_length])\n#             labels.append(y.iloc[i+self.sequence_length-1])\n#         return np.array(sequences), np.array(labels)\n    \n#     # def setup(self, stage=None):\n#     #     self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n#     #     self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n#     #     self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n        \n#     #     print(f\"Training samples: {len(self.train_dataset)} (limited to {self.max_train_samples})\")\n#     #     print(f\"Validation samples: {len(self.val_dataset)} (limited to {self.max_val_samples})\")\n#     #     print(f\"Test samples: {len(self.test_dataset)} (limited to {self.max_test_samples})\")\n\n#     def setup(self, stage=None):\n#         self.scaler = StandardScaler()\n#         self.scaler.fit(self.X_train.reshape(-1, self.X_train.shape[-1]))\n        \n#         self.X_train = self.scaler.transform(self.X_train.reshape(-1, self.X_train.shape[-1])).reshape(self.X_train.shape)\n#         self.X_val = self.scaler.transform(self.X_val.reshape(-1, self.X_val.shape[-1])).reshape(self.X_val.shape)\n#         self.X_test = self.scaler.transform(self.X_test.reshape(-1, self.X_test.shape[-1])).reshape(self.X_test.shape)\n        \n#         # self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n#         # self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n#         # self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n\n#         self.train_dataset = TimeSeriesDataset(\n#             data=torch.FloatTensor(self.X_train),\n#             sequence_length=self.sequence_length,\n#             target_idx=torch.LongTensor(self.y_train.values)\n#         )\n        \n#         self.val_dataset = TimeSeriesDataset(\n#             data=torch.FloatTensor(self.X_val),\n#             sequence_length=self.sequence_length,\n#             target_idx=torch.LongTensor(self.y_val.values)\n#         )\n        \n#         self.test_dataset = TimeSeriesDataset(\n#             data=torch.FloatTensor(self.X_test),\n#             sequence_length=self.sequence_length,\n#             target_idx=torch.LongTensor(self.y_test.values)\n#         )\n\n#         print(\"the model will be trained on: \",len(self.train_dataset),\" samples.\")\n#         print(\"the model will be validated on: \",len(self.val_dataset),\" samples.\")\n#         print(\"the model will be tested on: \",len(self.test_dataset),\" samples.\")\n        \n#     def train_dataloader(self):\n#         if self.oversample:\n#             class_counts = np.bincount(self.y_train)\n#             weights = 1. / class_counts[self.y_train]\n#             sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n#         else:\n#             sampler = RandomSampler(self.train_dataset)\n            \n#         return DataLoader(\n#             self.train_dataset,\n#             batch_size=self.batch_size,\n#             sampler=sampler,\n#             num_workers=self.num_workers,\n#             persistent_workers=True,\n#             pin_memory=True\n#         )\n    \n#     def val_dataloader(self):\n#         return DataLoader(\n#             self.val_dataset,\n#             batch_size=self.batch_size,\n#             shuffle=False,\n#             num_workers=self.num_workers,\n#             pin_memory=True\n#         )\n    \n#     def test_dataloader(self):\n#         return DataLoader(\n#             self.test_dataset,\n#             batch_size=self.batch_size,\n#             shuffle=False,\n#             num_workers=self.num_workers,\n#             pin_memory=True\n#         )\n\n# def init_wandb():\n#     user_secrets = UserSecretsClient()\n#     wandb_api_key = user_secrets.get_secret(\"mohammad_wandb_secret\")\n#     wandb.login(key=wandb_api_key)\n    \n#     run = wandb.init(\n#         project=config.wandb.project,\n#         entity=config.wandb.entity,\n#         tags=config.wandb.tags,\n#         notes=config.wandb.notes,\n#         config={\n#             \"input_size\": None,\n#             \"num_classes\": None,\n#             \"sequence_length\": config.training.sequence_length,\n#             # \"train_samples\": config.training.max_train_samples,\n#             # \"val_samples\": config.training.max_val_samples,\n#             # \"test_samples\": config.training.max_test_samples,\n#             \"model_config\": dict(config.model),\n#             \"training_config\": dict(config.training)\n#         }\n#     )\n    \n#     wandb_logger = WandbLogger(\n#         experiment=run,\n#         log_model='all'\n#     )\n    \n#     return wandb_logger, run\n\n# def main():\n#     wandb_logger, run = init_wandb()\n    \n#     data_module = NIDSDataModule(config)\n#     data_module.prepare_data()\n#     data_module.setup()\n    \n#     sample_x, _ = next(iter(data_module.train_dataloader()))\n#     input_size_per_timestep = sample_x.shape[2]  # Features per timestep\n#     total_input_size = input_size_per_timestep * config.training.sequence_length\n#     num_classes = len(data_module.classes)\n    \n#     run.config.update({\n#         \"input_size_per_timestep\": input_size_per_timestep,\n#         \"total_input_size\": total_input_size,\n#         \"num_classes\": num_classes\n#     })\n    \n#     model = AutoEncoderModel(total_input_size, num_classes, config)\n    \n#     early_stopping = EarlyStopping(\n#         monitor='val_loss',\n#         patience=config.training.early_stopping_patience,\n#         mode='min'\n#     )\n    \n#     checkpoint_callback = ModelCheckpoint(\n#         monitor='val_loss',\n#         mode='min',\n#         save_top_k=1,\n#         dirpath='checkpoints',\n#         filename='best_model'\n#     )\n \n#     trainer = pl.Trainer(\n#         precision=16,\n#         logger=wandb_logger,\n#         max_epochs=config.training.max_epochs,\n#         callbacks=[early_stopping, checkpoint_callback],\n#         deterministic=True,\n#         gradient_clip_val=1.0,\n#         enable_progress_bar=True,\n#         log_every_n_steps=1000\n#     )\n    \n#     trainer.fit(model, datamodule=data_module)\n    \n#     test_results = trainer.test(model, datamodule=data_module)\n    \n#     # Collect all predictions and targets\n#     test_loader = data_module.test_dataloader()\n#     all_preds = []\n#     all_targets = []\n    \n#     model.eval()\n#     with torch.no_grad():\n#         for batch in test_loader:\n#             x, y = batch\n#             y_hat = model(x)\n#             preds = torch.argmax(y_hat, dim=1)\n#             all_preds.extend(preds.cpu().numpy())\n#             all_targets.extend(y.cpu().numpy())\n    \n#     # Calculate metrics\n#     test_acc = accuracy_score(all_targets, all_preds)\n#     test_f1 = f1_score(all_targets, all_preds, average='weighted')\n    \n#     # Log final metrics\n#     wandb.log({\n#         'test_acc': test_acc,\n#         'test_f1': test_f1,\n#         'test_loss': test_results[0]['test_loss']\n#     })\n    \n#     # Enhanced multiclass confusion matrix\n#     class_names = data_module.classes.tolist()\n#     conf_mat = confusion_matrix(all_targets, all_preds)\n    \n#     # Create a custom confusion matrix plot\n#     data = []\n#     for i in range(len(class_names)):\n#         for j in range(len(class_names)):\n#             data.append([class_names[i], class_names[j], conf_mat[i, j]])\n    \n#     fields = {\n#         \"Actual\": \"Actual\",\n#         \"Predicted\": \"Predicted\",\n#         \"n\": \"Count\"\n#     }\n    \n#     wandb.log({\n#         \"multiclass_confusion_matrix\": wandb.plot_table(\n#             \"wandb/confusion_matrix/v1\",\n#             wandb.Table(columns=[\"Actual\", \"Predicted\", \"Count\"], data=data),\n#             fields,\n#             {\"title\": \"Multiclass Confusion Matrix\"}\n#         )\n#     })\n    \n#     # Classification Report\n#     report = classification_report(\n#         all_targets, all_preds, \n#         target_names=class_names,\n#         output_dict=True\n#     )\n    \n#     report_table = wandb.Table(columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n#     for class_name in class_names:\n#         report_table.add_data(\n#             class_name,\n#             report[class_name][\"precision\"],\n#             report[class_name][\"recall\"],\n#             report[class_name][\"f1-score\"],\n#             report[class_name][\"support\"]\n#         )\n    \n#     report_table.add_data(\n#         \"Weighted Avg\",\n#         report[\"weighted avg\"][\"precision\"],\n#         report[\"weighted avg\"][\"recall\"],\n#         report[\"weighted avg\"][\"f1-score\"],\n#         report[\"weighted avg\"][\"support\"]\n#     )\n    \n#     wandb.log({\"classification_report\": report_table})\n    \n#     wandb.finish()\n\n# if __name__ == \"__main__\":\n#     main()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T16:31:10.328266Z","iopub.execute_input":"2025-06-29T16:31:10.328553Z","iopub.status.idle":"2025-06-29T17:17:14.376034Z","shell.execute_reply.started":"2025-06-29T16:31:10.328529Z","shell.execute_reply":"2025-06-29T17:17:14.374989Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohammad-fleity\u001b[0m (\u001b[33mmohammad-fleity-lebanese-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250629_163143-nud6rv3w</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/nud6rv3w' target=\"_blank\">stellar-fog-91</a></strong> to <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/nud6rv3w' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/nud6rv3w</a>"},"metadata":{}},{"name":"stdout","text":"the model will be trained on:  1979373  samples.\nthe model will be validated on:  424152  samples.\nthe model will be tested on:  424152  samples.\nthe model will be trained on:  1979373  samples.\nthe model will be validated on:  424152  samples.\nthe model will be tested on:  424152  samples.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fccfcec27db24441a6b2617f9dfd7210"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"the model will be trained on:  1979373  samples.\nthe model will be validated on:  424152  samples.\nthe model will be tested on:  424152  samples.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d208bb9e77014e52a6e50e219be6a3b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    95.40021514892578    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9019314646720886    \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     95.40021514892578     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9019314646720886     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1272574408.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/1272574408.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m             \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m             \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0mall_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: argmax(): argument 'input' (position 1) must be Tensor, not tuple"],"ename":"TypeError","evalue":"argmax(): argument 'input' (position 1) must be Tensor, not tuple","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"\n\nclass AutoEncoderModel(pl.LightningModule):\n    def __init__(self, input_size, num_classes, config):\n        super().__init__()\n        self.save_hyperparameters({'config': config})\n        self.config = config\n\n        # ----- Encoder -----\n        self.encoder = nn.Sequential(\n            nn.Linear(input_size, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n\n            nn.Linear(128, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n\n            nn.Linear(64, 32),\n            nn.BatchNorm1d(32),  # Optional: normalize latent space\n        )\n\n        # ----- Decoder -----\n        self.decoder = nn.Sequential(\n            nn.Linear(32, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n\n            nn.Linear(64, 128),\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n\n            nn.Linear(128, input_size)\n        )\n\n        # ----- Classifier -----\n        self.classifier = nn.Sequential(\n            nn.Linear(32, 64),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n\n            nn.Linear(64, num_classes)\n        )\n\n        # Loss functions\n        self.recon_loss = nn.MSELoss()\n        self.class_loss = nn.CrossEntropyLoss()\n\n        # Metrics\n        self.train_acc = torchmetrics.classification.Accuracy(task='multiclass', num_classes=num_classes)\n        self.val_acc = torchmetrics.classification.Accuracy(task='multiclass', num_classes=num_classes)\n        self.test_acc = torchmetrics.classification.Accuracy(task='multiclass', num_classes=num_classes)\n\n    def forward(self, x):\n        batch_size, seq_len, features = x.shape\n        x_flat = x.view(batch_size, -1)\n        z = self.encoder(x_flat)\n\n        x_hat = self.decoder(z).view(batch_size, seq_len, features)\n        logits = self.classifier(z)\n        return x_hat, logits\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x_hat, logits = self(x)\n        loss_recon = self.recon_loss(x_hat, x)\n        loss_cls = self.class_loss(logits, y)\n        loss = loss_recon + loss_cls\n\n        preds = torch.argmax(logits, dim=1)\n        self.train_acc.update(preds, y)\n\n        self.log(\"train_loss_epoch\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"train_acc_epoch\", self.train_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        x_hat, logits = self(x)\n        loss_recon = self.recon_loss(x_hat, x)\n        loss_cls = self.class_loss(logits, y)\n        loss = loss_recon + loss_cls\n\n        preds = torch.argmax(logits, dim=1)\n        self.val_acc.update(preds, y)\n\n        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log(\"val_acc\", self.val_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        x_hat, logits = self(x)\n        loss = self.recon_loss(x_hat, x)  # classification loss optional in test\n\n        preds = torch.argmax(logits, dim=1)\n        self.test_acc.update(preds, y)\n\n        self.log(\"test_loss\", loss, on_step=False, on_epoch=True)\n        self.log(\"test_acc\", self.test_acc.compute() * 100, on_step=False, on_epoch=True, prog_bar=True)\n        return {\"loss\": loss}\n\n    def configure_optimizers(self):\n        return optim.AdamW(\n            self.parameters(),\n            lr=self.hparams.config.model.learning_rate,\n            weight_decay=self.hparams.config.model.weight_decay\n        )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}