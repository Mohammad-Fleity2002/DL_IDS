{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8089266,"sourceType":"datasetVersion","datasetId":4775518},{"sourceId":8089281,"sourceType":"datasetVersion","datasetId":4775527}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --quiet wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T08:59:50.319722Z","iopub.execute_input":"2025-06-17T08:59:50.320299Z","iopub.status.idle":"2025-06-17T08:59:53.940308Z","shell.execute_reply.started":"2025-06-17T08:59:50.320277Z","shell.execute_reply":"2025-06-17T08:59:53.939513Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import wandb\nprint(wandb.__version__)\n# from wandb.keras import WandbCallback\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T09:05:23.895873Z","iopub.execute_input":"2025-06-17T09:05:23.896079Z","iopub.status.idle":"2025-06-17T09:05:26.682338Z","shell.execute_reply.started":"2025-06-17T09:05:23.896060Z","shell.execute_reply":"2025-06-17T09:05:26.681030Z"}},"outputs":[{"name":"stdout","text":"0.19.9\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\n\nclass LSTMIDS(pl.LightningModule):\n    def __init__(self, input_size, hidden_size, dense_units, num_classes, learning_rate):\n        super().__init__()\n        self.save_hyperparameters()\n\n        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n        self.dropout = nn.Dropout(0.3)\n        self.batch_norm = nn.BatchNorm1d(hidden_size)\n\n        layers = []\n        prev_units = hidden_size\n        for units in dense_units:\n            layers.extend([\n                nn.Linear(prev_units, units),\n                nn.ReLU(),\n                nn.BatchNorm1d(units),\n                nn.Dropout(0.3)\n            ])\n            prev_units = units\n\n        self.dense_block = nn.Sequential(*layers)\n        self.output = nn.Linear(prev_units, num_classes)\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        x = lstm_out[:, -1, :]\n        x = self.batch_norm(x)\n        x = self.dropout(x)\n        x = self.dense_block(x)\n        return self.output(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss_fn(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log(\"train_loss\", loss)\n        self.log(\"train_acc\", acc, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.loss_fn(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log(\"val_loss\", loss)\n        self.log(\"val_acc\", acc, prog_bar=True)\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T09:10:09.791605Z","iopub.execute_input":"2025-06-17T09:10:09.791903Z","iopub.status.idle":"2025-06-17T09:10:09.801166Z","shell.execute_reply.started":"2025-06-17T09:10:09.791883Z","shell.execute_reply":"2025-06-17T09:10:09.800604Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\nclass IDSDataModule(pl.LightningDataModule):\n    def __init__(self, X, y, sequence_length, batch_size):\n        super().__init__()\n        self.sequence_length = sequence_length\n        self.batch_size = batch_size\n        self.X, self.y = X, y\n\n    def setup(self, stage=None):\n        X_train, X_val, y_train, y_val = train_test_split(self.X, self.y, test_size=0.2, stratify=self.y)\n        self.train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n                                           torch.tensor(y_train, dtype=torch.long))\n        self.val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n                                         torch.tensor(y_val, dtype=torch.long))\n\n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T09:10:12.637804Z","iopub.execute_input":"2025-06-17T09:10:12.638100Z","iopub.status.idle":"2025-06-17T09:10:12.644959Z","shell.execute_reply.started":"2025-06-17T09:10:12.638076Z","shell.execute_reply":"2025-06-17T09:10:12.644133Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_parquet('/kaggle/input/cic-ids-2017-parquet/cic_ids_2017.parquet')\ndf.head()\n# df.columns_type","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:43:28.767580Z","iopub.execute_input":"2025-06-18T14:43:28.768294Z","iopub.status.idle":"2025-06-18T14:43:40.277185Z","shell.execute_reply.started":"2025-06-18T14:43:28.768271Z","shell.execute_reply":"2025-06-18T14:43:40.276637Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"262024980"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"df.columns\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:48:12.319586Z","iopub.execute_input":"2025-06-18T14:48:12.320196Z","iopub.status.idle":"2025-06-18T14:48:12.327061Z","shell.execute_reply.started":"2025-06-18T14:48:12.320173Z","shell.execute_reply":"2025-06-18T14:48:12.326536Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"Index(['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol',\n       'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts',\n       'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max',\n       'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std',\n       'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean',\n       'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean',\n       'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot',\n       'Fwd IAT Mean', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot',\n       'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min',\n       'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags',\n       'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s',\n       'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std',\n       'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt',\n       'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count',\n       'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg',\n       'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg',\n       'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg',\n       'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts',\n       'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts',\n       'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min',\n       'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\n       'Idle Std', 'Idle Max', 'Idle Min', 'Label', 'Attack'],\n      dtype='object')"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:48:23.824001Z","iopub.execute_input":"2025-06-18T14:48:23.824597Z","iopub.status.idle":"2025-06-18T14:48:23.829130Z","shell.execute_reply.started":"2025-06-18T14:48:23.824574Z","shell.execute_reply":"2025-06-18T14:48:23.828401Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(3119345, 84)"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nimport wandb\nfrom omegaconf import OmegaConf\nimport os\nimport pickle\nimport warnings\nfrom kaggle_secrets import UserSecretsClient\n\nwarnings.filterwarnings('ignore')\n\n# Configuration\nconfig = OmegaConf.create({\n  \"wandb\": {\n    \"project\": \"DL-NIDS-2--cic-ids-2017\",\n    \"entity\": \"mohammad-fleity-lebanese-university\",\n    \"tags\": [\"LSTM\", \"CIC-IDS-2017\", \"PyTorch\"],\n    \"notes\": \"LSTM model for network intrusion detection with PyTorch Lightning\"\n  },\n  \"model\": {\n    \"name\": \"lstm\",\n    \"hidden_size\": 80,\n    \"num_layers\": 1,\n    \"dropout\": 0.3,\n    \"dense_units\": [80],\n    \"learning_rate\": 0.001,\n    \"weight_decay\": 1e-5\n  },\n  \"training\": {\n    \"sequence_length\": 3,\n    \"batch_size\": 64,\n    \"max_epochs\": 10,\n    \"early_stopping_patience\": 5,\n    \"gpus\": 1 if torch.cuda.is_available() else 0\n  },\n  \"data\": {\n    \"raw\": \"cic_ids_2017.parquet\",\n    \"test_size\": 0.2,\n    \"num_workers\": 4\n  }\n})\n\nclass LSTMModel(pl.LightningModule):\n    def __init__(self, input_size, num_classes, config):\n        super().__init__()\n        self.config = config\n        self.save_hyperparameters()\n        \n        # LSTM Layer\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=config.model.hidden_size,\n            num_layers=config.model.num_layers,\n            batch_first=True,\n            dropout=config.model.dropout if config.model.num_layers > 1 else 0\n        )\n        \n        # Batch normalization after LSTM\n        self.lstm_bn = nn.BatchNorm1d(config.model.hidden_size)\n        \n        # Dropout layer\n        self.dropout = nn.Dropout(config.model.dropout)\n        \n        # Dense layers\n        self.dense_layers = nn.ModuleList()\n        prev_units = config.model.hidden_size\n        for units in config.model.dense_units:\n            self.dense_layers.append(nn.Linear(prev_units, units))\n            self.dense_layers.append(nn.BatchNorm1d(units))\n            self.dense_layers.append(nn.ReLU())\n            self.dense_layers.append(nn.Dropout(config.model.dropout))\n            prev_units = units\n        \n        # Output layer\n        self.output = nn.Linear(prev_units, num_classes)\n        \n        # Loss function\n        self.criterion = nn.CrossEntropyLoss()\n    \n    def forward(self, x):\n        # LSTM layer\n        lstm_out, _ = self.lstm(x)\n        # We only need the last timestep's output for classification\n        lstm_out = lstm_out[:, -1, :]\n        \n        # Batch norm\n        lstm_out = self.lstm_bn(lstm_out)\n        \n        # Dense layers\n        x = lstm_out\n        for layer in self.dense_layers:\n            x = layer(x)\n        \n        # Output\n        return self.output(x)\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        \n        # Log training metrics\n        self.log('train_loss', loss, prog_bar=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        \n        # Calculate accuracy\n        preds = torch.argmax(y_hat, dim=1)\n        acc = (preds == y).float().mean()\n        \n        # Log validation metrics\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', acc, prog_bar=True)\n        return {'val_loss': loss, 'val_acc': acc}\n    \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        \n        preds = torch.argmax(y_hat, dim=1)\n        acc = (preds == y).float().mean()\n        \n        # Log test metrics\n        self.log('test_loss', loss)\n        self.log('test_acc', acc)\n        return {'test_loss': loss, 'test_acc': acc, 'preds': preds, 'targets': y}\n    \n    def configure_optimizers(self):\n        optimizer = optim.Adam(\n            self.parameters(),\n            lr=self.config.model.learning_rate,\n            weight_decay=self.config.model.weight_decay\n        )\n        return optimizer\n\n# class NIDSDataModule(pl.LightningDataModule):\n#     def __init__(self, config):\n#         super().__init__()\n#         self.config = config\n#         self.batch_size = config.training.batch_size\n#         self.sequence_length = config.training.sequence_length\n#         self.num_workers = config.data.num_workers\n        \n#     def prepare_data(self):\n#         # Load and preprocess data\n#         df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n        \n#         # Clean data\n#         df.replace([np.inf, -np.inf], np.nan, inplace=True)\n#         df.dropna(inplace=True)\n        \n#         # Remove duplicates\n#         df.drop_duplicates(inplace=True)\n        \n#         # Encode labels\n#         self.label_encoder = LabelEncoder()\n#         df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n#         self.classes = self.label_encoder.classes_\n        \n#         # Split data\n#         train_df, test_df = train_test_split(\n#             df, test_size=self.config.data.test_size, \n#             random_state=42, \n#             stratify=df['Label_Num']\n#         )\n        \n#         # Prepare features and labels\n#         X_train = train_df.drop(['Label', 'Label_Num', 'Timestamp', 'Flow ID'], axis=1)\n#         y_train = train_df['Label_Num']\n#         X_test = test_df.drop(['Label', 'Label_Num', 'Timestamp', 'Flow ID'], axis=1)\n#         y_test = test_df['Label_Num']\n        \n#         # Standardize features\n#         self.scaler = StandardScaler()\n#         X_train = self.scaler.fit_transform(X_train)\n#         X_test = self.scaler.transform(X_test)\n        \n#         # Create sequences\n#         self.X_train_seq, self.y_train_seq = self.create_sequences(X_train, y_train)\n#         self.X_test_seq, self.y_test_seq = self.create_sequences(X_test, y_test)\n\nclass NIDSDataModule(pl.LightningDataModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.batch_size = config.training.batch_size\n        self.sequence_length = config.training.sequence_length\n        self.num_workers = config.data.num_workers\n        \n    def prepare_data(self):\n        # Load and preprocess data\n        df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n        \n        # Clean data\n        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df.dropna(inplace=True)\n        \n        # Remove duplicates\n        df.drop_duplicates(inplace=True)\n        # Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol',\n        # Identify non-numeric columns to exclude\n        self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', 'Src Port','Attack', 'Dst IP', 'Dst Port', 'Protocol']\n        self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n        \n        # Encode labels\n        self.label_encoder = LabelEncoder()\n        df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n        self.classes = self.label_encoder.classes_\n        \n        # Split data\n        train_df, test_df = train_test_split(\n            df, test_size=self.config.data.test_size, \n            random_state=42, \n            stratify=df['Label_Num']\n        )\n        \n        # Prepare features and labels\n        X_train = train_df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n        y_train = train_df['Label_Num']\n        X_test = test_df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n        y_test = test_df['Label_Num']\n        \n        # Standardize features\n        self.scaler = StandardScaler()\n        X_train = self.scaler.fit_transform(X_train)\n        X_test = self.scaler.transform(X_test)\n        \n        # Create sequences\n        self.X_train_seq, self.y_train_seq = self.create_sequences(X_train, y_train)\n        self.X_test_seq, self.y_test_seq = self.create_sequences(X_test, y_test)        \n    \n    def create_sequences(self, X, y):\n        X_sequences = []\n        y_sequences = []\n        \n        for i in range(len(X) - self.sequence_length):\n            X_sequences.append(X[i:i+self.sequence_length])\n            y_sequences.append(y.iloc[i+self.sequence_length-1])\n            \n        return np.array(X_sequences), np.array(y_sequences)\n    \n    def setup(self, stage=None):\n        # Convert to tensors\n        self.train_dataset = TensorDataset(\n            torch.FloatTensor(self.X_train_seq),\n            torch.LongTensor(self.y_train_seq)\n        )\n        self.test_dataset = TensorDataset(\n            torch.FloatTensor(self.X_test_seq),\n            torch.LongTensor(self.y_test_seq)\n        )\n    \n    def train_dataloader(self):\n        # return DataLoader(\n        #     self.train_dataset,\n        #     batch_size=self.batch_size,\n        #     shuffle=True,\n        #     num_workers=self.num_workers,\n        #     pin_memory=True\n        # )\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=self.num_workers,\n            pin_memory=True,\n            persistent_workers=True\n        )\n    \n    def val_dataloader(self):\n        return DataLoader(\n            self.test_dataset,  # Using test set for validation in this example\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n\ndef init_wandb():\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"mohammad_wandb_secret\")\n    wandb.login(key=wandb_api_key)\n    wandb_logger = WandbLogger(\n        project=config.wandb.project,\n        entity=config.wandb.entity,\n        tags=config.wandb.tags,\n        notes=config.wandb.notes,\n        log_model='all'\n    )\n    \n    return wandb_logger\n\ndef main():\n    # Initialize wandb\n    wandb_logger = init_wandb()\n    \n    # Initialize data module\n    data_module = NIDSDataModule(config)\n    data_module.prepare_data()\n    data_module.setup()\n    \n    # Get input size from data\n    sample_x, _ = next(iter(data_module.train_dataloader()))\n    input_size = sample_x.shape[2]\n    num_classes = len(data_module.classes)\n    # Log dataset info to wandb\n    wandb.config.update({\n        \"input_size\": input_size,\n        \"num_classes\": num_classes,\n        \"sequence_length\": config.training.sequence_length,\n        \"train_samples\": len(data_module.train_dataset),\n        \"test_samples\": len(data_module.test_dataset)\n    })\n    \n    # Initialize model\n    model = LSTMModel(input_size, num_classes, config)\n    \n    # Callbacks\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=config.training.early_stopping_patience,\n        mode='min'\n    )\n    \n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_acc',\n        mode='max',\n        save_top_k=1,\n        dirpath='checkpoints',\n        filename='best_model'\n    )\n    \n    # Initialize trainer\n    trainer = pl.Trainer(\n        logger=wandb_logger,\n        max_epochs=config.training.max_epochs,\n        # gpus=config.training.gpus,\n        callbacks=[early_stopping, checkpoint_callback],\n        deterministic=True,\n        enable_progress_bar=True,\n        log_every_n_steps=1000\n    )\n    \n    # Train model\n    trainer.fit(model, datamodule=data_module)\n    \n    # Test model\n    test_results = trainer.test(model, datamodule=data_module)\n    \n    # Log confusion matrix\n    test_loader = data_module.test_dataloader()\n    all_preds = []\n    all_targets = []\n    \n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            x, y = batch\n            y_hat = model(x)\n            preds = torch.argmax(y_hat, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(y.cpu().numpy())\n    \n    wandb.log({\n        \"confusion_matrix\": wandb.plot.confusion_matrix(\n            y_true=all_targets,\n            preds=all_preds,\n            class_names=data_module.classes.tolist())\n    })\n    \n    # Log classification report\n    report = classification_report(\n        all_targets, all_preds, \n        target_names=data_module.classes.tolist(),\n        output_dict=True\n    )\n    \n    wandb.log({\n        \"classification_report\": report,\n        \"test_accuracy\": accuracy_score(all_targets, all_preds)\n    })\n    \n    # Finish wandb run\n    wandb.finish()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T15:36:32.448790Z","iopub.execute_input":"2025-06-17T15:36:32.449395Z","execution_failed":"2025-06-17T16:37:52.124Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohammad-fleity\u001b[0m (\u001b[33mmohammad-fleity-lebanese-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250617_153746-261lsejs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohammad-fleity-lebanese-university/uncategorized/runs/261lsejs' target=\"_blank\">vocal-salad-3</a></strong> to <a href='https://wandb.ai/mohammad-fleity-lebanese-university/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/uncategorized' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/uncategorized</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/uncategorized/runs/261lsejs' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/uncategorized/runs/261lsejs</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9434355e1af04c5e85889d697d94a62a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91b1d43917604648931a75d1bdc8f2e2"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset, Dataset, WeightedRandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nimport wandb\nfrom omegaconf import OmegaConf\nimport os\nimport pickle\nimport warnings\nfrom kaggle_secrets import UserSecretsClient\nfrom sklearn.metrics import f1_score\nwarnings.filterwarnings('ignore')\n\n# Configuration\nconfig = OmegaConf.create({\n  \"wandb\": {\n    \"project\": \"DL-NIDS-2--cic-ids-2017\",\n    \"entity\": \"mohammad-fleity-lebanese-university\",\n    \"tags\": [\"LSTM\", \"CIC-IDS-2017\", \"PyTorch\"],\n    \"notes\": \"LSTM model for network intrusion detection with PyTorch Lightning\"\n  },\n  \"model\": {\n    \"name\": \"lstm\",\n    \"hidden_size\": 80,\n    \"num_layers\": 2,\n    \"dropout\": 0.3,\n    \"dense_units\": [160],\n    \"learning_rate\": 0.0001,\n    \"weight_decay\": 1e-5\n  },\n  \"training\": {\n    \"sequence_length\": 3,\n    \"batch_size\": 64,\n    \"max_epochs\": 8,\n    \"early_stopping_patience\": 5,\n    \"oversample\": True,\n    \"gpus\": 1 if torch.cuda.is_available() else 0\n  },\n  \"data\": {\n    \"raw\": \"cic_ids_2017.parquet\",\n    \"test_size\": 0.2,\n    \"num_workers\": 4\n  }\n})\n\nclass LSTMModel(pl.LightningModule):\n    # def __init__(self, input_size, num_classes, config):\n    def __init__(self, input_size, num_classes, config):\n        super().__init__()\n        self.config = config\n        self.save_hyperparameters()\n        \n        # LSTM Layer\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=config.model.hidden_size,\n            num_layers=config.model.num_layers,\n            batch_first=True,\n            # dropout=config.model.dropout if config.model.num_layers > 1 else 0\n            dropout=0.5 if config.model.num_layers > 1 else 0.3\n        )\n        \n        # Batch normalization after LSTM\n        self.lstm_bn = nn.BatchNorm1d(config.model.hidden_size)\n        \n        # Dropout layer\n        self.dropout = nn.Dropout(config.model.dropout)\n        \n        # Dense layers\n        self.dense_layers = nn.ModuleList()\n        prev_units = config.model.hidden_size\n        for units in config.model.dense_units:\n            self.dense_layers.append(nn.Linear(prev_units, units))\n            self.dense_layers.append(nn.BatchNorm1d(units))\n            self.dense_layers.append(nn.ReLU())\n            self.dense_layers.append(nn.Dropout(config.model.dropout))\n            prev_units = units\n        \n        # Output layer\n        self.output = nn.Linear(prev_units, num_classes)\n        \n        # Loss function\n        self.criterion = nn.CrossEntropyLoss()\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        preds = torch.argmax(y_hat, dim=1)\n        acc = (preds == y).float().mean()\n        \n        # Calculate F1 score\n        f1 = f1_score(y.cpu().numpy(), preds.cpu().numpy(), average='weighted')\n        \n        # Log metrics\n        self.log('train_loss_epoch', loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log('train_acc_epoch', acc*100, on_step=False, on_epoch=True, prog_bar=True)\n        self.log('train_f1_score', torch.tensor(f1), on_step=False, on_epoch=True)\n        \n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        preds = torch.argmax(y_hat, dim=1)\n        acc = (preds == y).float().mean()\n        \n        # Calculate F1 score\n        f1 = f1_score(y.cpu().numpy(), preds.cpu().numpy(), average='weighted')\n        \n        # Log metrics\n        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        self.log('val_acc', acc*100, on_step=False, on_epoch=True, prog_bar=True)\n        self.log('val_f1_score', torch.tensor(f1), on_step=False, on_epoch=True)\n        \n        return {'val_loss': loss, 'val_acc': acc, 'val_f1_score': f1, 'preds': preds, 'targets': y}\n    \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        preds = torch.argmax(y_hat, dim=1)\n        acc = (preds == y).float().mean()\n        \n        # Calculate F1 score\n        f1 = f1_score(y.cpu().numpy(), preds.cpu().numpy(), average='weighted')\n        \n        # Log metrics\n        self.log('test_loss', loss, on_step=False, on_epoch=True)\n        self.log('test_acc', acc*100, on_step=False, on_epoch=True)\n        self.log('test_f1', torch.tensor(f1), on_step=False, on_epoch=True)\n        \n        return {'test_loss': loss, 'test_acc': acc, 'test_f1': f1, 'preds': preds, 'targets': y}\n    \n    def forward(self, x):\n        # LSTM layer\n        lstm_out, _ = self.lstm(x)\n        # We only need the last timestep's output for classification\n        lstm_out = lstm_out[:, -1, :]\n        \n        # Batch norm\n        lstm_out = self.lstm_bn(lstm_out)\n        \n        # Dense layers\n        x = lstm_out\n        for layer in self.dense_layers:\n            x = layer(x)\n        \n        # Output\n        return self.output(x)\n    \n \n    def configure_optimizers(self):\n        optimizer = optim.Adam(\n            self.parameters(),\n            lr=self.config.model.learning_rate,\n            weight_decay=self.config.model.weight_decay\n        )\n        return optimizer\n\n\nclass NIDSDataModule(pl.LightningDataModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.batch_size = config.training.batch_size\n        self.sequence_length = config.training.sequence_length\n        self.num_workers = config.data.num_workers\n        self.oversample=config.training.oversample\n        self.alpha = 0.5 \n        \n    def prepare_data(self):\n        # Load and preprocess data\n        df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n        \n        # Clean data\n        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df.dropna(inplace=True)\n        \n        # Remove duplicates\n        df.drop_duplicates(inplace=True)\n        # Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol',\n        # Identify non-numeric columns to exclude\n        self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', 'Src Port','Attack', 'Dst IP', 'Dst Port', 'Protocol']\n        self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n        \n        # Encode labels\n        self.label_encoder = LabelEncoder()\n        df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n        self.classes = self.label_encoder.classes_\n        \n        # Split data\n        train_df, test_df = train_test_split(\n            df, test_size=self.config.data.test_size, \n            random_state=42, \n            stratify=df['Label_Num']\n        )\n        \n        # Prepare features and labels\n        X_train = train_df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n        y_train = train_df['Label_Num']\n        X_test = test_df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n        y_test = test_df['Label_Num']\n        \n        # Standardize features\n        self.scaler = StandardScaler()\n        X_train = self.scaler.fit_transform(X_train)\n        X_test = self.scaler.transform(X_test)\n        \n        # Create sequences\n        self.X_train_seq, self.y_train_seq = self.create_sequences(X_train, y_train)\n        self.X_test_seq, self.y_test_seq = self.create_sequences(X_test, y_test)        \n    \n    def create_sequences(self, X, y):\n        X_sequences = []\n        y_sequences = []\n        \n        for i in range(len(X) - self.sequence_length):\n            X_sequences.append(X[i:i+self.sequence_length])\n            y_sequences.append(y.iloc[i+self.sequence_length-1])\n            \n        return np.array(X_sequences), np.array(y_sequences)\n    \n    def setup(self, stage=None):\n        # Convert to tensors\n        self.train_dataset = TensorDataset(\n            torch.FloatTensor(self.X_train_seq),\n            torch.LongTensor(self.y_train_seq)\n        )\n        self.test_dataset = TensorDataset(\n            torch.FloatTensor(self.X_test_seq),\n            torch.LongTensor(self.y_test_seq)\n        )\n    \n    def train_dataloader(self):\n        if self.oversample:\n            # Compute sample weights based on class frequency\n            y_train_np = self.y_train_seq  # Use the already processed sequences\n            class_counts = np.bincount(y_train_np)\n            inv_freq = 1.0 / class_counts\n            class_weights = inv_freq ** self.alpha\n            sample_weights = class_weights[y_train_np]\n            \n            # sampler = WeightedRandomSampler(\n            #     weights=sample_weights,\n            #     num_samples=len(sample_weights),  # Maintain full dataset size\n            #     replacement=True\n            # )\n            sampler = WeightedRandomSampler(\n                weights=sample_weights,\n                num_samples=8000,  # FIXED number of samples per epoch\n                replacement=True\n            )\n        else:\n            sampler = RandomSampler(\n                self.train_dataset,\n                num_samples=8000,  # Same fixed size for consistency\n                replacement=True\n            )\n        # else:\n        #     sampler = RandomSampler(\n        #         self.train_dataset,\n        #         replacement=False\n        #     )\n        return DataLoader(\n            dataset=self.train_dataset,\n            batch_size=self.batch_size,\n            sampler=sampler,\n            num_workers=self.num_workers,\n            persistent_workers=True,\n            pin_memory=True\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.test_dataset,  # Using test set for validation in this example\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n\n\ndef init_wandb():\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"mohammad_wandb_secret\")\n    wandb.login(key=wandb_api_key)\n    \n    # Initialize the run first\n    run = wandb.init(\n        project=config.wandb.project,\n        entity=config.wandb.entity,\n        tags=config.wandb.tags,\n        notes=config.wandb.notes,\n        config={\n            \"input_size\": None,  # Will be updated later\n            \"num_classes\": None,\n            \"sequence_length\": config.training.sequence_length,\n            \"train_samples\": None,\n            \"test_samples\": None,\n            \"model_config\": dict(config.model),\n            \"training_config\": dict(config.training)\n        }\n    )\n    \n    # Then create the logger\n    wandb_logger = WandbLogger(\n        experiment=run,\n        log_model='all'\n    )\n    \n    return wandb_logger, run\n\ndef main():\n    # Initialize wandb - now returns both logger and run\n    wandb_logger, run = init_wandb()\n    \n    # Initialize data module\n    data_module = NIDSDataModule(config)\n    data_module.prepare_data()\n    data_module.setup()\n    \n    # Get input size from data\n    sample_x, _ = next(iter(data_module.train_dataloader()))\n    input_size = sample_x.shape[2]\n    num_classes = len(data_module.classes)\n    \n    # Update the config with actual values\n    run.config.update({\n        \"input_size\": input_size,\n        \"num_classes\": num_classes,\n        \"train_samples\": len(data_module.train_dataset),\n        \"test_samples\": len(data_module.test_dataset)\n    })\n    \n    model = LSTMModel(input_size, num_classes, config)\n    \n    # Callbacks\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=config.training.early_stopping_patience,\n        mode='min'\n    )\n    \n    # checkpoint_callback = ModelCheckpoint(\n    #     monitor='val_f1',  # Now monitoring F1 score\n    #     mode='max',\n    #     save_top_k=1,\n    #     dirpath='checkpoints',\n    #     filename='best_model'\n    # )\n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_acc',\n        mode='max',\n        save_top_k=1,\n        dirpath='checkpoints',\n        filename='best_model'\n    )\n \n    # Initialize trainer\n    trainer = pl.Trainer(\n        logger=wandb_logger,\n        max_epochs=config.training.max_epochs,\n        callbacks=[early_stopping, checkpoint_callback],\n        deterministic=True,\n        gradient_clip_val=1.0,\n        enable_progress_bar=True,\n        log_every_n_steps=1000\n    )\n    \n    # Train model\n    trainer.fit(model, datamodule=data_module)\n    \n    # Test model\n    test_results = trainer.test(model, datamodule=data_module)\n    \n    # Collect all predictions and targets for final evaluation\n    test_loader = data_module.test_dataloader()\n    all_preds = []\n    all_targets = []\n    \n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            x, y = batch\n            y_hat = model(x)\n            preds = torch.argmax(y_hat, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(y.cpu().numpy())\n    \n    # Final metrics calculation\n    test_acc = accuracy_score(all_targets, all_preds)\n    test_f1 = f1_score(all_targets, all_preds, average='weighted')\n    \n    # Log final test metrics\n    wandb.log({\n        'test_acc': test_acc,\n        'test_f1': test_f1,\n        'test_loss': test_results[0]['test_loss']\n    })\n    \n    # Confusion matrix and classification report\n    class_names = data_module.classes.tolist()\n    \n    # Confusion Matrix\n    wandb.log({\n        \"confusion_matrix\": wandb.plot.confusion_matrix(\n            y_true=all_targets,\n            preds=all_preds,\n            class_names=class_names,\n            title=\"Confusion Matrix\"\n        )\n    })\n    \n    # Classification Report\n    report = classification_report(\n        all_targets, all_preds, \n        target_names=class_names,\n        output_dict=True\n    )\n    \n    # Create a wandb Table for the classification report\n    report_table = wandb.Table(columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n    for class_name in class_names:\n        report_table.add_data(\n            class_name,\n            report[class_name][\"precision\"],\n            report[class_name][\"recall\"],\n            report[class_name][\"f1-score\"],\n            report[class_name][\"support\"]\n        )\n    \n    # Add weighted averages\n    report_table.add_data(\n        \"Weighted Avg\",\n        report[\"weighted avg\"][\"precision\"],\n        report[\"weighted avg\"][\"recall\"],\n        report[\"weighted avg\"][\"f1-score\"],\n        report[\"weighted avg\"][\"support\"]\n    )\n    \n    wandb.log({\"classification_report\": report_table})\n    \n    # Finish wandb run\n    wandb.finish()\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:54:22.094675Z","iopub.execute_input":"2025-06-18T16:54:22.095155Z","execution_failed":"2025-06-18T17:13:25.994Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">restful-night-20</strong> at: <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/fdsxdco1' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/fdsxdco1</a><br> View project at: <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250618_164908-fdsxdco1/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250618_165422-85xh4zmo</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/85xh4zmo' target=\"_blank\">decent-sun-21</a></strong> to <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/85xh4zmo' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/85xh4zmo</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1a487047a5341aea016d756d5bbc0db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce8beedfb29d44228bd24fff8ea89083"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nimport wandb\nfrom omegaconf import OmegaConf\nimport os\nimport pickle\nimport warnings\nfrom kaggle_secrets import UserSecretsClient\n\nwarnings.filterwarnings('ignore')\n\n# Configuration\nconfig = OmegaConf.create({\n  \"wandb\": {\n    \"project\": \"DL-NIDS-2--cic-ids-2017\",\n    \"entity\": \"mohammad-fleity-lebanese-university\",\n    \"tags\": [\"LSTM\", \"CIC-IDS-2017\", \"PyTorch\"],\n    \"notes\": \"LSTM model for network intrusion detection with PyTorch Lightning\"\n  },\n  \"model\": {\n    \"name\": \"lstm\",\n    \"hidden_size\": 80,\n    \"num_layers\": 1,\n    \"dropout\": 0.3,\n    \"dense_units\": [80],\n    \"learning_rate\": 0.001,\n    \"weight_decay\": 1e-5\n  },\n  \"training\": {\n    \"sequence_length\": 3,\n    \"batch_size\": 64,\n    \"max_epochs\": 10,\n    \"early_stopping_patience\": 5,\n    \"gpus\": 1 if torch.cuda.is_available() else 0\n  },\n  \"data\": {\n    \"raw\": \"cic_ids_2017.parquet\",\n    \"test_size\": 0.2,\n    \"num_workers\": 4\n  }\n})\n\nclass LSTMModel(pl.LightningModule):\n    def __init__(self, input_size, num_classes, config):\n        super().__init__()\n        self.config = config\n        self.save_hyperparameters()\n        \n        # LSTM Layer\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=config.model.hidden_size,\n            num_layers=config.model.num_layers,\n            batch_first=True,\n            dropout=config.model.dropout if config.model.num_layers > 1 else 0\n        )\n        \n        # Batch normalization after LSTM\n        self.lstm_bn = nn.BatchNorm1d(config.model.hidden_size)\n        \n        # Dropout layer\n        self.dropout = nn.Dropout(config.model.dropout)\n        \n        # Dense layers\n        self.dense_layers = nn.ModuleList()\n        prev_units = config.model.hidden_size\n        for units in config.model.dense_units:\n            self.dense_layers.append(nn.Linear(prev_units, units))\n            self.dense_layers.append(nn.BatchNorm1d(units))\n            self.dense_layers.append(nn.ReLU())\n            self.dense_layers.append(nn.Dropout(config.model.dropout))\n            prev_units = units\n        \n        # Output layer\n        self.output = nn.Linear(prev_units, num_classes)\n        \n        # Loss function\n        self.criterion = nn.CrossEntropyLoss()\n    \n    def forward(self, x):\n        # LSTM layer\n        lstm_out, _ = self.lstm(x)\n        # We only need the last timestep's output for classification\n        lstm_out = lstm_out[:, -1, :]\n        \n        # Batch norm\n        lstm_out = self.lstm_bn(lstm_out)\n        \n        # Dense layers\n        x = lstm_out\n        for layer in self.dense_layers:\n            x = layer(x)\n        \n        # Output\n        return self.output(x)\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        \n        # Log training metrics\n        self.log('train_loss', loss, prog_bar=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        \n        # Calculate accuracy\n        preds = torch.argmax(y_hat, dim=1)\n        acc = (preds == y).float().mean()\n        \n        # Log validation metrics\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', acc, prog_bar=True)\n        return {'val_loss': loss, 'val_acc': acc}\n    \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = self.criterion(y_hat, y)\n        \n        preds = torch.argmax(y_hat, dim=1)\n        acc = (preds == y).float().mean()\n        \n        # Log test metrics\n        self.log('test_loss', loss)\n        self.log('test_acc', acc)\n        return {'test_loss': loss, 'test_acc': acc, 'preds': preds, 'targets': y}\n    \n    def configure_optimizers(self):\n        optimizer = optim.Adam(\n            self.parameters(),\n            lr=self.config.model.learning_rate,\n            weight_decay=self.config.model.weight_decay\n        )\n        return optimizer\n\n# class NIDSDataModule(pl.LightningDataModule):\n#     def __init__(self, config):\n#         super().__init__()\n#         self.config = config\n#         self.batch_size = config.training.batch_size\n#         self.sequence_length = config.training.sequence_length\n#         self.num_workers = config.data.num_workers\n        \n#     def prepare_data(self):\n#         # Load and preprocess data\n#         df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n        \n#         # Clean data\n#         df.replace([np.inf, -np.inf], np.nan, inplace=True)\n#         df.dropna(inplace=True)\n        \n#         # Remove duplicates\n#         df.drop_duplicates(inplace=True)\n        \n#         # Encode labels\n#         self.label_encoder = LabelEncoder()\n#         df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n#         self.classes = self.label_encoder.classes_\n        \n#         # Split data\n#         train_df, test_df = train_test_split(\n#             df, test_size=self.config.data.test_size, \n#             random_state=42, \n#             stratify=df['Label_Num']\n#         )\n        \n#         # Prepare features and labels\n#         X_train = train_df.drop(['Label', 'Label_Num', 'Timestamp', 'Flow ID'], axis=1)\n#         y_train = train_df['Label_Num']\n#         X_test = test_df.drop(['Label', 'Label_Num', 'Timestamp', 'Flow ID'], axis=1)\n#         y_test = test_df['Label_Num']\n        \n#         # Standardize features\n#         self.scaler = StandardScaler()\n#         X_train = self.scaler.fit_transform(X_train)\n#         X_test = self.scaler.transform(X_test)\n        \n#         # Create sequences\n#         self.X_train_seq, self.y_train_seq = self.create_sequences(X_train, y_train)\n#         self.X_test_seq, self.y_test_seq = self.create_sequences(X_test, y_test)\n\nclass NIDSDataModule(pl.LightningDataModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.batch_size = config.training.batch_size\n        self.sequence_length = config.training.sequence_length\n        self.num_workers = config.data.num_workers\n        \n    def prepare_data(self):\n        # Load and preprocess data\n        df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n        \n        # Clean data\n        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df.dropna(inplace=True)\n        \n        # Remove duplicates\n        df.drop_duplicates(inplace=True)\n        # Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol',\n        # Identify non-numeric columns to exclude\n        self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', 'Src Port','Attack', 'Dst IP', 'Dst Port', 'Protocol']\n        self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n        \n        # Encode labels\n        self.label_encoder = LabelEncoder()\n        df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n        self.classes = self.label_encoder.classes_\n        \n        # Split data\n        train_df, test_df = train_test_split(\n            df, test_size=self.config.data.test_size, \n            random_state=42, \n            stratify=df['Label_Num']\n        )\n        print(len(train_df))\n        print(len(test_df))\n        # max_rows = 90_000\n        # if len(train_df) > max_rows:\n        #     train_df = train_df.sample(n=max_rows, random_state=42)\n\n        # Prepare features and labels\n        X_train = train_df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n        y_train = train_df['Label_Num']\n        X_test = test_df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n        y_test = test_df['Label_Num']\n        \n        # Standardize features\n        self.scaler = StandardScaler()\n        X_train = self.scaler.fit_transform(X_train)\n        X_test = self.scaler.transform(X_test)\n        \n        # Create sequences\n        self.X_train_seq, self.y_train_seq = self.create_sequences(X_train, y_train)\n        self.X_test_seq, self.y_test_seq = self.create_sequences(X_test, y_test)        \n    \n    def create_sequences(self, X, y):\n        X_sequences = []\n        y_sequences = []\n        \n        for i in range(len(X) - self.sequence_length):\n            X_sequences.append(X[i:i+self.sequence_length])\n            y_sequences.append(y.iloc[i+self.sequence_length-1])\n            \n        return np.array(X_sequences), np.array(y_sequences)\n    \n    def setup(self, stage=None):\n        # Convert to tensors\n        self.train_dataset = TensorDataset(\n            torch.FloatTensor(self.X_train_seq),\n            torch.LongTensor(self.y_train_seq)\n        )\n        self.test_dataset = TensorDataset(\n            torch.FloatTensor(self.X_test_seq),\n            torch.LongTensor(self.y_test_seq)\n        )\n    \n    def train_dataloader(self):\n        # return DataLoader(\n        #     self.train_dataset,\n        #     batch_size=self.batch_size,\n        #     shuffle=True,\n        #     num_workers=self.num_workers,\n        #     pin_memory=True\n        # )\n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=self.num_workers,\n            pin_memory=True,\n            persistent_workers=True\n        )\n    \n    def val_dataloader(self):\n        return DataLoader(\n            self.test_dataset,  # Using test set for validation in this example\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n\n# def init_wandb():\n#     user_secrets = UserSecretsClient()\n#     wandb_api_key = user_secrets.get_secret(\"mohammad_wandb_secret\")\n#     wandb.login(key=wandb_api_key)\n#     wandb_logger = WandbLogger(\n#         project=config.wandb.project,\n#         entity=config.wandb.entity,\n#         tags=config.wandb.tags,\n#         notes=config.wandb.notes,\n#         log_model='all'\n#     )\n    \n#     return wandb_logger\n\ndef init_wandb():\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"mohammad_wandb_secret\")\n    wandb.login(key=wandb_api_key)\n    \n    # Initialize the run first\n    run = wandb.init(\n        project=config.wandb.project,\n        entity=config.wandb.entity,\n        tags=config.wandb.tags,\n        notes=config.wandb.notes,\n        config={\n            \"input_size\": None,  # Will be updated later\n            \"num_classes\": None,\n            \"sequence_length\": config.training.sequence_length,\n            \"train_samples\": None,\n            \"test_samples\": None,\n            \"model_config\": dict(config.model),\n            \"training_config\": dict(config.training)\n        }\n    )\n    \n    # Then create the logger\n    wandb_logger = WandbLogger(\n        experiment=run,\n        log_model='all'\n    )\n    \n    return wandb_logger, run\n\ndef main():\n    # Initialize wandb - now returns both logger and run\n    wandb_logger, run = init_wandb()\n    \n    # Initialize data module\n    data_module = NIDSDataModule(config)\n    data_module.prepare_data()\n    data_module.setup()\n    \n    # Get input size from data\n    sample_x, _ = next(iter(data_module.train_dataloader()))\n    input_size = sample_x.shape[2]\n    num_classes = len(data_module.classes)\n    \n    # Update the config with actual values\n    run.config.update({\n        \"input_size\": input_size,\n        \"num_classes\": num_classes,\n        \"train_samples\": len(data_module.train_dataset),\n        \"test_samples\": len(data_module.test_dataset)\n    })\n    \n    # Initialize model\n    model = LSTMModel(input_size, num_classes, config)\n    \n    # Callbacks\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=config.training.early_stopping_patience,\n        mode='min'\n    )\n    \n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_acc',\n        mode='max',\n        save_top_k=1,\n        dirpath='checkpoints',\n        filename='best_model'\n    )\n    \n    # Initialize trainer\n    trainer = pl.Trainer(\n        logger=wandb_logger,\n        max_epochs=config.training.max_epochs,\n        callbacks=[early_stopping, checkpoint_callback],\n        deterministic=True,\n        enable_progress_bar=True,\n        log_every_n_steps=1000\n    )\n    \n    # Train model\n    trainer.fit(model, datamodule=data_module)\n    \n    # Test model\n    test_results = trainer.test(model, datamodule=data_module)\n    \n    # Collect all predictions and targets\n    test_loader = data_module.test_dataloader()\n    all_preds = []\n    all_targets = []\n    \n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            x, y = batch\n            y_hat = model(x)\n            preds = torch.argmax(y_hat, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(y.cpu().numpy())\n    \n    # Convert to numpy arrays\n    all_preds = np.array(all_preds)\n    all_targets = np.array(all_targets)\n    \n    # Enhanced Confusion Matrix Logging\n    class_names = data_module.classes.tolist()\n    \n    # Create a more detailed confusion matrix\n    wandb.log({\n        \"confusion_matrix\": wandb.plot.confusion_matrix(\n            y_true=all_targets,\n            preds=all_preds,\n            class_names=class_names,\n            title=\"Confusion Matrix\",\n            normalize=None\n        ),\n        \"normalized_confusion_matrix\": wandb.plot.confusion_matrix(\n            y_true=all_targets,\n            preds=all_preds,\n            class_names=class_names,\n            title=\"Normalized Confusion Matrix\",\n            normalize=\"all\"\n        )\n    })\n    \n    # Log classification report\n    report = classification_report(\n        all_targets, all_preds, \n        target_names=class_names,\n        output_dict=True\n    )\n    \n    # Convert report to wandb.Table for better visualization\n    report_table = wandb.Table(columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n    for class_name in class_names:\n        report_table.add_data(\n            class_name,\n            report[class_name][\"precision\"],\n            report[class_name][\"recall\"],\n            report[class_name][\"f1-score\"],\n            report[class_name][\"support\"]\n        )\n    \n    # Add overall metrics\n    report_table.add_data(\n        \"Weighted Avg\",\n        report[\"weighted avg\"][\"precision\"],\n        report[\"weighted avg\"][\"recall\"],\n        report[\"weighted avg\"][\"f1-score\"],\n        report[\"weighted avg\"][\"support\"]\n    )\n    \n    wandb.log({\n        \"classification_report\": report_table,\n        \"test_accuracy\": accuracy_score(all_targets, all_preds),\n        \"test_precision\": report[\"weighted avg\"][\"precision\"],\n        \"test_recall\": report[\"weighted avg\"][\"recall\"],\n        \"test_f1\": report[\"weighted avg\"][\"f1-score\"]\n    })\n    \n    # Save the confusion matrix as an image as well\n    plt.figure(figsize=(12, 10))\n    cm = confusion_matrix(all_targets, all_preds)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=class_names, yticklabels=class_names)\n    plt.title('Confusion Matrix')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.tight_layout()\n    wandb.log({\"confusion_matrix_image\": wandb.Image(plt)})\n    plt.close()\n    \n    # Finish wandb run\n    wandb.finish()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T17:18:40.064843Z","iopub.execute_input":"2025-06-17T17:18:40.065411Z","execution_failed":"2025-06-17T18:21:11.796Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohammad-fleity\u001b[0m (\u001b[33mmohammad-fleity-lebanese-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250617_171846-qoo0pdxl</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/qoo0pdxl' target=\"_blank\">solar-tree-15</a></strong> to <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/qoo0pdxl' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/qoo0pdxl</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"681fbb22822d4473bde654ec580a3a41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler, RandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nimport wandb\nfrom omegaconf import OmegaConf\nimport os\nimport warnings\nfrom kaggle_secrets import UserSecretsClient\n\nwarnings.filterwarnings('ignore')\n\n# Optimized Configuration\nconfig = OmegaConf.create({\n    \"wandb\": {\n        \"project\": \"DL-NIDS-2--cic-ids-2017\",\n        \"entity\": \"mohammad-fleity-lebanese-university\",\n        \"tags\": [\"LSTM\", \"CIC-IDS-2017\", \"PyTorch\"],\n        \"notes\": \"Optimized LSTM for network intrusion detection\"\n    },\n    \"model\": {\n        \"hidden_size\": 128,          # Increased capacity\n        \"num_layers\": 2,             # Deeper network\n        \"dropout\": 0.4,              # Stronger regularization\n        \"dense_units\": [128, 64],    # Better feature extraction\n        \"learning_rate\": 0.0001,     # Slower learning\n        \"weight_decay\": 1e-4         # Stronger L2 regularization\n    },\n    \"training\": {\n        \"sequence_length\": 5,        # Longer temporal context\n        \"batch_size\": 128,           # Larger batches\n        \"max_epochs\": 10,            # More training time\n        \"early_stopping_patience\": 7,# More patience\n        \"oversample\": True,          # Class balancing\n        \"gpus\": 1 if torch.cuda.is_available() else 0,\n        \"train_size\": 0.7,           # Proper train/val split\n        \"val_size\": 0.15             # 70/15/15 split\n    },\n    \"data\": {\n        \"raw\": \"cic_ids_2017.parquet\",\n        \"num_workers\": 4\n    }\n})\n\nclass LSTMModel(pl.LightningModule):\n    def __init__(self, input_size, num_classes, config):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Enhanced LSTM with layer normalization\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=config.model.hidden_size,\n            num_layers=config.model.num_layers,\n            batch_first=True,\n            dropout=config.model.dropout if config.model.num_layers > 1 else 0\n        )\n        \n        self.lstm_ln = nn.LayerNorm(config.model.hidden_size)  # Better for sequences\n        \n        # Improved dense layers\n        self.dense = nn.Sequential(\n            nn.Linear(config.model.hidden_size, config.model.dense_units[0]),\n            nn.LayerNorm(config.model.dense_units[0]),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(config.model.dense_units[0], config.model.dense_units[1]),\n            nn.LayerNorm(config.model.dense_units[1]),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout)\n        )\n        \n        self.output = nn.Linear(config.model.dense_units[1], num_classes)\n        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Regularization\n\n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        lstm_out = lstm_out[:, -1, :]  # Last timestep\n        lstm_out = self.lstm_ln(lstm_out)\n        features = self.dense(lstm_out)\n        return self.output(features)\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('train_loss', loss, prog_bar=True)\n        self.log('train_acc', acc, prog_bar=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', acc, prog_bar=True)\n    \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('test_loss', loss)\n        self.log('test_acc', acc)\n    \n    def configure_optimizers(self):\n        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.config.model.learning_rate,\n                                weight_decay=self.hparams.config.model.weight_decay)\n        return optimizer\n\nclass NIDSDataModule(pl.LightningDataModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.batch_size = config.training.batch_size\n        self.sequence_length = config.training.sequence_length\n        self.num_workers = config.data.num_workers\n        self.oversample = config.training.oversample\n        self.alpha = 0.5\n\n    def prepare_data(self):\n        df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n    \n        # Clean data\n        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df.dropna(inplace=True)\n        df.drop_duplicates(inplace=True)\n    \n        # Identify non-numeric columns\n        self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', \n                                 'Src Port', 'Attack', 'Dst IP', 'Dst Port', 'Protocol']\n        self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n    \n        # Encode labels\n        self.label_encoder = LabelEncoder()\n        df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n        self.classes = self.label_encoder.classes_\n    \n        # Initialize scaler\n        self.scaler = StandardScaler()\n    \n        # Train/Val/Test split\n        train_df, test_df = train_test_split(\n            df,\n            test_size=1 - self.config.training.train_size,\n            random_state=42,\n            stratify=df['Label_Num']\n        )\n        val_df, test_df = train_test_split(\n            test_df,\n            test_size=0.5,\n            random_state=42,\n            stratify=test_df['Label_Num']\n        )\n        print(len(train_df))\n        print(len(test_df))\n        # max_rows = 90_000\n        # if len(train_df) > max_rows:\n        #     train_df = train_df.sample(n=max_rows, random_state=42)\n\n        # Use `fit=True` only for training data\n        self.X_train, self.y_train = self._prepare_features(train_df, fit=True)\n        self.X_val, self.y_val = self._prepare_features(val_df, fit=False)\n        self.X_test, self.y_test = self._prepare_features(test_df, fit=False)\n\n    \n    # def prepare_data(self):\n    #     df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n        \n    #     # Clean data\n    #     df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    #     df.dropna(inplace=True)\n    #     df.drop_duplicates(inplace=True)\n        \n    #     # Identify non-numeric columns\n    #     self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', \n    #                            'Src Port', 'Attack', 'Dst IP', 'Dst Port', 'Protocol']\n    #     self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n        \n    #     # Encode labels\n    #     self.label_encoder = LabelEncoder()\n    #     df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n    #     self.classes = self.label_encoder.classes_\n    #     self.scaler = StandardScaler()\n\n    #     # Proper train/val/test split\n    #     train_df, test_df = train_test_split(\n    #         df, \n    #         test_size=1 - self.config.training.train_size,\n    #         random_state=42,\n    #         stratify=df['Label_Num']\n    #     )\n    #     val_df, test_df = train_test_split(\n    #         test_df,\n    #         test_size=0.5,  # Splits remaining 30% into 15% val, 15% test\n    #         random_state=42,\n    #         stratify=test_df['Label_Num']\n    #     )\n        \n    #     # Process each split\n    #     self.X_train, self.y_train = self._prepare_features(train_df)\n    #     self.X_val, self.y_val = self._prepare_features(val_df)\n    #     self.X_test, self.y_test = self._prepare_features(test_df)\n \n    # def _prepare_features(self, df):\n    #     X = df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n    #     y = df['Label_Num']\n    #     X = self.scaler.transform(X) if hasattr(self, 'scaler') else self.scaler.fit_transform(X)\n    #     return self.create_sequences(X, y)\n    \n    def _prepare_features(self, df, fit=False):\n        X = df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n        y = df['Label_Num']\n        if fit:\n            X = self.scaler.fit_transform(X)\n        else:\n            X = self.scaler.transform(X)\n        return self.create_sequences(X, y)\n\n    def create_sequences(self, X, y):\n        sequences = []\n        labels = []\n        for i in range(len(X) - self.sequence_length):\n            sequences.append(X[i:i+self.sequence_length])\n            labels.append(y.iloc[i+self.sequence_length-1])\n        return np.array(sequences), np.array(labels)\n    \n    def setup(self, stage=None):\n        self.scaler = StandardScaler()\n        self.scaler.fit(self.X_train.reshape(-1, self.X_train.shape[-1]))\n        \n        self.X_train = self.scaler.transform(self.X_train.reshape(-1, self.X_train.shape[-1])).reshape(self.X_train.shape)\n        self.X_val = self.scaler.transform(self.X_val.reshape(-1, self.X_val.shape[-1])).reshape(self.X_val.shape)\n        self.X_test = self.scaler.transform(self.X_test.reshape(-1, self.X_test.shape[-1])).reshape(self.X_test.shape)\n        \n        self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n        self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n        self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n        print(\"the model will be trained on: \",len(self.train_dataset),\" samples.\")\n        print(\"the model will be validated on: \",len(self.val_dataset),\" samples.\")\n        print(\"the model will be tested on: \",len(self.test_dataset),\" samples.\")\n    \n    def train_dataloader(self):\n        if self.oversample:\n            class_counts = np.bincount(self.y_train)\n            weights = 1. / class_counts[self.y_train]\n            sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n        else:\n            sampler = RandomSampler(self.train_dataset)\n            \n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            sampler=sampler,\n            num_workers=self.num_workers,\n            persistent_workers=True,\n            pin_memory=True\n        )\n    \n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n\n\ndef init_wandb():\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"mohammad_wandb_secret\")\n    wandb.login(key=wandb_api_key)\n    \n    # Initialize the run first\n    run = wandb.init(\n        project=config.wandb.project,\n        entity=config.wandb.entity,\n        tags=config.wandb.tags,\n        notes=config.wandb.notes,\n        config={\n            \"input_size\": None,  # Will be updated later\n            \"num_classes\": None,\n            \"sequence_length\": config.training.sequence_length,\n            \"train_samples\": None,\n            \"test_samples\": None,\n            \"model_config\": dict(config.model),\n            \"training_config\": dict(config.training)\n        }\n    )\n    \n    # Then create the logger\n    wandb_logger = WandbLogger(\n        experiment=run,\n        log_model='all'\n    )\n    \n    return wandb_logger, run\n\ndef main():\n    # Initialize wandb - now returns both logger and run\n    wandb_logger, run = init_wandb()\n    \n    # Initialize data module\n    data_module = NIDSDataModule(config)\n    data_module.prepare_data()\n    data_module.setup()\n    \n    # Get input size from data\n    sample_x, _ = next(iter(data_module.train_dataloader()))\n    input_size = sample_x.shape[2]\n    num_classes = len(data_module.classes)\n    \n    # Update the config with actual values\n    run.config.update({\n        \"input_size\": input_size,\n        \"num_classes\": num_classes,\n        \"train_samples\": len(data_module.train_dataset),\n        \"test_samples\": len(data_module.test_dataset)\n    })\n    \n    model = LSTMModel(input_size, num_classes, config)\n    \n    # Callbacks\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=config.training.early_stopping_patience,\n        mode='min'\n    )\n    \n    # checkpoint_callback = ModelCheckpoint(\n    #     monitor='val_f1',  # Now monitoring F1 score\n    #     mode='max',\n    #     save_top_k=1,\n    #     dirpath='checkpoints',\n    #     filename='best_model'\n    # )\n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_acc',\n        mode='max',\n        save_top_k=1,\n        dirpath='checkpoints',\n        filename='best_model'\n    )\n \n    # Initialize trainer\n    trainer = pl.Trainer(\n        precision=16,\n        logger=wandb_logger,\n        max_epochs=config.training.max_epochs,\n        callbacks=[early_stopping, checkpoint_callback],\n        deterministic=True,\n        gradient_clip_val=1.0,\n        enable_progress_bar=True,\n        log_every_n_steps=1000\n    )\n    \n    # Train model\n    trainer.fit(model, datamodule=data_module)\n    \n    # Test model\n    test_results = trainer.test(model, datamodule=data_module)\n    \n    # Collect all predictions and targets for final evaluation\n    test_loader = data_module.test_dataloader()\n    all_preds = []\n    all_targets = []\n    \n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            x, y = batch\n            y_hat = model(x)\n            preds = torch.argmax(y_hat, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(y.cpu().numpy())\n    \n    # Final metrics calculation\n    test_acc = accuracy_score(all_targets, all_preds)\n    test_f1 = f1_score(all_targets, all_preds, average='weighted')\n    \n    # Log final test metrics\n    wandb.log({\n        'test_acc': test_acc,\n        'test_f1': test_f1,\n        'test_loss': test_results[0]['test_loss']\n    })\n    \n    # Confusion matrix and classification report\n    class_names = data_module.classes.tolist()\n    \n    # Confusion Matrix\n    wandb.log({\n        \"confusion_matrix\": wandb.plot.confusion_matrix(\n            y_true=all_targets,\n            preds=all_preds,\n            class_names=class_names,\n            title=\"Confusion Matrix\"\n        )\n    })\n    \n    # Classification Report\n    report = classification_report(\n        all_targets, all_preds, \n        target_names=class_names,\n        output_dict=True\n    )\n    \n    # Create a wandb Table for the classification report\n    report_table = wandb.Table(columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n    for class_name in class_names:\n        report_table.add_data(\n            class_name,\n            report[class_name][\"precision\"],\n            report[class_name][\"recall\"],\n            report[class_name][\"f1-score\"],\n            report[class_name][\"support\"]\n        )\n    \n    # Add weighted averages\n    report_table.add_data(\n        \"Weighted Avg\",\n        report[\"weighted avg\"][\"precision\"],\n        report[\"weighted avg\"][\"recall\"],\n        report[\"weighted avg\"][\"f1-score\"],\n        report[\"weighted avg\"][\"support\"]\n    )\n    \n    wandb.log({\"classification_report\": report_table})\n    \n    # Finish wandb run\n    wandb.finish()\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-19T08:39:42.573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler, RandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nimport wandb\nfrom omegaconf import OmegaConf\nimport os\nimport warnings\nfrom kaggle_secrets import UserSecretsClient\n\nwarnings.filterwarnings('ignore')\n\n# Optimized Configuration\nconfig = OmegaConf.create({\n    \"wandb\": {\n        \"project\": \"DL-NIDS-2--cic-ids-2017\",\n        \"entity\": \"mohammad-fleity-lebanese-university\",\n        \"tags\": [\"LSTM\", \"CIC-IDS-2017\", \"PyTorch\"],\n        \"notes\": \"Optimized LSTM for network intrusion detection\"\n    },\n    \"model\": {\n        \"hidden_size\": 128,          # Increased capacity\n        \"num_layers\": 2,             # Deeper network\n        \"dropout\": 0.4,              # Stronger regularization\n        \"dense_units\": [128, 64],    # Better feature extraction\n        \"learning_rate\": 0.0001,     # Slower learning\n        \"weight_decay\": 1e-4         # Stronger L2 regularization\n    },\n    \"training\": {\n        \"sequence_length\": 5,        # Longer temporal context\n        \"batch_size\": 128,           # Larger batches\n        \"max_epochs\": 7,            # More training time\n        \"early_stopping_patience\": 7,# More patience\n        \"oversample\": True,          # Class balancing\n        \"gpus\": 1 if torch.cuda.is_available() else 0,\n        \"train_size\": 0.7,           # Proper train/val split\n        \"val_size\": 0.15             # 70/15/15 split\n    },\n    \"data\": {\n        \"raw\": \"cic_ids_2017.parquet\",\n        \"num_workers\": 4\n    }\n})\n\nclass LSTMModel(pl.LightningModule):\n    def __init__(self, input_size, num_classes, config):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Enhanced LSTM with layer normalization\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=config.model.hidden_size,\n            num_layers=config.model.num_layers,\n            batch_first=True,\n            dropout=config.model.dropout if config.model.num_layers > 1 else 0\n        )\n        \n        self.lstm_ln = nn.LayerNorm(config.model.hidden_size)  # Better for sequences\n        \n        # Improved dense layers\n        self.dense = nn.Sequential(\n            nn.Linear(config.model.hidden_size, config.model.dense_units[0]),\n            nn.LayerNorm(config.model.dense_units[0]),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(config.model.dense_units[0], config.model.dense_units[1]),\n            nn.LayerNorm(config.model.dense_units[1]),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout)\n        )\n        \n        self.output = nn.Linear(config.model.dense_units[1], num_classes)\n        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Regularization\n\n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        lstm_out = lstm_out[:, -1, :]  # Last timestep\n        lstm_out = self.lstm_ln(lstm_out)\n        features = self.dense(lstm_out)\n        return self.output(features)\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('train_loss_epoch', loss, prog_bar=True)\n        self.log('train_acc_epoch', acc*100, prog_bar=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', acc*100, prog_bar=True)\n    \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('test_loss', loss)\n        self.log('test_acc', acc*100)\n    \n    def configure_optimizers(self):\n        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.config.model.learning_rate,\n                                weight_decay=self.hparams.config.model.weight_decay)\n        return optimizer\n\nclass NIDSDataModule(pl.LightningDataModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.batch_size = config.training.batch_size\n        self.sequence_length = config.training.sequence_length\n        self.num_workers = config.data.num_workers\n        self.oversample = config.training.oversample\n        self.alpha = 0.5\n\n    def prepare_data(self):\n        df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n    \n        # Clean data\n        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df.dropna(inplace=True)\n        df.drop_duplicates(inplace=True)\n    \n        # Identify non-numeric columns\n        self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', \n                                 'Src Port', 'Attack', 'Dst IP', 'Dst Port', 'Protocol']\n        self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n    \n        # Encode labels\n        self.label_encoder = LabelEncoder()\n        df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n        self.classes = self.label_encoder.classes_\n    \n        # Initialize scaler\n        self.scaler = StandardScaler()\n    \n        # Train/Val/Test split\n        train_df, test_df = train_test_split(\n            df,\n            test_size=1 - self.config.training.train_size,\n            random_state=42,\n            stratify=df['Label_Num']\n        )\n        val_df, test_df = train_test_split(\n            test_df,\n            test_size=0.5,\n            random_state=42,\n            stratify=test_df['Label_Num']\n        )\n        print(len(train_df))\n        print(len(test_df))\n        # max_rows = 90_000\n        # if len(train_df) > max_rows:\n        #     train_df = train_df.sample(n=max_rows, random_state=42)\n\n        # Use `fit=True` only for training data\n        self.X_train, self.y_train = self._prepare_features(train_df, fit=True)\n        self.X_val, self.y_val = self._prepare_features(val_df, fit=False)\n        self.X_test, self.y_test = self._prepare_features(test_df, fit=False)\n\n    \n    # def prepare_data(self):\n    #     df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n        \n    #     # Clean data\n    #     df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    #     df.dropna(inplace=True)\n    #     df.drop_duplicates(inplace=True)\n        \n    #     # Identify non-numeric columns\n    #     self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', \n    #                            'Src Port', 'Attack', 'Dst IP', 'Dst Port', 'Protocol']\n    #     self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n        \n    #     # Encode labels\n    #     self.label_encoder = LabelEncoder()\n    #     df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n    #     self.classes = self.label_encoder.classes_\n    #     self.scaler = StandardScaler()\n\n    #     # Proper train/val/test split\n    #     train_df, test_df = train_test_split(\n    #         df, \n    #         test_size=1 - self.config.training.train_size,\n    #         random_state=42,\n    #         stratify=df['Label_Num']\n    #     )\n    #     val_df, test_df = train_test_split(\n    #         test_df,\n    #         test_size=0.5,  # Splits remaining 30% into 15% val, 15% test\n    #         random_state=42,\n    #         stratify=test_df['Label_Num']\n    #     )\n        \n    #     # Process each split\n    #     self.X_train, self.y_train = self._prepare_features(train_df)\n    #     self.X_val, self.y_val = self._prepare_features(val_df)\n    #     self.X_test, self.y_test = self._prepare_features(test_df)\n \n    # def _prepare_features(self, df):\n    #     X = df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n    #     y = df['Label_Num']\n    #     X = self.scaler.transform(X) if hasattr(self, 'scaler') else self.scaler.fit_transform(X)\n    #     return self.create_sequences(X, y)\n    \n    def _prepare_features(self, df, fit=False):\n        X = df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n        y = df['Label_Num']\n        if fit:\n            X = self.scaler.fit_transform(X)\n        else:\n            X = self.scaler.transform(X)\n        return self.create_sequences(X, y)\n\n    def create_sequences(self, X, y):\n        sequences = []\n        labels = []\n        for i in range(len(X) - self.sequence_length):\n            sequences.append(X[i:i+self.sequence_length])\n            labels.append(y.iloc[i+self.sequence_length-1])\n        return np.array(sequences), np.array(labels)\n    \n    def setup(self, stage=None):\n        self.scaler = StandardScaler()\n        self.scaler.fit(self.X_train.reshape(-1, self.X_train.shape[-1]))\n        \n        self.X_train = self.scaler.transform(self.X_train.reshape(-1, self.X_train.shape[-1])).reshape(self.X_train.shape)\n        self.X_val = self.scaler.transform(self.X_val.reshape(-1, self.X_val.shape[-1])).reshape(self.X_val.shape)\n        self.X_test = self.scaler.transform(self.X_test.reshape(-1, self.X_test.shape[-1])).reshape(self.X_test.shape)\n        \n        self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n        self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n        self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n        print(\"the model will be trained on: \",len(self.train_dataset),\" samples.\")\n        print(\"the model will be validated on: \",len(self.val_dataset),\" samples.\")\n        print(\"the model will be tested on: \",len(self.test_dataset),\" samples.\")\n    \n    def train_dataloader(self):\n        if self.oversample:\n            class_counts = np.bincount(self.y_train)\n            weights = 1. / class_counts[self.y_train]\n            sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n        else:\n            sampler = RandomSampler(self.train_dataset)\n            \n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            sampler=sampler,\n            num_workers=self.num_workers,\n            persistent_workers=True,\n            pin_memory=True\n        )\n    \n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n\n\ndef init_wandb():\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"mohammad_wandb_secret\")\n    wandb.login(key=wandb_api_key)\n    \n    # Initialize the run first\n    run = wandb.init(\n        project=config.wandb.project,\n        entity=config.wandb.entity,\n        tags=config.wandb.tags,\n        notes=config.wandb.notes,\n        config={\n            \"input_size\": None,  # Will be updated later\n            \"num_classes\": None,\n            \"sequence_length\": config.training.sequence_length,\n            \"train_samples\": None,\n            \"test_samples\": None,\n            \"model_config\": dict(config.model),\n            \"training_config\": dict(config.training)\n        }\n    )\n    \n    # Then create the logger\n    wandb_logger = WandbLogger(\n        experiment=run,\n        log_model='all'\n    )\n    \n    return wandb_logger, run\n\ndef main():\n    # Initialize wandb - now returns both logger and run\n    wandb_logger, run = init_wandb()\n    \n    # Initialize data module\n    data_module = NIDSDataModule(config)\n    data_module.prepare_data()\n    data_module.setup()\n    \n    # Get input size from data\n    sample_x, _ = next(iter(data_module.train_dataloader()))\n    input_size = sample_x.shape[2]\n    num_classes = len(data_module.classes)\n    \n    # Update the config with actual values\n    run.config.update({\n        \"input_size\": input_size,\n        \"num_classes\": num_classes,\n        \"train_samples\": len(data_module.train_dataset),\n        \"test_samples\": len(data_module.test_dataset)\n    })\n    \n    model = LSTMModel(input_size, num_classes, config)\n    \n    # Callbacks\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=config.training.early_stopping_patience,\n        mode='min'\n    )\n    \n    # checkpoint_callback = ModelCheckpoint(\n    #     monitor='val_f1',  # Now monitoring F1 score\n    #     mode='max',\n    #     save_top_k=1,\n    #     dirpath='checkpoints',\n    #     filename='best_model'\n    # )\n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_acc',\n        mode='max',\n        save_top_k=1,\n        dirpath='checkpoints',\n        filename='best_model'\n    )\n \n    # Initialize trainer\n    trainer = pl.Trainer(\n        precision=16,\n        logger=wandb_logger,\n        max_epochs=config.training.max_epochs,\n        callbacks=[early_stopping, checkpoint_callback],\n        deterministic=True,\n        gradient_clip_val=1.0,\n        enable_progress_bar=True,\n        log_every_n_steps=1000\n    )\n    \n    # Train model\n    trainer.fit(model, datamodule=data_module)\n    \n    # Test model\n    test_results = trainer.test(model, datamodule=data_module)\n    \n    # Collect all predictions and targets for final evaluation\n    test_loader = data_module.test_dataloader()\n    all_preds = []\n    all_targets = []\n    \n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            x, y = batch\n            y_hat = model(x)\n            preds = torch.argmax(y_hat, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(y.cpu().numpy())\n    \n    # Final metrics calculation\n    test_acc = accuracy_score(all_targets, all_preds)\n    test_f1 = f1_score(all_targets, all_preds, average='weighted')\n    \n    # Log final test metrics\n    wandb.log({\n        'test_acc': test_acc,\n        'test_f1': test_f1,\n        'test_loss': test_results[0]['test_loss']\n    })\n    \n    # Confusion matrix and classification report\n    class_names = data_module.classes.tolist()\n    \n    # Confusion Matrix\n    wandb.log({\n        \"confusion_matrix\": wandb.plot.confusion_matrix(\n            y_true=all_targets,\n            preds=all_preds,\n            class_names=class_names,\n            title=\"Confusion Matrix\"\n        )\n    })\n    \n    # Classification Report\n    report = classification_report(\n        all_targets, all_preds, \n        target_names=class_names,\n        output_dict=True\n    )\n    \n    # Create a wandb Table for the classification report\n    report_table = wandb.Table(columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n    for class_name in class_names:\n        report_table.add_data(\n            class_name,\n            report[class_name][\"precision\"],\n            report[class_name][\"recall\"],\n            report[class_name][\"f1-score\"],\n            report[class_name][\"support\"]\n        )\n    \n    # Add weighted averages\n    report_table.add_data(\n        \"Weighted Avg\",\n        report[\"weighted avg\"][\"precision\"],\n        report[\"weighted avg\"][\"recall\"],\n        report[\"weighted avg\"][\"f1-score\"],\n        report[\"weighted avg\"][\"support\"]\n    )\n    \n    wandb.log({\"classification_report\": report_table})\n    \n    # Finish wandb run\n    wandb.finish()\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T10:57:08.226615Z","iopub.execute_input":"2025-06-19T10:57:08.226859Z","execution_failed":"2025-06-19T11:32:07.286Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohammad-fleity\u001b[0m (\u001b[33mmohammad-fleity-lebanese-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250619_105732-c6jfvd0r</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/c6jfvd0r' target=\"_blank\">hardy-shadow-42</a></strong> to <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/c6jfvd0r' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/c6jfvd0r</a>"},"metadata":{}},{"name":"stdout","text":"1979373\n424152\nthe model will be trained on:  1979368  samples.\nthe model will be validated on:  424147  samples.\nthe model will be tested on:  424147  samples.\n1979373\n424152\nthe model will be trained on:  1979368  samples.\nthe model will be validated on:  424147  samples.\nthe model will be tested on:  424147  samples.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7be7648e3b944b30b6da67407251780e"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"<h2>second dataset</h2>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler, RandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nimport wandb\nfrom omegaconf import OmegaConf\nimport os\nimport warnings\nfrom kaggle_secrets import UserSecretsClient\n\nwarnings.filterwarnings('ignore')\n\n# Optimized Configuration\nconfig = OmegaConf.create({\n    \"wandb\": {\n        \"project\": \"DL-NIDS-2--cic-ton-iot\",\n        \"entity\": \"mohammad-fleity-lebanese-university\",\n        \"tags\": [\"LSTM\", \"CIC-IDS-2017\", \"PyTorch\"],\n        \"notes\": \"Optimized LSTM for network intrusion detection\"\n    },\n    \"model\": {\n        \"hidden_size\": 128,          # Increased capacity\n        \"num_layers\": 2,             # Deeper network\n        \"dropout\": 0.4,              # Stronger regularization\n        \"dense_units\": [128, 64],    # Better feature extraction\n        \"learning_rate\": 0.0001,     # Slower learning\n        \"weight_decay\": 1e-4         # Stronger L2 regularization\n    },\n    \"training\": {\n        \"sequence_length\": 5,        # Longer temporal context\n        \"batch_size\": 128,           # Larger batches\n        \"max_epochs\": 25,            # More training time\n        \"early_stopping_patience\": 7,# More patience\n        \"oversample\": True,          # Class balancing\n        \"gpus\": 1 if torch.cuda.is_available() else 0,\n        \"train_size\": 0.7,           # Proper train/val split\n        \"val_size\": 0.15             # 70/15/15 split\n    },\n    \"data\": {\n        \"raw\": \"cic_ton_iot.parquet\",\n        \"num_workers\": 4\n    }\n})\n\nclass LSTMModel(pl.LightningModule):\n    def __init__(self, input_size, num_classes, config):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Enhanced LSTM with layer normalization\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=config.model.hidden_size,\n            num_layers=config.model.num_layers,\n            batch_first=True,\n            dropout=config.model.dropout if config.model.num_layers > 1 else 0\n        )\n        \n        self.lstm_ln = nn.LayerNorm(config.model.hidden_size)  # Better for sequences\n        \n        # Improved dense layers\n        self.dense = nn.Sequential(\n            nn.Linear(config.model.hidden_size, config.model.dense_units[0]),\n            nn.LayerNorm(config.model.dense_units[0]),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(config.model.dense_units[0], config.model.dense_units[1]),\n            nn.LayerNorm(config.model.dense_units[1]),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout)\n        )\n        \n        self.output = nn.Linear(config.model.dense_units[1], num_classes)\n        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Regularization\n\n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        lstm_out = lstm_out[:, -1, :]  # Last timestep\n        lstm_out = self.lstm_ln(lstm_out)\n        features = self.dense(lstm_out)\n        return self.output(features)\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('train_loss_epoch', loss, prog_bar=True)\n        self.log('train_acc_epoch', acc*100, prog_bar=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', acc*100, prog_bar=True)\n    \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('test_loss', loss)\n        self.log('test_acc', acc*100)\n    \n    def configure_optimizers(self):\n        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.config.model.learning_rate,\n                                weight_decay=self.hparams.config.model.weight_decay)\n        return optimizer\n\nclass NIDSDataModule(pl.LightningDataModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.batch_size = config.training.batch_size\n        self.sequence_length = config.training.sequence_length\n        self.num_workers = config.data.num_workers\n        self.oversample = config.training.oversample\n        self.alpha = 0.5\n\n    def prepare_data(self):\n        # /kaggle/input/cic-ton-iot-parquet\n        df = pd.read_parquet(os.path.join('/kaggle/input/cic-ton-iot-parquet', self.config.data.raw))\n    \n        # Clean data\n        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df.dropna(inplace=True)\n        df.drop_duplicates(inplace=True)\n    \n        # Identify non-numeric columns\n        self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', \n                                 'Src Port', 'Attack', 'Dst IP', 'Dst Port', 'Protocol']\n        self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n    \n        # Encode labels\n        self.label_encoder = LabelEncoder()\n        df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n        self.classes = self.label_encoder.classes_\n    \n        # Initialize scaler\n        self.scaler = StandardScaler()\n    \n        # Train/Val/Test split\n        train_df, test_df = train_test_split(\n            df,\n            test_size=1 - self.config.training.train_size,\n            random_state=42,\n            stratify=df['Label_Num']\n        )\n        val_df, test_df = train_test_split(\n            test_df,\n            test_size=0.5,\n            random_state=42,\n            stratify=test_df['Label_Num']\n        )\n        print(len(train_df))\n        print(len(test_df))\n        train_max_rows = 300000\n        if len(train_df) > train_max_rows:\n            train_df = train_df.sample(n=train_max_rows, random_state=42)\n        val_max_rows = 45000\n        if len(val_df) > val_max_rows:\n            val_df = val_df.sample(n=val_max_rows, random_state=42)\n\n        # max_rows = 90_000\n        # if len(train_df) > max_rows:\n        #     train_df = train_df.sample(n=max_rows, random_state=42)\n\n        # Use `fit=True` only for training data\n        self.X_train, self.y_train = self._prepare_features(train_df, fit=True)\n        self.X_val, self.y_val = self._prepare_features(val_df, fit=False)\n        self.X_test, self.y_test = self._prepare_features(test_df, fit=False)\n\n    \n    # def prepare_data(self):\n    #     df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n        \n    #     # Clean data\n    #     df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    #     df.dropna(inplace=True)\n    #     df.drop_duplicates(inplace=True)\n        \n    #     # Identify non-numeric columns\n    #     self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', \n    #                            'Src Port', 'Attack', 'Dst IP', 'Dst Port', 'Protocol']\n    #     self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n        \n    #     # Encode labels\n    #     self.label_encoder = LabelEncoder()\n    #     df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n    #     self.classes = self.label_encoder.classes_\n    #     self.scaler = StandardScaler()\n\n    #     # Proper train/val/test split\n    #     train_df, test_df = train_test_split(\n    #         df, \n    #         test_size=1 - self.config.training.train_size,\n    #         random_state=42,\n    #         stratify=df['Label_Num']\n    #     )\n    #     val_df, test_df = train_test_split(\n    #         test_df,\n    #         test_size=0.5,  # Splits remaining 30% into 15% val, 15% test\n    #         random_state=42,\n    #         stratify=test_df['Label_Num']\n    #     )\n        \n    #     # Process each split\n    #     self.X_train, self.y_train = self._prepare_features(train_df)\n    #     self.X_val, self.y_val = self._prepare_features(val_df)\n    #     self.X_test, self.y_test = self._prepare_features(test_df)\n \n    # def _prepare_features(self, df):\n    #     X = df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n    #     y = df['Label_Num']\n    #     X = self.scaler.transform(X) if hasattr(self, 'scaler') else self.scaler.fit_transform(X)\n    #     return self.create_sequences(X, y)\n    \n    def _prepare_features(self, df, fit=False):\n        X = df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n        y = df['Label_Num']\n        if fit:\n            X = self.scaler.fit_transform(X)\n        else:\n            X = self.scaler.transform(X)\n        return self.create_sequences(X, y)\n\n    def create_sequences(self, X, y):\n        sequences = []\n        labels = []\n        for i in range(len(X) - self.sequence_length):\n            sequences.append(X[i:i+self.sequence_length])\n            labels.append(y.iloc[i+self.sequence_length-1])\n        return np.array(sequences), np.array(labels)\n    \n    def setup(self, stage=None):\n        self.scaler = StandardScaler()\n        self.scaler.fit(self.X_train.reshape(-1, self.X_train.shape[-1]))\n        \n        self.X_train = self.scaler.transform(self.X_train.reshape(-1, self.X_train.shape[-1])).reshape(self.X_train.shape)\n        self.X_val = self.scaler.transform(self.X_val.reshape(-1, self.X_val.shape[-1])).reshape(self.X_val.shape)\n        self.X_test = self.scaler.transform(self.X_test.reshape(-1, self.X_test.shape[-1])).reshape(self.X_test.shape)\n        \n        self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n        self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n        self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n        print(\"the model will be trained on: \",len(self.train_dataset),\" samples.\")\n        print(\"the model will be validated on: \",len(self.val_dataset),\" samples.\")\n        print(\"the model will be tested on: \",len(self.test_dataset),\" samples.\")\n    \n    def train_dataloader(self):\n        if self.oversample:\n            class_counts = np.bincount(self.y_train)\n            weights = 1. / class_counts[self.y_train]\n            sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n        else:\n            sampler = RandomSampler(self.train_dataset)\n            \n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            sampler=sampler,\n            num_workers=self.num_workers,\n            persistent_workers=True,\n            pin_memory=True\n        )\n    \n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n\n\ndef init_wandb():\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"mohammad_wandb_secret\")\n    wandb.login(key=wandb_api_key)\n    \n    # Initialize the run first\n    run = wandb.init(\n        project=config.wandb.project,\n        entity=config.wandb.entity,\n        tags=config.wandb.tags,\n        notes=config.wandb.notes,\n        config={\n            \"input_size\": None,  # Will be updated later\n            \"num_classes\": None,\n            \"sequence_length\": config.training.sequence_length,\n            \"train_samples\": None,\n            \"test_samples\": None,\n            \"model_config\": dict(config.model),\n            \"training_config\": dict(config.training)\n        }\n    )\n    \n    # Then create the logger\n    wandb_logger = WandbLogger(\n        experiment=run,\n        log_model='all'\n    )\n    \n    return wandb_logger, run\n\ndef main():\n    # Initialize wandb - now returns both logger and run\n    wandb_logger, run = init_wandb()\n    \n    # Initialize data module\n    data_module = NIDSDataModule(config)\n    data_module.prepare_data()\n    data_module.setup()\n    \n    # Get input size from data\n    sample_x, _ = next(iter(data_module.train_dataloader()))\n    input_size = sample_x.shape[2]\n    num_classes = len(data_module.classes)\n    \n    # Update the config with actual values\n    run.config.update({\n        \"input_size\": input_size,\n        \"num_classes\": num_classes,\n        \"train_samples\": len(data_module.train_dataset),\n        \"test_samples\": len(data_module.test_dataset)\n    })\n    \n    model = LSTMModel(input_size, num_classes, config)\n    \n    # Callbacks\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=config.training.early_stopping_patience,\n        mode='min'\n    )\n    \n    # checkpoint_callback = ModelCheckpoint(\n    #     monitor='val_f1',  # Now monitoring F1 score\n    #     mode='max',\n    #     save_top_k=1,\n    #     dirpath='checkpoints',\n    #     filename='best_model'\n    # )\n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_acc',\n        mode='max',\n        save_top_k=1,\n        dirpath='checkpoints',\n        filename='best_model'\n    )\n \n    # Initialize trainer\n    trainer = pl.Trainer(\n        precision=16,\n        logger=wandb_logger,\n        max_epochs=config.training.max_epochs,\n        callbacks=[early_stopping, checkpoint_callback],\n        deterministic=True,\n        gradient_clip_val=1.0,\n        enable_progress_bar=True,\n        log_every_n_steps=1000\n    )\n    \n    # Train model\n    trainer.fit(model, datamodule=data_module)\n    \n    # Test model\n    test_results = trainer.test(model, datamodule=data_module)\n    \n    # Collect all predictions and targets for final evaluation\n    test_loader = data_module.test_dataloader()\n    all_preds = []\n    all_targets = []\n    \n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            x, y = batch\n            y_hat = model(x)\n            preds = torch.argmax(y_hat, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(y.cpu().numpy())\n    \n    # Final metrics calculation\n    test_acc = accuracy_score(all_targets, all_preds)\n    test_f1 = f1_score(all_targets, all_preds, average='weighted')\n    \n    # Log final test metrics\n    wandb.log({\n        'test_acc': test_acc,\n        'test_f1': test_f1,\n        'test_loss': test_results[0]['test_loss']\n    })\n    \n    # Confusion matrix and classification report\n    class_names = data_module.classes.tolist()\n    \n    # Confusion Matrix\n    wandb.log({\n        \"confusion_matrix\": wandb.plot.confusion_matrix(\n            y_true=all_targets,\n            preds=all_preds,\n            class_names=class_names,\n            title=\"Confusion Matrix\"\n        )\n    })\n    \n    # Classification Report\n    report = classification_report(\n        all_targets, all_preds, \n        target_names=class_names,\n        output_dict=True\n    )\n    \n    # Create a wandb Table for the classification report\n    report_table = wandb.Table(columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n    for class_name in class_names:\n        report_table.add_data(\n            class_name,\n            report[class_name][\"precision\"],\n            report[class_name][\"recall\"],\n            report[class_name][\"f1-score\"],\n            report[class_name][\"support\"]\n        )\n    \n    # Add weighted averages\n    report_table.add_data(\n        \"Weighted Avg\",\n        report[\"weighted avg\"][\"precision\"],\n        report[\"weighted avg\"][\"recall\"],\n        report[\"weighted avg\"][\"f1-score\"],\n        report[\"weighted avg\"][\"support\"]\n    )\n    \n    wandb.log({\"classification_report\": report_table})\n    \n    # Finish wandb run\n    wandb.finish()\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T18:12:18.255092Z","iopub.execute_input":"2025-06-20T18:12:18.255346Z","iopub.status.idle":"2025-06-20T18:33:57.917931Z","shell.execute_reply.started":"2025-06-20T18:12:18.255326Z","shell.execute_reply":"2025-06-20T18:33:57.916712Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohammad-fleity\u001b[0m (\u001b[33mmohammad-fleity-lebanese-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250620_181243-rzrjm6m0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot/runs/rzrjm6m0' target=\"_blank\">vivid-sponge-50</a></strong> to <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot/runs/rzrjm6m0' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot/runs/rzrjm6m0</a>"},"metadata":{}},{"name":"stdout","text":"3745408\n802588\nthe model will be trained on:  299995  samples.\nthe model will be validated on:  44995  samples.\nthe model will be tested on:  802583  samples.\n3745408\n802588\nthe model will be trained on:  299995  samples.\nthe model will be validated on:  44995  samples.\nthe model will be tested on:  802583  samples.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b39db8e37f74360819e9d2493bed885"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"3745408\n802588\nthe model will be trained on:  299995  samples.\nthe model will be validated on:  44995  samples.\nthe model will be tested on:  802583  samples.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Testing: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da015d4788d042718cba935f2d4fa23f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    98.03202056884766    \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.23894713819026947   \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     98.03202056884766     </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.23894713819026947    </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/1359971509.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_35/1359971509.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Add weighted averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     report_table.add_data(\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0;34m\"Weighted Avg\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0mreport\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weighted avg\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"precision\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/table.py\u001b[0m in \u001b[0;36madd_data\u001b[0;34m(self, *data)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;31m# Update the table's column types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mresult_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_updated_result_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_column_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/table.py\u001b[0m in \u001b[0;36m_get_updated_result_type\u001b[0;34m(self, row)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0mresult_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincoming_row_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    471\u001b[0m                 \"Data row contained incompatible types:\\n{}\".format(\n\u001b[1;32m    472\u001b[0m                     \u001b[0mcurrent_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincoming_row_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Data row contained incompatible types:\n{'Class': 'Weighted Avg', 'Precision': 0.9803419318795807, 'Recall': 0.9803135625848043, 'F1-Score': 0.9803077994903139, 'Support': 802583} of type {'Class': String, 'Precision': Number, 'Recall': Number, 'F1-Score': Number, 'Support': Number} is not assignable to {'Class': None or Number, 'Precision': None or Number, 'Recall': None or Number, 'F1-Score': None or Number, 'Support': None or Number}\nKey 'Class':\n\tString not assignable to None or Number\n\t\tString not assignable to None\n\tand\n\t\tString not assignable to Number"],"ename":"TypeError","evalue":"Data row contained incompatible types:\n{'Class': 'Weighted Avg', 'Precision': 0.9803419318795807, 'Recall': 0.9803135625848043, 'F1-Score': 0.9803077994903139, 'Support': 802583} of type {'Class': String, 'Precision': Number, 'Recall': Number, 'F1-Score': Number, 'Support': Number} is not assignable to {'Class': None or Number, 'Precision': None or Number, 'Recall': None or Number, 'F1-Score': None or Number, 'Support': None or Number}\nKey 'Class':\n\tString not assignable to None or Number\n\t\tString not assignable to None\n\tand\n\t\tString not assignable to Number","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}