{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8089266,"sourceType":"datasetVersion","datasetId":4775518},{"sourceId":8089281,"sourceType":"datasetVersion","datasetId":4775527}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_parquet('/kaggle/input/cic-ids-2017-parquet/cic_ids_2017.parquet')\ndf.head()\n# df.columns_type","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:43:28.767580Z","iopub.execute_input":"2025-06-18T14:43:28.768294Z","iopub.status.idle":"2025-06-18T14:43:40.277185Z","shell.execute_reply.started":"2025-06-18T14:43:28.768271Z","shell.execute_reply":"2025-06-18T14:43:40.276637Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"262024980"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"df.columns\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:48:12.319586Z","iopub.execute_input":"2025-06-18T14:48:12.320196Z","iopub.status.idle":"2025-06-18T14:48:12.327061Z","shell.execute_reply.started":"2025-06-18T14:48:12.320173Z","shell.execute_reply":"2025-06-18T14:48:12.326536Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"Index(['Flow ID', 'Src IP', 'Src Port', 'Dst IP', 'Dst Port', 'Protocol',\n       'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts',\n       'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max',\n       'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std',\n       'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean',\n       'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean',\n       'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot',\n       'Fwd IAT Mean', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot',\n       'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min',\n       'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags',\n       'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s',\n       'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std',\n       'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt',\n       'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count',\n       'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg',\n       'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg',\n       'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg',\n       'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts',\n       'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts',\n       'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min',\n       'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\n       'Idle Std', 'Idle Max', 'Idle Min', 'Label', 'Attack'],\n      dtype='object')"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T14:48:23.824001Z","iopub.execute_input":"2025-06-18T14:48:23.824597Z","iopub.status.idle":"2025-06-18T14:48:23.829130Z","shell.execute_reply.started":"2025-06-18T14:48:23.824574Z","shell.execute_reply":"2025-06-18T14:48:23.828401Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(3119345, 84)"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# new method \nclass TimeSeriesDataset(Dataset):\n    def __init__(self, data, sequence_length, target_idx=None):\n        \"\"\"\n        Dataset for time series data that creates sequences on-the-fly.\n \n        Args:\n            data: Input data tensor of shape (n_samples, n_features)\n            sequence_length: Length of sequences to create\n            target_idx: Optional tensor of target indices. If None, uses the last position in sequence\n        \"\"\"\n        self.data = data\n        self.sequence_length = sequence_length\n        self.target_idx = target_idx\n \n    def __len__(self):\n        return len(self.data)\n \n    def __getitem__(self, idx):\n        # Calculate start index for the sequence\n        start_idx = max(0, idx - self.sequence_length + 1)\n \n        # Get the sequence\n        sequence = self.data[start_idx:idx + 1]\n \n        # If sequence is shorter than sequence_length, pad with zeros at the beginning\n        if len(sequence) < self.sequence_length:\n            padding = torch.zeros(self.sequence_length -\n                                  len(sequence), sequence.shape[1])\n            sequence = torch.cat([padding, sequence], dim=0)\n \n        # Get target index (either from target_idx or use the last position)\n        target_idx = self.target_idx[idx] if self.target_idx is not None else -1\n \n        return sequence, target_idx\n \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler, RandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nimport wandb\nfrom omegaconf import OmegaConf\nimport os\nimport warnings\nfrom kaggle_secrets import UserSecretsClient\n\nwarnings.filterwarnings('ignore')\n\n# Optimized Configuration\nconfig = OmegaConf.create({\n    \"wandb\": {\n        \"project\": \"DL-NIDS-2--cic-ids-2017\",\n        \"entity\": \"mohammad-fleity-lebanese-university\",\n        \"tags\": [\"LSTM\", \"CIC-IDS-2017\", \"PyTorch\"],\n        \"notes\": \"Optimized LSTM for network intrusion detection\"\n    },\n    \"model\": {\n        \"hidden_size\": 128,          # Increased capacity\n        \"num_layers\": 2,             # Deeper network\n        \"dropout\": 0.4,              # Stronger regularization\n        \"dense_units\": [128, 64],    # Better feature extraction\n        \"learning_rate\": 0.0001,     # Slower learning\n        \"weight_decay\": 1e-4         # Stronger L2 regularization\n    },\n    \"training\": {\n        \"sequence_length\": 5,        # Longer temporal context\n        \"batch_size\": 128,           # Larger batches\n        \"max_epochs\": 7,            # More training time\n        \"early_stopping_patience\": 7,# More patience\n        \"oversample\": True,          # Class balancing\n        \"gpus\": 1 if torch.cuda.is_available() else 0,\n        \"train_size\": 0.7,           # Proper train/val split\n        \"val_size\": 0.15             # 70/15/15 split\n    },\n    \"data\": {\n        \"raw\": \"cic_ids_2017.parquet\",\n        \"num_workers\": 4\n    }\n})\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, data, sequence_length, target_idx=None):\n        \"\"\"\n        Dataset for time series data that creates sequences on-the-fly.\n \n        Args:\n            data: Input data tensor of shape (n_samples, n_features)\n            sequence_length: Length of sequences to create\n            target_idx: Optional tensor of target indices. If None, uses the last position in sequence\n        \"\"\"\n        self.data = data\n        self.sequence_length = sequence_length\n        self.target_idx = target_idx\n \n    def __len__(self):\n        return len(self.data)\n \n    def __getitem__(self, idx):\n        # Calculate start index for the sequence\n        start_idx = max(0, idx - self.sequence_length + 1)\n        \n        # Get the sequence\n        sequence = self.data[start_idx:idx + 1]\n        \n        # Ensure sequence is 2D [seq_len, features]\n        if sequence.dim() == 1:\n            sequence = sequence.unsqueeze(0)  # Add sequence length dimension if missing\n        \n        # Pad the beginning if needed\n        if len(sequence) < self.sequence_length:\n            padding = torch.zeros(self.sequence_length - len(sequence), sequence.shape[1])\n            sequence = torch.cat([padding, sequence], dim=0)\n        \n        # Get target\n        target = self.target_idx[idx] if self.target_idx is not None else -1\n        \n        return sequence, target\n\n\n\nclass LSTMModel(pl.LightningModule):\n    def __init__(self, input_size, num_classes, config):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Enhanced LSTM with layer normalization\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=config.model.hidden_size,\n            num_layers=config.model.num_layers,\n            batch_first=True,\n            dropout=config.model.dropout if config.model.num_layers > 1 else 0\n        )\n        \n        self.lstm_ln = nn.LayerNorm(config.model.hidden_size)  # Better for sequences\n        \n        # Improved dense layers\n        self.dense = nn.Sequential(\n            nn.Linear(config.model.hidden_size, config.model.dense_units[0]),\n            nn.LayerNorm(config.model.dense_units[0]),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(config.model.dense_units[0], config.model.dense_units[1]),\n            nn.LayerNorm(config.model.dense_units[1]),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout)\n        )\n        \n        self.output = nn.Linear(config.model.dense_units[1], num_classes)\n        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Regularization\n\n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        lstm_out = lstm_out[:, -1, :]  # Last timestep\n        lstm_out = self.lstm_ln(lstm_out)\n        features = self.dense(lstm_out)\n        return self.output(features)\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('train_loss_epoch', loss, prog_bar=True)\n        self.log('train_acc_epoch', acc*100, prog_bar=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', acc*100, prog_bar=True)\n    \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('test_loss', loss)\n        self.log('test_acc', acc*100)\n    \n    def configure_optimizers(self):\n        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.config.model.learning_rate,\n                                weight_decay=self.hparams.config.model.weight_decay)\n        return optimizer\n\nclass NIDSDataModule(pl.LightningDataModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.batch_size = config.training.batch_size\n        self.sequence_length = config.training.sequence_length\n        self.num_workers = config.data.num_workers\n        self.oversample = config.training.oversample\n        self.alpha = 0.5\n\n    def prepare_data(self):\n        df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n        \n        # Clean data\n        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df.dropna(inplace=True)\n        df.drop_duplicates(inplace=True)\n        \n        # Reset index after cleaning\n        df = df.reset_index(drop=True)\n        \n        # Identify non-numeric columns\n        self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', \n                                 'Src Port', 'Attack', 'Dst IP', 'Dst Port', 'Protocol']\n        self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n        \n        # Encode labels\n        self.label_encoder = LabelEncoder()\n        df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n        self.classes = self.label_encoder.classes_\n        \n        # Initialize scaler\n        self.scaler = StandardScaler()\n        \n        # Train/Val/Test split\n        train_df, test_df = train_test_split(\n            df,\n            test_size=1 - self.config.training.train_size,\n            random_state=42,\n            stratify=df['Label_Num']\n        )\n        val_df, test_df = train_test_split(\n            test_df,\n            test_size=0.5,\n            random_state=42,\n            stratify=test_df['Label_Num']\n        )\n        \n        # Reset indices after splitting\n        train_df = train_df.reset_index(drop=True)\n        val_df = val_df.reset_index(drop=True)\n        test_df = test_df.reset_index(drop=True)\n        \n        # Use `fit=True` only for training data\n        self.X_train, self.y_train = self._prepare_features(train_df, fit=True)\n        self.X_val, self.y_val = self._prepare_features(val_df, fit=False)\n        self.X_test, self.y_test = self._prepare_features(test_df, fit=False)\n\n    def _prepare_features(self, df, fit=False):\n        X = df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n        y = df['Label_Num']\n        if fit:\n            X = self.scaler.fit_transform(X)\n        else:\n            X = self.scaler.transform(X)\n        # return self.create_sequences(X, y)\n        return X, y\n    # def create_sequences(self, X, y):\n    #     sequences = []\n    #     labels = []\n    #     for i in range(len(X) - self.sequence_length):\n    #         sequences.append(X[i:i+self.sequence_length])\n    #         labels.append(y.iloc[i+self.sequence_length-1])\n    #     return np.array(sequences), np.array(labels)\n        \n    def create_sequences(self, X, y):\n        sequences = []\n        labels = []\n        y_values = y.values if hasattr(y, 'values') else y  # Convert to numpy array if pandas Series\n        for i in range(len(X) - self.sequence_length):\n            seq = X[i:i+self.sequence_length]\n            sequences.append(seq)\n            labels.append(y_values[i+self.sequence_length-1])\n        return np.array(sequences), np.array(labels)\n       \n    def setup(self, stage=None):\n        self.scaler = StandardScaler()\n        self.scaler.fit(self.X_train.reshape(-1, self.X_train.shape[-1]))\n        \n        self.X_train = self.scaler.transform(self.X_train.reshape(-1, self.X_train.shape[-1])).reshape(self.X_train.shape)\n        self.X_val = self.scaler.transform(self.X_val.reshape(-1, self.X_val.shape[-1])).reshape(self.X_val.shape)\n        self.X_test = self.scaler.transform(self.X_test.reshape(-1, self.X_test.shape[-1])).reshape(self.X_test.shape)\n        \n        # self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n        # self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n        # self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n\n        self.train_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_train),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_train)\n        )\n        \n        self.val_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_val),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_val)\n        )\n        \n        self.test_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_test),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_test)\n        )\n        \n        print(\"the model will be trained on: \",len(self.train_dataset),\" samples.\")\n        print(\"the model will be validated on: \",len(self.val_dataset),\" samples.\")\n        print(\"the model will be tested on: \",len(self.test_dataset),\" samples.\")\n    \n    def train_dataloader(self):\n        if self.oversample:\n            class_counts = np.bincount(self.y_train)\n            weights = 1. / class_counts[self.y_train]\n            sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n        else:\n            sampler = RandomSampler(self.train_dataset)\n            \n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            sampler=sampler,\n            num_workers=self.num_workers,\n            persistent_workers=True,\n            pin_memory=True\n        )\n    \n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n\n\ndef init_wandb():\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"mohammad_wandb_secret\")\n    wandb.login(key=wandb_api_key)\n    \n    # Initialize the run first\n    run = wandb.init(\n        project=config.wandb.project,\n        entity=config.wandb.entity,\n        tags=config.wandb.tags,\n        notes=config.wandb.notes,\n        config={\n            \"input_size\": None,  # Will be updated later\n            \"num_classes\": None,\n            \"sequence_length\": config.training.sequence_length,\n            \"train_samples\": None,\n            \"test_samples\": None,\n            \"model_config\": dict(config.model),\n            \"training_config\": dict(config.training)\n        }\n    )\n    \n    # Then create the logger\n    wandb_logger = WandbLogger(\n        experiment=run,\n        log_model='all'\n    )\n    \n    return wandb_logger, run\n\ndef main():\n    # Initialize wandb - now returns both logger and run\n    wandb_logger, run = init_wandb()\n    \n    # Initialize data module\n    data_module = NIDSDataModule(config)\n    data_module.prepare_data()\n    data_module.setup()\n    \n    # Get input size from data\n    sample_x, _ = next(iter(data_module.train_dataloader()))\n    input_size = sample_x.shape[2]\n    num_classes = len(data_module.classes)\n    \n    # Update the config with actual values\n    run.config.update({\n        \"input_size\": input_size,\n        \"num_classes\": num_classes,\n        \"train_samples\": len(data_module.train_dataset),\n        \"test_samples\": len(data_module.test_dataset)\n    })\n    \n    model = LSTMModel(input_size, num_classes, config)\n    \n    # Callbacks\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=config.training.early_stopping_patience,\n        mode='min'\n    )\n    \n    # checkpoint_callback = ModelCheckpoint(\n    #     monitor='val_f1',  # Now monitoring F1 score\n    #     mode='max',\n    #     save_top_k=1,\n    #     dirpath='checkpoints',\n    #     filename='best_model'\n    # )\n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_acc',\n        mode='max',\n        save_top_k=1,\n        dirpath='checkpoints',\n        filename='best_model'\n    )\n \n    # Initialize trainer\n    trainer = pl.Trainer(\n        precision=16,\n        logger=wandb_logger,\n        max_epochs=config.training.max_epochs,\n        callbacks=[early_stopping, checkpoint_callback],\n        deterministic=True,\n        gradient_clip_val=1.0,\n        enable_progress_bar=True,\n        log_every_n_steps=1000\n    )\n    \n    # Train model\n    trainer.fit(model, datamodule=data_module)\n    \n    # Test model\n    test_results = trainer.test(model, datamodule=data_module)\n    \n    # Collect all predictions and targets for final evaluation\n    test_loader = data_module.test_dataloader()\n    all_preds = []\n    all_targets = []\n    \n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            x, y = batch\n            y_hat = model(x)\n            preds = torch.argmax(y_hat, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(y.cpu().numpy())\n    \n    # Final metrics calculation\n    test_acc = accuracy_score(all_targets, all_preds)\n    test_f1 = f1_score(all_targets, all_preds, average='weighted')\n    \n    # Log final test metrics\n    wandb.log({\n        'test_acc': test_acc,\n        'test_f1': test_f1,\n        'test_loss': test_results[0]['test_loss']\n    })\n    \n    # Confusion matrix and classification report\n    class_names = data_module.classes.tolist()\n    \n    # Confusion Matrix\n    wandb.log({\n        \"confusion_matrix\": wandb.plot.confusion_matrix(\n            y_true=all_targets,\n            preds=all_preds,\n            class_names=class_names,\n            title=\"Confusion Matrix\"\n        )\n    })\n    \n    # Classification Report\n    report = classification_report(\n        all_targets, all_preds, \n        target_names=class_names,\n        output_dict=True\n    )\n    \n    # Create a wandb Table for the classification report\n    report_table = wandb.Table(columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n    for class_name in class_names:\n        report_table.add_data(\n            class_name,\n            report[class_name][\"precision\"],\n            report[class_name][\"recall\"],\n            report[class_name][\"f1-score\"],\n            report[class_name][\"support\"]\n        )\n    \n    # Add weighted averages\n    report_table.add_data(\n        \"Weighted Avg\",\n        report[\"weighted avg\"][\"precision\"],\n        report[\"weighted avg\"][\"recall\"],\n        report[\"weighted avg\"][\"f1-score\"],\n        report[\"weighted avg\"][\"support\"]\n    )\n    \n    wandb.log({\"classification_report\": report_table})\n    \n    # Finish wandb run\n    wandb.finish()\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T08:57:12.363015Z","iopub.execute_input":"2025-06-27T08:57:12.363266Z","execution_failed":"2025-06-27T09:43:38.025Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohammad-fleity\u001b[0m (\u001b[33mmohammad-fleity-lebanese-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250627_085737-94clwjpi</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/94clwjpi' target=\"_blank\">legendary-puddle-79</a></strong> to <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/94clwjpi' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/94clwjpi</a>"},"metadata":{}},{"name":"stdout","text":"the model will be trained on:  1979373  samples.\nthe model will be validated on:  424152  samples.\nthe model will be tested on:  424152  samples.\nthe model will be trained on:  1979373  samples.\nthe model will be validated on:  424152  samples.\nthe model will be tested on:  424152  samples.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae13c0ac268849d1ba335f876be38f23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"<h2>second dataset</h2>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset,TensorDataset, WeightedRandomSampler, RandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nimport wandb\nfrom omegaconf import OmegaConf\nimport os\nimport warnings\nfrom kaggle_secrets import UserSecretsClient\n\nwarnings.filterwarnings('ignore')\n\n# Optimized Configuration\nconfig = OmegaConf.create({\n    \"wandb\": {\n        \"project\": \"DL-NIDS-2--cic-ton-iot\",\n        \"entity\": \"mohammad-fleity-lebanese-university\",\n        \"tags\": [\"LSTM\", \"CIC-IDS-2017\", \"PyTorch\"],\n        \"notes\": \"Optimized LSTM for network intrusion detection\"\n    },\n    \"model\": {\n        \"hidden_size\": 128,          # Increased capacity\n        \"num_layers\": 2,             # Deeper network\n        \"dropout\": 0.4,              # Stronger regularization\n        \"dense_units\": [128, 64],    # Better feature extraction\n        \"learning_rate\": 0.0001,     # Slower learning\n        \"weight_decay\": 1e-4         # Stronger L2 regularization\n    },\n    \"training\": {\n        \"sequence_length\": 5,        # Longer temporal context\n        \"batch_size\": 128,           # Larger batches\n        \"max_epochs\": 10,            # More training time\n        \"early_stopping_patience\": 7,# More patience\n        \"oversample\": True,          # Class balancing\n        \"gpus\": 1 if torch.cuda.is_available() else 0,\n        \"train_size\": 0.7,           # Proper train/val split\n        \"val_size\": 0.15             # 70/15/15 split\n    },\n    \"data\": {\n        \"raw\": \"cic_ton_iot.parquet\",\n        \"num_workers\": 4\n    }\n})\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, data, sequence_length, target_idx=None):\n        \"\"\"\n        Dataset for time series data that creates sequences on-the-fly.\n \n        Args:\n            data: Input data tensor of shape (n_samples, n_features)\n            sequence_length: Length of sequences to create\n            target_idx: Optional tensor of target indices. If None, uses the last position in sequence\n        \"\"\"\n        self.data = data\n        self.sequence_length = sequence_length\n        self.target_idx = target_idx\n \n    def __len__(self):\n        return len(self.data)\n \n    def __getitem__(self, idx):\n        # Calculate start index for the sequence\n        start_idx = max(0, idx - self.sequence_length + 1)\n        \n        # Get the sequence\n        sequence = self.data[start_idx:idx + 1]\n        \n        # Ensure sequence is 2D [seq_len, features]\n        if sequence.dim() == 1:\n            sequence = sequence.unsqueeze(0)  # Add sequence length dimension if missing\n        \n        # Pad the beginning if needed\n        if len(sequence) < self.sequence_length:\n            padding = torch.zeros(self.sequence_length - len(sequence), sequence.shape[1])\n            sequence = torch.cat([padding, sequence], dim=0)\n        \n        # Get target\n        target = self.target_idx[idx] if self.target_idx is not None else -1\n        \n        return sequence, target\n\nclass LSTMModel(pl.LightningModule):\n    def __init__(self, input_size, num_classes, config):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Enhanced LSTM with layer normalization\n        self.lstm = nn.LSTM(\n            input_size=input_size,\n            hidden_size=config.model.hidden_size,\n            num_layers=config.model.num_layers,\n            batch_first=True,\n            dropout=config.model.dropout if config.model.num_layers > 1 else 0\n        )\n        \n        self.lstm_ln = nn.LayerNorm(config.model.hidden_size)  # Better for sequences\n        \n        # Improved dense layers\n        self.dense = nn.Sequential(\n            nn.Linear(config.model.hidden_size, config.model.dense_units[0]),\n            nn.LayerNorm(config.model.dense_units[0]),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(config.model.dense_units[0], config.model.dense_units[1]),\n            nn.LayerNorm(config.model.dense_units[1]),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout)\n        )\n        \n        self.output = nn.Linear(config.model.dense_units[1], num_classes)\n        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Regularization\n\n    def forward(self, x):\n        lstm_out, _ = self.lstm(x)\n        lstm_out = lstm_out[:, -1, :]  # Last timestep\n        lstm_out = self.lstm_ln(lstm_out)\n        features = self.dense(lstm_out)\n        return self.output(features)\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('train_loss_epoch', loss, prog_bar=True)\n        self.log('train_acc_epoch', acc*100, prog_bar=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', acc*100, prog_bar=True)\n    \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log('test_loss', loss)\n        self.log('test_acc', acc*100)\n    \n    def configure_optimizers(self):\n        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.config.model.learning_rate,\n                                weight_decay=self.hparams.config.model.weight_decay)\n        return optimizer\n\nclass NIDSDataModule(pl.LightningDataModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.batch_size = config.training.batch_size\n        self.sequence_length = config.training.sequence_length\n        self.num_workers = config.data.num_workers\n        self.oversample = config.training.oversample\n        self.alpha = 0.5\n\n    def prepare_data(self):\n        # /kaggle/input/cic-ton-iot-parquet\n        df = pd.read_parquet(os.path.join('/kaggle/input/cic-ton-iot-parquet', self.config.data.raw))\n    \n        # Clean data\n        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df.dropna(inplace=True)\n        df.drop_duplicates(inplace=True)\n    \n        # Identify non-numeric columns\n        self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', \n                                 'Src Port', 'Attack', 'Dst IP', 'Dst Port', 'Protocol']\n        self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n    \n        # Encode labels\n        self.label_encoder = LabelEncoder()\n        df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n        self.classes = self.label_encoder.classes_\n    \n        # Initialize scaler\n        self.scaler = StandardScaler()\n    \n        # Train/Val/Test split\n        train_df, test_df = train_test_split(\n            df,\n            test_size=1 - self.config.training.train_size,\n            random_state=42,\n            stratify=df['Label_Num']\n        )\n        val_df, test_df = train_test_split(\n            test_df,\n            test_size=0.5,\n            random_state=42,\n            stratify=test_df['Label_Num']\n        )\n        print(len(train_df))\n        print(len(test_df))\n        # train_max_rows = 300000\n        # if len(train_df) > train_max_rows:\n        #     train_df = train_df.sample(n=train_max_rows, random_state=42)\n        # val_max_rows = 45000\n        # if len(val_df) > val_max_rows:\n        #     val_df = val_df.sample(n=val_max_rows, random_state=42)\n        # max_rows = 90_000\n        # if len(train_df) > max_rows:\n        #     train_df = train_df.sample(n=max_rows, random_state=42)\n\n        # Use `fit=True` only for training data\n        self.X_train, self.y_train = self._prepare_features(train_df, fit=True)\n        self.X_val, self.y_val = self._prepare_features(val_df, fit=False)\n        self.X_test, self.y_test = self._prepare_features(test_df, fit=False)\n\n    \n    def _prepare_features(self, df, fit=False):\n        X = df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n        y = df['Label_Num']\n        if fit:\n            X = self.scaler.fit_transform(X)\n        else:\n            X = self.scaler.transform(X)\n        # return self.create_sequences(X, y)\n        return X, y\n\n    # def create_sequences(self, X, y):\n    #     sequences = []\n    #     labels = []\n    #     for i in range(len(X) - self.sequence_length):\n    #         sequences.append(X[i:i+self.sequence_length])\n    #         labels.append(y.iloc[i+self.sequence_length-1])\n    #     return np.array(sequences), np.array(labels)\n\n    # not used anymore\n    def create_sequences(self, X, y):\n        sequences = []\n        labels = []\n        y_values = y.values if hasattr(y, 'values') else y  # Convert to numpy array if pandas Series\n        for i in range(len(X) - self.sequence_length):\n            seq = X[i:i+self.sequence_length]\n            sequences.append(seq)\n            labels.append(y_values[i+self.sequence_length-1])\n        return np.array(sequences), np.array(labels)\n    \n    def setup(self, stage=None):\n        self.scaler = StandardScaler()\n        self.scaler.fit(self.X_train.reshape(-1, self.X_train.shape[-1]))\n        \n        self.X_train = self.scaler.transform(self.X_train.reshape(-1, self.X_train.shape[-1])).reshape(self.X_train.shape)\n        self.X_val = self.scaler.transform(self.X_val.reshape(-1, self.X_val.shape[-1])).reshape(self.X_val.shape)\n        self.X_test = self.scaler.transform(self.X_test.reshape(-1, self.X_test.shape[-1])).reshape(self.X_test.shape)\n        \n        # self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n        # self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n        # self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n        \n        self.train_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_train),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_train.values)\n        )\n        \n        self.val_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_val),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_val.values)\n        )\n        \n        self.test_dataset = TimeSeriesDataset(\n            data=torch.FloatTensor(self.X_test),\n            sequence_length=self.sequence_length,\n            target_idx=torch.LongTensor(self.y_test.values)\n        )\n\n        print(\"the model will be trained on: \",len(self.train_dataset),\" samples.\")\n        print(\"the model will be validated on: \",len(self.val_dataset),\" samples.\")\n        print(\"the model will be tested on: \",len(self.test_dataset),\" samples.\")\n    \n    def train_dataloader(self):\n        if self.oversample:\n            class_counts = np.bincount(self.y_train)\n            weights = 1. / class_counts[self.y_train]\n            sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n        else:\n            sampler = RandomSampler(self.train_dataset)\n            \n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            sampler=sampler,\n            num_workers=self.num_workers,\n            persistent_workers=True,\n            pin_memory=True\n        )\n    \n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n\n\ndef init_wandb():\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"mohammad_wandb_secret\")\n    wandb.login(key=wandb_api_key)\n    \n    # Initialize the run first\n    run = wandb.init(\n        project=config.wandb.project,\n        entity=config.wandb.entity,\n        tags=config.wandb.tags,\n        notes=config.wandb.notes,\n        config={\n            \"input_size\": None,  # Will be updated later\n            \"num_classes\": None,\n            \"sequence_length\": config.training.sequence_length,\n            \"train_samples\": None,\n            \"test_samples\": None,\n            \"model_config\": dict(config.model),\n            \"training_config\": dict(config.training)\n        }\n    )\n    \n    # Then create the logger\n    wandb_logger = WandbLogger(\n        experiment=run,\n        log_model='all'\n    )\n    \n    return wandb_logger, run\n\ndef main():\n    # Initialize wandb - now returns both logger and run\n    wandb_logger, run = init_wandb()\n    \n    # Initialize data module\n    data_module = NIDSDataModule(config)\n    data_module.prepare_data()\n    data_module.setup()\n    \n    # Get input size from data\n    sample_x, _ = next(iter(data_module.train_dataloader()))\n    input_size = sample_x.shape[2]\n    num_classes = len(data_module.classes)\n    \n    # Update the config with actual values\n    run.config.update({\n        \"input_size\": input_size,\n        \"num_classes\": num_classes,\n        \"train_samples\": len(data_module.train_dataset),\n        \"test_samples\": len(data_module.test_dataset)\n    })\n    \n    model = LSTMModel(input_size, num_classes, config)\n    \n    # Callbacks\n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=config.training.early_stopping_patience,\n        mode='min'\n    )\n    \n    # checkpoint_callback = ModelCheckpoint(\n    #     monitor='val_f1',  # Now monitoring F1 score\n    #     mode='max',\n    #     save_top_k=1,\n    #     dirpath='checkpoints',\n    #     filename='best_model'\n    # )\n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_acc',\n        mode='max',\n        save_top_k=1,\n        dirpath='checkpoints',\n        filename='best_model'\n    )\n \n    # Initialize trainer\n    trainer = pl.Trainer(\n        precision=16,\n        logger=wandb_logger,\n        max_epochs=config.training.max_epochs,\n        callbacks=[early_stopping, checkpoint_callback],\n        deterministic=True,\n        gradient_clip_val=1.0,\n        enable_progress_bar=True,\n        log_every_n_steps=1000\n    )\n    \n    # Train model\n    trainer.fit(model, datamodule=data_module)\n    \n    # Test model\n    test_results = trainer.test(model, datamodule=data_module)\n    \n    # Collect all predictions and targets for final evaluation\n    test_loader = data_module.test_dataloader()\n    all_preds = []\n    all_targets = []\n    \n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            x, y = batch\n            y_hat = model(x)\n            preds = torch.argmax(y_hat, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(y.cpu().numpy())\n    \n    # Final metrics calculation\n    test_acc = accuracy_score(all_targets, all_preds)\n    test_f1 = f1_score(all_targets, all_preds, average='weighted')\n    \n    # Log final test metrics\n    wandb.log({\n        'test_acc': test_acc,\n        'test_f1': test_f1,\n        'test_loss': test_results[0]['test_loss']\n    })\n    \n    # Confusion matrix and classification report\n    class_names = data_module.classes.tolist()\n    \n    # Confusion Matrix\n    wandb.log({\n        \"confusion_matrix\": wandb.plot.confusion_matrix(\n            y_true=all_targets,\n            preds=all_preds,\n            class_names=class_names,\n            title=\"Confusion Matrix\"\n        )\n    })\n    \n    # Classification Report\n    report = classification_report(\n        all_targets, all_preds, \n        target_names=class_names,\n        output_dict=True\n    )\n    \n    # Create a wandb Table for the classification report\n    report_table = wandb.Table(columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n    for class_name in class_names:\n        report_table.add_data(\n            class_name,\n            report[class_name][\"precision\"],\n            report[class_name][\"recall\"],\n            report[class_name][\"f1-score\"],\n            report[class_name][\"support\"]\n        )\n    \n    # Add weighted averages\n    report_table.add_data(\n        \"Weighted Avg\",\n        report[\"weighted avg\"][\"precision\"],\n        report[\"weighted avg\"][\"recall\"],\n        report[\"weighted avg\"][\"f1-score\"],\n        report[\"weighted avg\"][\"support\"]\n    )\n    \n    wandb.log({\"classification_report\": report_table})\n    \n    # Finish wandb run\n    wandb.finish()\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T12:30:15.754579Z","iopub.execute_input":"2025-06-27T12:30:15.754871Z","execution_failed":"2025-06-27T14:10:34.791Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohammad-fleity\u001b[0m (\u001b[33mmohammad-fleity-lebanese-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250627_123048-mlow85bg</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot/runs/mlow85bg' target=\"_blank\">dutiful-disco-68</a></strong> to <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot/runs/mlow85bg' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot/runs/mlow85bg</a>"},"metadata":{}},{"name":"stdout","text":"3745408\n802588\nthe model will be trained on:  3745408  samples.\nthe model will be validated on:  802587  samples.\nthe model will be tested on:  802588  samples.\n3745408\n802588\nthe model will be trained on:  3745408  samples.\nthe model will be validated on:  802587  samples.\nthe model will be tested on:  802588  samples.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24326ae1d26446218dec1b762fd3b4a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"993bf4daec544f4c983e61a1d4ac4a99"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}