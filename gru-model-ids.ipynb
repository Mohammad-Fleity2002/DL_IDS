{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8089266,"sourceType":"datasetVersion","datasetId":4775518},{"sourceId":8089281,"sourceType":"datasetVersion","datasetId":4775527}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler, RandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nimport wandb\nfrom omegaconf import OmegaConf\nimport os\nimport warnings\nfrom kaggle_secrets import UserSecretsClient\n# from kaggle_secrets import UserSecretsClient\n\n\nwarnings.filterwarnings('ignore')\n\n# GRU Configuration\nconfig = OmegaConf.create({\n    \"wandb\": {\n        \"project\": \"DL-NIDS-2--cic-ids-2017\",\n        \"entity\": \"mohammad-fleity-lebanese-university\",\n        \"tags\": [\"GRU\", \"CIC-IDS-2017\", \"PyTorch\"],\n        \"notes\": \"Optimized GRU for network intrusion detection\"\n    },\n    \"model\": {\n        \"hidden_size\": 128,          # Hidden state size\n        \"num_layers\": 2,             # Number of GRU layers\n        \"dropout\": 0.4,              # Dropout rate\n        \"dense_units\": [128, 64],    # Dense layer sizes\n        \"learning_rate\": 0.0001,     # Learning rate\n        \"weight_decay\": 1e-4         # L2 regularization\n    },\n    \"training\": {\n        \"sequence_length\": 5,        # Sequence length\n        \"batch_size\": 128,           # Batch size\n        \"max_epochs\": 10,            # Max training epochs\n        \"early_stopping_patience\": 7, # Early stopping patience\n        \"oversample\": True,          # Class balancing\n        \"gpus\": 1 if torch.cuda.is_available() else 0,\n        \"train_size\": 0.7,           # Train split size\n        \"val_size\": 0.15            # Validation split size\n    },\n    \"data\": {\n        \"raw\": \"cic_ids_2017.parquet\",\n        \"num_workers\": 4\n    }\n})\n\nclass GRUModel(pl.LightningModule):\n    def __init__(self, input_size, num_classes, config):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # GRU with layer normalization\n        self.gru = nn.GRU(\n            input_size=input_size,\n            hidden_size=config.model.hidden_size,\n            num_layers=config.model.num_layers,\n            batch_first=True,\n            dropout=config.model.dropout if config.model.num_layers > 1 else 0\n        )\n        \n        self.gru_ln = nn.LayerNorm(config.model.hidden_size)\n        \n        # Dense layers\n        self.dense = nn.Sequential(\n            nn.Linear(config.model.hidden_size, config.model.dense_units[0]),\n            nn.LayerNorm(config.model.dense_units[0]),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(config.model.dense_units[0], config.model.dense_units[1]),\n            nn.LayerNorm(config.model.dense_units[1]),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout)\n        )\n        \n        self.output = nn.Linear(config.model.dense_units[1], num_classes)\n        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n    def forward(self, x):\n        gru_out, _ = self.gru(x)\n        gru_out = gru_out[:, -1, :]  # Last timestep\n        gru_out = self.gru_ln(gru_out)\n        features = self.dense(gru_out)\n        return self.output(features)\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        \n        # Log metrics per step\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n        \n        return loss\n    \n    # def training_epoch_end(self, outputs):\n    #     # Log epoch-level metrics\n    #     avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n    #     self.log('train_epoch_loss', avg_loss, prog_bar=True)\n        \n    #     # Calculate epoch accuracy\n    #     correct = sum([x['correct'] for x in outputs])\n    #     total = sum([x['total'] for x in outputs])\n    #     epoch_acc = correct / total\n    #     self.log('train_acc_epoch', epoch_acc, prog_bar=True)\n    def on_train_epoch_end(self, outputs):\n        # Log epoch-level metrics\n        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n        self.log('train_epoch_loss', avg_loss, prog_bar=True)\n        \n        # Calculate epoch accuracy\n        correct = sum([x['correct'] for x in outputs])\n        total = sum([x['total'] for x in outputs])\n        epoch_acc = correct / total\n        self.log('train_acc_epoch', epoch_acc, prog_bar=True)\n        \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        \n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', acc, prog_bar=True)\n        \n        return {'val_loss': loss, 'val_acc': acc, 'correct': (logits.argmax(dim=1) == y).sum(), 'total': len(y)}\n    \n    # def validation_epoch_end(self, outputs):\n    #     avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n    #     self.log('val_epoch_loss', avg_loss, prog_bar=True)\n        \n    #     correct = sum([x['correct'] for x in outputs])\n    #     total = sum([x['total'] for x in outputs])\n    #     epoch_acc = correct / total\n    #     self.log('val_acc_epoch', epoch_acc, prog_bar=True)\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        self.log('val_epoch_loss', avg_loss, prog_bar=True)\n    \n        correct = sum([x['correct'] for x in outputs])\n        total = sum([x['total'] for x in outputs])\n        epoch_acc = correct / total\n        self.log('val_acc_epoch', epoch_acc, prog_bar=True)\n\n    def on_validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        self.log('val_epoch_loss', avg_loss, prog_bar=True)\n        \n        correct = sum([x['correct'] for x in outputs])\n        total = sum([x['total'] for x in outputs])\n        epoch_acc = correct / total\n        self.log('val_acc_epoch', epoch_acc, prog_bar=True)\n\n\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        \n        self.log('test_loss', loss)\n        self.log('test_acc', acc)\n        \n        return {'test_loss': loss, 'test_acc': acc, 'preds': logits.argmax(dim=1), 'targets': y}\n    \n    def configure_optimizers(self):\n        optimizer = optim.AdamW(self.parameters(), \n                              lr=self.hparams.config.model.learning_rate,\n                              weight_decay=self.hparams.config.model.weight_decay)\n        return optimizer\n\nclass NIDSDataModule(pl.LightningDataModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.batch_size = config.training.batch_size\n        self.sequence_length = config.training.sequence_length\n        self.num_workers = config.data.num_workers\n        self.oversample = config.training.oversample\n\n    def prepare_data(self):\n        df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n    \n        # Clean data\n        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df.dropna(inplace=True)\n        df.drop_duplicates(inplace=True)\n    \n        # Identify non-numeric columns\n        self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', \n                               'Src Port', 'Attack', 'Dst IP', 'Dst Port', 'Protocol']\n        self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n    \n        # Encode labels\n        self.label_encoder = LabelEncoder()\n        df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n        self.classes = self.label_encoder.classes_\n    \n        # Initialize scaler\n        self.scaler = StandardScaler()\n    \n        # Train/Val/Test split\n        train_df, test_df = train_test_split(\n            df,\n            test_size=1 - self.config.training.train_size,\n            random_state=42,\n            stratify=df['Label_Num']\n        )\n        val_df, test_df = train_test_split(\n            test_df,\n            test_size=0.5,\n            random_state=42,\n            stratify=test_df['Label_Num']\n        )\n        print(len(train_df))\n        print(len(test_df))\n        # max_rows = 90_000\n        # if len(train_df) > max_rows:\n        #     train_df = train_df.sample(n=max_rows, random_state=42)\n\n        # Use `fit=True` only for training data\n        self.X_train, self.y_train = self._prepare_features(train_df, fit=True)\n        self.X_val, self.y_val = self._prepare_features(val_df, fit=False)\n        self.X_test, self.y_test = self._prepare_features(test_df, fit=False)\n    \n    def _prepare_features(self, df, fit=False):\n        X = df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n        y = df['Label_Num']\n        if fit:\n            X = self.scaler.fit_transform(X)\n        else:\n            X = self.scaler.transform(X)\n        return self.create_sequences(X, y)\n\n    def create_sequences(self, X, y):\n        sequences = []\n        labels = []\n        for i in range(len(X) - self.sequence_length):\n            sequences.append(X[i:i+self.sequence_length])\n            labels.append(y.iloc[i+self.sequence_length-1])\n        return np.array(sequences), np.array(labels)\n    \n    def setup(self, stage=None):\n        self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n        self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n        self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n        \n        print(\"Training samples:\", len(self.train_dataset))\n        print(\"Validation samples:\", len(self.val_dataset))\n        print(\"Test samples:\", len(self.test_dataset))\n    \n    def train_dataloader(self):\n        if self.oversample:\n            class_counts = np.bincount(self.y_train)\n            weights = 1. / class_counts[self.y_train]\n            sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n        else:\n            sampler = RandomSampler(self.train_dataset)\n            \n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            sampler=sampler,\n            num_workers=self.num_workers,\n            persistent_workers=True,\n            pin_memory=True\n        )\n    \n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n\ndef init_wandb():\n    user_secrets = UserSecretsClient()\n    # secret_value_0 = user_secrets.get_secret(\"mohammad_wandb_secret\")\n    # user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"mohammad_wandb_secret\")\n    wandb.login(key=wandb_api_key)\n    \n    run = wandb.init(\n        project=config.wandb.project,\n        entity=config.wandb.entity,\n        tags=config.wandb.tags,\n        notes=config.wandb.notes,\n        config={\n            \"input_size\": None,\n            \"num_classes\": None,\n            \"sequence_length\": config.training.sequence_length,\n            \"train_samples\": None,\n            \"test_samples\": None,\n            \"model_config\": dict(config.model),\n            \"training_config\": dict(config.training)\n        }\n    )\n    \n    wandb_logger = WandbLogger(\n        experiment=run,\n        log_model='all'\n    )\n    \n    return wandb_logger, run\n\ndef main():\n    wandb_logger, run = init_wandb()\n    \n    data_module = NIDSDataModule(config)\n    data_module.prepare_data()\n    data_module.setup()\n    \n    sample_x, _ = next(iter(data_module.train_dataloader()))\n    input_size = sample_x.shape[2]\n    num_classes = len(data_module.classes)\n    \n    run.config.update({\n        \"input_size\": input_size,\n        \"num_classes\": num_classes,\n        \"train_samples\": len(data_module.train_dataset),\n        \"test_samples\": len(data_module.test_dataset)\n    })\n    \n    model = GRUModel(input_size, num_classes, config)\n    \n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=config.training.early_stopping_patience,\n        mode='min'\n    )\n    \n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_acc',\n        mode='max',\n        save_top_k=1,\n        dirpath='checkpoints',\n        filename='best_model'\n    )\n \n    trainer = pl.Trainer(\n        precision=16,\n        logger=wandb_logger,\n        max_epochs=config.training.max_epochs,\n        callbacks=[early_stopping, checkpoint_callback],\n        deterministic=True,\n        gradient_clip_val=1.0,\n        enable_progress_bar=True,\n        log_every_n_steps=1000\n    )\n    \n    trainer.fit(model, datamodule=data_module)\n    \n    test_results = trainer.test(model, datamodule=data_module)\n    \n    # Collect all predictions and targets\n    test_loader = data_module.test_dataloader()\n    all_preds = []\n    all_targets = []\n    \n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            x, y = batch\n            y_hat = model(x)\n            preds = torch.argmax(y_hat, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(y.cpu().numpy())\n    \n    # Calculate metrics\n    test_acc = accuracy_score(all_targets, all_preds)\n    test_f1 = f1_score(all_targets, all_preds, average='weighted')\n    \n    # Log final metrics\n    wandb.log({\n        'test_acc': test_acc,\n        'test_f1': test_f1,\n        'test_loss': test_results[0]['test_loss']\n    })\n    \n    # Enhanced multiclass confusion matrix\n    class_names = data_module.classes.tolist()\n    conf_mat = confusion_matrix(all_targets, all_preds)\n    \n    # Create a custom confusion matrix plot\n    data = []\n    for i in range(len(class_names)):\n        for j in range(len(class_names)):\n            data.append([class_names[i], class_names[j], conf_mat[i, j]])\n    \n    fields = {\n        \"Actual\": \"Actual\",\n        \"Predicted\": \"Predicted\",\n        \"n\": \"Count\"\n    }\n    \n    wandb.log({\n        \"multiclass_confusion_matrix\": wandb.plot_table(\n            \"wandb/confusion_matrix/v1\",\n            wandb.Table(columns=[\"Actual\", \"Predicted\", \"Count\"], data=data),\n            fields,\n            {\"title\": \"Multiclass Confusion Matrix\"}\n        )\n    })\n    \n    # Classification Report\n    report = classification_report(\n        all_targets, all_preds, \n        target_names=class_names,\n        output_dict=True\n    )\n    \n    report_table = wandb.Table(columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n    for class_name in class_names:\n        report_table.add_data(\n            class_name,\n            report[class_name][\"precision\"],\n            report[class_name][\"recall\"],\n            report[class_name][\"f1-score\"],\n            report[class_name][\"support\"]\n        )\n    \n    report_table.add_data(\n        \"Weighted Avg\",\n        report[\"weighted avg\"][\"precision\"],\n        report[\"weighted avg\"][\"recall\"],\n        report[\"weighted avg\"][\"f1-score\"],\n        report[\"weighted avg\"][\"support\"]\n    )\n    \n    wandb.log({\"classification_report\": report_table})\n    \n    wandb.finish()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-19T08:32:11.087738Z","iopub.execute_input":"2025-06-19T08:32:11.088465Z","execution_failed":"2025-06-19T08:32:27.031Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">driven-cloud-32</strong> at: <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/ga5richm' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/ga5richm</a><br> View project at: <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250619_081928-ga5richm/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250619_083211-o3u26ecr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/o3u26ecr' target=\"_blank\">sweet-durian-33</a></strong> to <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/o3u26ecr' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/o3u26ecr</a>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler, RandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nimport wandb\nfrom omegaconf import OmegaConf\nimport os\nimport warnings\nfrom kaggle_secrets import UserSecretsClient\n\nwarnings.filterwarnings('ignore')\n\n# GRU Configuration\nconfig = OmegaConf.create({\n    \"wandb\": {\n        \"project\": \"https://wandb.ai/mohamDL-NIDS-2--cic-ids-2017\",\n        \"entity\": \"mohammad-fleity-lebanese-university\",\n        \"tags\": [\"GRU\", \"CIC-IDS-2017\", \"PyTorch\"],\n        \"notes\": \"Optimized GRU for network intrusion detection with limited samples\"\n    },\n    \"model\": {\n        \"hidden_size\": 128,\n        \"num_layers\": 2,\n        \"dropout\": 0.4,\n        \"dense_units\": [128, 64],\n        \"learning_rate\": 0.0001,\n        \"weight_decay\": 1e-4\n    },\n    \"training\": {\n        \"sequence_length\": 5,\n        \"batch_size\": 128,\n        \"max_epochs\": 10,            # Hard limit of 10 epochs\n        \"early_stopping_patience\": 7,\n        \"oversample\": True,\n        \"gpus\": 1 if torch.cuda.is_available() else 0,\n        \"max_train_samples\": 1000000,  # Maximum training samples\n        \"max_val_samples\": 200000,     # Maximum validation samples\n        \"max_test_samples\": 200000     # Maximum test samples\n    },\n    \"data\": {\n        \"raw\": \"cic_ids_2017.parquet\",\n        \"num_workers\": 4\n    }\n})\n\n# class GRUModel(pl.LightningModule):\n#     def __init__(self, input_size, num_classes, config):\n#         super().__init__()\n#         self.save_hyperparameters()\n#         self.outputs=[]\n#         self.gru = nn.GRU(\n#             input_size=input_size,\n#             hidden_size=config.model.hidden_size,\n#             num_layers=config.model.num_layers,\n#             batch_first=True,\n#             dropout=config.model.dropout if config.model.num_layers > 1 else 0\n#         )\n        \n#         self.gru_ln = nn.LayerNorm(config.model.hidden_size)\n        \n#         self.dense = nn.Sequential(\n#             nn.Linear(config.model.hidden_size, config.model.dense_units[0]),\n#             nn.LayerNorm(config.model.dense_units[0]),\n#             nn.ReLU(),\n#             nn.Dropout(config.model.dropout),\n#             nn.Linear(config.model.dense_units[0], config.model.dense_units[1]),\n#             nn.LayerNorm(config.model.dense_units[1]),\n#             nn.ReLU(),\n#             nn.Dropout(config.model.dropout)\n#         )\n        \n#         self.output = nn.Linear(config.model.dense_units[1], num_classes)\n#         self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n#     def forward(self, x):\n#         gru_out, _ = self.gru(x)\n#         gru_out = gru_out[:, -1, :]\n#         gru_out = self.gru_ln(gru_out)\n#         features = self.dense(gru_out)\n#         return self.output(features)\n    \n#     def training_step(self, batch, batch_idx):\n#         x, y = batch\n#         logits = self(x)\n#         loss = self.criterion(logits, y)\n#         acc = (logits.argmax(dim=1) == y).float().mean()\n        \n#         self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n#         self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n        \n#         return {'loss': loss, 'correct': (logits.argmax(dim=1) == y).sum(), 'total': len(y)}\n    \n#     # def training_epoch_end(self, outputs):\n#     #     avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n#     #     self.log('train_epoch_loss', avg_loss, prog_bar=True)\n        \n#     #     correct = sum([x['correct'] for x in outputs])\n#     #     total = sum([x['total'] for x in outputs])\n#     #     epoch_acc = correct / total\n#     #     self.log('train_acc_epoch', epoch_acc, prog_bar=True)\n#     def on_train_epoch_end(self):\n#         # Access saved outputs from training_step\n#         if not hasattr(self, 'train_step_outputs'):\n#             return\n            \n#         avg_loss = torch.stack([x['loss'] for x in self.train_step_outputs]).mean()\n#         correct = sum([x['correct'] for x in self.train_step_outputs])\n#         total = sum([x['total'] for x in self.train_step_outputs])\n#         epoch_acc = correct / total\n        \n#         self.log('train_epoch_loss', avg_loss, prog_bar=True)\n#         self.log('train_acc_epoch', epoch_acc, prog_bar=True)\n#         self.train_step_outputs.clear()  # free memory\n\n#     def training_step(self, batch, batch_idx):\n#         x, y = batch\n#         logits = self(x)\n#         loss = self.criterion(logits, y)\n#         acc = (logits.argmax(dim=1) == y).float().mean()\n        \n#         self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n#         self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n        \n#         output = {'loss': loss, 'correct': (logits.argmax(dim=1) == y).sum(), 'total': len(y)}\n        \n#         if not hasattr(self, 'train_step_outputs'):\n#             self.train_step_outputs = []\n#         self.train_step_outputs.append(output)\n        \n#         return output\n\n#     def validation_step(self, batch, batch_idx):\n#         x, y = batch\n#         logits = self(x)\n#         loss = self.criterion(logits, y)\n#         acc = (logits.argmax(dim=1) == y).float().mean()\n        \n#         self.log('val_loss', loss, prog_bar=True)\n#         self.log('val_acc', acc, prog_bar=True)\n        \n#         return {'val_loss': loss, 'correct': (logits.argmax(dim=1) == y).sum(), 'total': len(y)}\n    \n#     def validation_epoch_end(self, outputs):\n#         avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n#         self.log('val_epoch_loss', avg_loss, prog_bar=True)\n        \n#         correct = sum([x['correct'] for x in outputs])\n#         total = sum([x['total'] for x in outputs])\n#         epoch_acc = correct / total\n#         self.log('val_acc_epoch', epoch_acc, prog_bar=True)\n    \n#     def test_step(self, batch, batch_idx):\n#         x, y = batch\n#         logits = self(x)\n#         loss = self.criterion(logits, y)\n#         acc = (logits.argmax(dim=1) == y).float().mean()\n        \n#         self.log('test_loss', loss)\n#         self.log('test_acc', acc)\n        \n#         return {'test_loss': loss, 'preds': logits.argmax(dim=1), 'targets': y}\n    \n#     def configure_optimizers(self):\n#         optimizer = optim.AdamW(self.parameters(), \n#                               lr=self.hparams.config.model.learning_rate,\n#                               weight_decay=self.hparams.config.model.weight_decay)\n#         return optimizer\n\nclass GRUModel(pl.LightningModule):\n    def __init__(self, input_size, num_classes, config):\n        super().__init__()\n        self.save_hyperparameters()\n\n        # Temporary storage for step outputs\n        self.train_outputs = []\n        self.val_outputs = []\n\n        self.gru = nn.GRU(\n            input_size=input_size,\n            hidden_size=config.model.hidden_size,\n            num_layers=config.model.num_layers,\n            batch_first=True,\n            dropout=config.model.dropout if config.model.num_layers > 1 else 0\n        )\n        \n        self.gru_ln = nn.LayerNorm(config.model.hidden_size)\n\n        self.dense = nn.Sequential(\n            nn.Linear(config.model.hidden_size, config.model.dense_units[0]),\n            nn.LayerNorm(config.model.dense_units[0]),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(config.model.dense_units[0], config.model.dense_units[1]),\n            nn.LayerNorm(config.model.dense_units[1]),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout)\n        )\n        \n        self.output = nn.Linear(config.model.dense_units[1], num_classes)\n        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n    def forward(self, x):\n        gru_out, _ = self.gru(x)\n        gru_out = gru_out[:, -1, :]\n        gru_out = self.gru_ln(gru_out)\n        features = self.dense(gru_out)\n        return self.output(features)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n\n        # Save for epoch-end\n        self.train_outputs.append({\n            'loss': loss.detach(),\n            'correct': (logits.argmax(dim=1) == y).sum().detach(),\n            'total': len(y)\n        })\n\n        return loss\n\n    def on_train_epoch_end(self):\n        if not self.train_outputs:\n            return\n\n        avg_loss = torch.stack([x['loss'] for x in self.train_outputs]).mean()\n        correct = sum([x['correct'] for x in self.train_outputs])\n        total = sum([x['total'] for x in self.train_outputs])\n        epoch_acc = correct / total\n\n        self.log('train_epoch_loss', avg_loss, prog_bar=True)\n        self.log('train_acc_epoch', epoch_acc*100, prog_bar=True)\n        self.train_outputs.clear()\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', acc, prog_bar=True)\n\n        # Save for epoch-end\n        self.val_outputs.append({\n            'val_loss': loss.detach(),\n            'correct': (logits.argmax(dim=1) == y).sum().detach(),\n            'total': len(y)\n        })\n\n        return loss\n\n    def on_validation_epoch_end(self):\n        if not self.val_outputs:\n            return\n\n        avg_loss = torch.stack([x['val_loss'] for x in self.val_outputs]).mean()\n        correct = sum([x['correct'] for x in self.val_outputs])\n        total = sum([x['total'] for x in self.val_outputs])\n        epoch_acc = (correct / total)*100\n\n        self.log('val_loss', avg_loss, prog_bar=True)\n        self.log('val_acc', epoch_acc, prog_bar=True)\n        self.val_outputs.clear()\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n\n        self.log('test_loss', loss)\n        self.log('test_acc', acc*100)\n\n        return {'test_loss': loss, 'preds': logits.argmax(dim=1), 'targets': y}\n\n    def configure_optimizers(self):\n        return optim.AdamW(\n            self.parameters(),\n            lr=self.hparams.config.model.learning_rate,\n            weight_decay=self.hparams.config.model.weight_decay\n        )\n\nclass NIDSDataModule(pl.LightningDataModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.batch_size = config.training.batch_size\n        self.sequence_length = config.training.sequence_length\n        self.num_workers = config.data.num_workers\n        self.oversample = config.training.oversample\n        self.max_train_samples = config.training.max_train_samples\n        self.max_val_samples = config.training.max_val_samples\n        self.max_test_samples = config.training.max_test_samples\n\n    def prepare_data(self):\n        df = pd.read_parquet(os.path.join('/kaggle/input/cic-ids-2017-parquet', self.config.data.raw))\n    \n        # Clean data\n        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df.dropna(inplace=True)\n        df.drop_duplicates(inplace=True)\n    \n        # Identify non-numeric columns\n        self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', \n                               'Src Port', 'Attack', 'Dst IP', 'Dst Port', 'Protocol']\n        self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n    \n        # Encode labels\n        self.label_encoder = LabelEncoder()\n        df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n        self.classes = self.label_encoder.classes_\n    \n        # Initialize scaler\n        self.scaler = StandardScaler()\n    \n        # Stratified split with sample limits\n        train_df, test_df = train_test_split(\n            df,\n            test_size=0.3,  # 70% train, 30% test+val\n            random_state=42,\n            stratify=df['Label_Num']\n        )\n        \n        # Further split test into val and test\n        val_df, test_df = train_test_split(\n            test_df,\n            test_size=0.5,  # 15% val, 15% test\n            random_state=42,\n            stratify=test_df['Label_Num']\n        )\n    \n        # Process each split with sample limits\n        self.X_train, self.y_train = self._prepare_features(train_df, fit=True)\n        self.X_val, self.y_val = self._prepare_features(val_df, fit=False)\n        self.X_test, self.y_test = self._prepare_features(test_df, fit=False)\n        \n        # Apply sample limits\n        self._limit_samples()\n\n    def _limit_samples(self):\n        \"\"\"Limit samples according to configuration\"\"\"\n        # Training data\n        if len(self.X_train) > self.max_train_samples:\n            indices = np.random.choice(len(self.X_train), self.max_train_samples, replace=False)\n            self.X_train = self.X_train[indices]\n            self.y_train = self.y_train[indices]\n        \n        # Validation data\n        if len(self.X_val) > self.max_val_samples:\n            indices = np.random.choice(len(self.X_val), self.max_val_samples, replace=False)\n            self.X_val = self.X_val[indices]\n            self.y_val = self.y_val[indices]\n        \n        # Test data\n        if len(self.X_test) > self.max_test_samples:\n            indices = np.random.choice(len(self.X_test), self.max_test_samples, replace=False)\n            self.X_test = self.X_test[indices]\n            self.y_test = self.y_test[indices]\n    \n    def _prepare_features(self, df, fit=False):\n        X = df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n        y = df['Label_Num']\n        if fit:\n            X = self.scaler.fit_transform(X)\n        else:\n            X = self.scaler.transform(X)\n        return self.create_sequences(X, y)\n\n    def create_sequences(self, X, y):\n        sequences = []\n        labels = []\n        for i in range(len(X) - self.sequence_length):\n            sequences.append(X[i:i+self.sequence_length])\n            labels.append(y.iloc[i+self.sequence_length-1])\n        return np.array(sequences), np.array(labels)\n    \n    def setup(self, stage=None):\n        self.train_dataset = TensorDataset(torch.FloatTensor(self.X_train), torch.LongTensor(self.y_train))\n        self.val_dataset = TensorDataset(torch.FloatTensor(self.X_val), torch.LongTensor(self.y_val))\n        self.test_dataset = TensorDataset(torch.FloatTensor(self.X_test), torch.LongTensor(self.y_test))\n        \n        print(f\"Training samples: {len(self.train_dataset)} (limited to {self.max_train_samples})\")\n        print(f\"Validation samples: {len(self.val_dataset)} (limited to {self.max_val_samples})\")\n        print(f\"Test samples: {len(self.test_dataset)} (limited to {self.max_test_samples})\")\n    \n    def train_dataloader(self):\n        if self.oversample:\n            class_counts = np.bincount(self.y_train)\n            weights = 1. / class_counts[self.y_train]\n            sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n        else:\n            sampler = RandomSampler(self.train_dataset)\n            \n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            sampler=sampler,\n            num_workers=self.num_workers,\n            persistent_workers=True,\n            pin_memory=True\n        )\n    \n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True\n        )\n\ndef init_wandb():\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"mohammad_wandb_secret\")\n    wandb.login(key=wandb_api_key)\n    \n    run = wandb.init(\n        project=config.wandb.project,\n        entity=config.wandb.entity,\n        tags=config.wandb.tags,\n        notes=config.wandb.notes,\n        config={\n            \"input_size\": None,\n            \"num_classes\": None,\n            \"sequence_length\": config.training.sequence_length,\n            \"train_samples\": config.training.max_train_samples,\n            \"val_samples\": config.training.max_val_samples,\n            \"test_samples\": config.training.max_test_samples,\n            \"model_config\": dict(config.model),\n            \"training_config\": dict(config.training)\n        }\n    )\n    \n    wandb_logger = WandbLogger(\n        experiment=run,\n        log_model='all'\n    )\n    \n    return wandb_logger, run\n\ndef main():\n    wandb_logger, run = init_wandb()\n    \n    data_module = NIDSDataModule(config)\n    data_module.prepare_data()\n    data_module.setup()\n    \n    sample_x, _ = next(iter(data_module.train_dataloader()))\n    input_size = sample_x.shape[2]\n    num_classes = len(data_module.classes)\n    \n    run.config.update({\n        \"input_size\": input_size,\n        \"num_classes\": num_classes\n    })\n    \n    model = GRUModel(input_size, num_classes, config)\n    \n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=config.training.early_stopping_patience,\n        mode='min'\n    )\n    \n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_acc',\n        mode='max',\n        save_top_k=1,\n        dirpath='checkpoints',\n        filename='best_model'\n    )\n \n    trainer = pl.Trainer(\n        precision=16,\n        logger=wandb_logger,\n        max_epochs=config.training.max_epochs,\n        callbacks=[early_stopping, checkpoint_callback],\n        deterministic=True,\n        gradient_clip_val=1.0,\n        enable_progress_bar=True,\n        log_every_n_steps=1000\n    )\n    \n    trainer.fit(model, datamodule=data_module)\n    \n    test_results = trainer.test(model, datamodule=data_module)\n    \n    # Collect all predictions and targets\n    test_loader = data_module.test_dataloader()\n    all_preds = []\n    all_targets = []\n    \n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            x, y = batch\n            y_hat = model(x)\n            preds = torch.argmax(y_hat, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_targets.extend(y.cpu().numpy())\n    \n    # Calculate metrics\n    test_acc = accuracy_score(all_targets, all_preds)\n    test_f1 = f1_score(all_targets, all_preds, average='weighted')\n    \n    # Log final metrics\n    wandb.log({\n        'test_acc': test_acc,\n        'test_f1': test_f1,\n        'test_loss': test_results[0]['test_loss']\n    })\n    \n    # Enhanced multiclass confusion matrix\n    class_names = data_module.classes.tolist()\n    conf_mat = confusion_matrix(all_targets, all_preds)\n    \n    # Create a custom confusion matrix plot\n    data = []\n    for i in range(len(class_names)):\n        for j in range(len(class_names)):\n            data.append([class_names[i], class_names[j], conf_mat[i, j]])\n    \n    fields = {\n        \"Actual\": \"Actual\",\n        \"Predicted\": \"Predicted\",\n        \"n\": \"Count\"\n    }\n    \n    wandb.log({\n        \"multiclass_confusion_matrix\": wandb.plot_table(\n            \"wandb/confusion_matrix/v1\",\n            wandb.Table(columns=[\"Actual\", \"Predicted\", \"Count\"], data=data),\n            fields,\n            {\"title\": \"Multiclass Confusion Matrix\"}\n        )\n    })\n    \n    # Classification Report\n    report = classification_report(\n        all_targets, all_preds, \n        target_names=class_names,\n        output_dict=True\n    )\n    \n    report_table = wandb.Table(columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n    for class_name in class_names:\n        report_table.add_data(\n            class_name,\n            report[class_name][\"precision\"],\n            report[class_name][\"recall\"],\n            report[class_name][\"f1-score\"],\n            report[class_name][\"support\"]\n        )\n    \n    report_table.add_data(\n        \"Weighted Avg\",\n        report[\"weighted avg\"][\"precision\"],\n        report[\"weighted avg\"][\"recall\"],\n        report[\"weighted avg\"][\"f1-score\"],\n        report[\"weighted avg\"][\"support\"]\n    )\n    \n    wandb.log({\"classification_report\": report_table})\n    \n    wandb.finish()\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-20T07:18:53.418872Z","iopub.execute_input":"2025-06-20T07:18:53.419155Z","execution_failed":"2025-06-20T07:20:00.191Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohammad-fleity\u001b[0m (\u001b[33mmohammad-fleity-lebanese-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250620_071919-9p7bvw4a</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/9p7bvw4a' target=\"_blank\">golden-waterfall-43</a></strong> to <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/9p7bvw4a' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ids-2017/runs/9p7bvw4a</a>"},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"<h2>SECOND DATASET</h2>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler, RandomSampler\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nimport wandb\nfrom omegaconf import OmegaConf\nimport os\nimport warnings\nfrom kaggle_secrets import UserSecretsClient\n\nwarnings.filterwarnings('ignore')\n\n# GRU Configuration (same architecture)\nconfig = OmegaConf.create({\n    \"wandb\": {\n        \"project\": \"DL-NIDS-2--cic-ton-iot\",\n        \"entity\": \"mohammad-fleity-lebanese-university\",\n        \"tags\": [\"GRU\", \"CIC-TON-IOT\", \"PyTorch\"],\n        \"notes\": \"GRU for network intrusion detection with memory optimizations\"\n    },\n    \"model\": {\n        \"hidden_size\": 128,\n        \"num_layers\": 2,\n        \"dropout\": 0.4,\n        \"dense_units\": [128, 64],\n        \"learning_rate\": 0.0001,\n        \"weight_decay\": 1e-4\n    },\n    \"training\": {\n        \"sequence_length\": 3,       # Reduced from 5\n        \"batch_size\": 64,           # Reduced from 128\n        \"max_epochs\": 40,\n        \"early_stopping_patience\": 7,\n        \"oversample\": True,\n        \"gpus\": 1 if torch.cuda.is_available() else 0,\n        \"max_train_samples\": 100000,  # Reduced from 100000\n        \"max_val_samples\": 20000,     # Reduced from 20000\n        \"max_test_samples\": 10000      # Reduced from 10000\n    },\n    \"data\": {\n        \"raw\": \"cic_ton_iot.parquet\",\n        \"num_workers\": 2            # Reduced from 4\n    }\n})\n\nclass GRUModel(pl.LightningModule):\n    def __init__(self, input_size, num_classes, config):\n        super().__init__()\n        self.save_hyperparameters()\n        \n        # Same architecture as original\n        self.gru = nn.GRU(\n            input_size=input_size,\n            hidden_size=config.model.hidden_size,\n            num_layers=config.model.num_layers,\n            batch_first=True,\n            dropout=config.model.dropout if config.model.num_layers > 1 else 0\n        )\n        \n        self.gru_ln = nn.LayerNorm(config.model.hidden_size)\n        \n        self.dense = nn.Sequential(\n            nn.Linear(config.model.hidden_size, config.model.dense_units[0]),\n            nn.LayerNorm(config.model.dense_units[0]),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout),\n            nn.Linear(config.model.dense_units[0], config.model.dense_units[1]),\n            nn.LayerNorm(config.model.dense_units[1]),\n            nn.ReLU(),\n            nn.Dropout(config.model.dropout)\n        )\n        \n        self.output = nn.Linear(config.model.dense_units[1], num_classes)\n        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n    def forward(self, x):\n        gru_out, _ = self.gru(x)\n        gru_out = gru_out[:, -1, :]\n        gru_out = self.gru_ln(gru_out)\n        features = self.dense(gru_out)\n        return self.output(features)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n\n        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n        self.log('train_acc', acc*100, on_step=True, on_epoch=True, prog_bar=True)\n\n        return {'loss': loss, 'correct': (logits.argmax(dim=1) == y).sum(), 'total': len(y)}\n\n    def on_train_epoch_end(self):\n        # This will be handled by Lightning automatically\n        pass\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n\n        self.log('val_loss', loss, prog_bar=True)\n        self.log('val_acc', acc*100, prog_bar=True)\n\n        return {'val_loss': loss, 'correct': (logits.argmax(dim=1) == y).sum(), 'total': len(y)}\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = self.criterion(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n\n        self.log('test_loss', loss)\n        self.log('test_acc', acc*100)\n\n        return {'test_loss': loss, 'preds': logits.argmax(dim=1), 'targets': y}\n\n    def configure_optimizers(self):\n        return optim.AdamW(\n            self.parameters(),\n            lr=self.hparams.config.model.learning_rate,\n            weight_decay=self.hparams.config.model.weight_decay\n        )\n\nclass NIDSDataModule(pl.LightningDataModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.batch_size = config.training.batch_size\n        self.sequence_length = config.training.sequence_length\n        self.num_workers = config.data.num_workers\n        self.oversample = config.training.oversample\n        self.max_train_samples = config.training.max_train_samples\n        self.max_val_samples = config.training.max_val_samples\n        self.max_test_samples = config.training.max_test_samples\n\n    def prepare_data(self):\n        # Load data\n        df = pd.read_parquet(os.path.join('/kaggle/input/cic-ton-iot-parquet', self.config.data.raw))\n    \n        # Clean data\n        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df.dropna(inplace=True)\n        df.drop_duplicates(inplace=True)\n        \n        # Downcast to save memory\n        for col in df.select_dtypes(include=['float64']).columns:\n            df[col] = pd.to_numeric(df[col], downcast='float')\n        for col in df.select_dtypes(include=['int64']).columns:\n            df[col] = pd.to_numeric(df[col], downcast='integer')\n    \n        # Identify non-numeric columns\n        self.non_numeric_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', \n                               'Src Port', 'Attack', 'Dst IP', 'Dst Port', 'Protocol']\n        self.non_numeric_cols = [col for col in self.non_numeric_cols if col in df.columns]\n    \n        # Encode labels\n        self.label_encoder = LabelEncoder()\n        df['Label_Num'] = self.label_encoder.fit_transform(df['Label'])\n        self.classes = self.label_encoder.classes_\n    \n        # Initialize scaler\n        self.scaler = StandardScaler()\n    \n        # Stratified split with sample limits\n        train_df, test_df = train_test_split(\n            df,\n            test_size=0.3,\n            random_state=42,\n            stratify=df['Label_Num']\n        )\n        \n        # Further split test into val and test\n        val_df, test_df = train_test_split(\n            test_df,\n            test_size=0.4,\n            random_state=42,\n            stratify=test_df['Label_Num']\n        )\n    \n        # Process each split with sample limits\n        self.X_train, self.y_train = self._prepare_features(train_df, fit=True)\n        self.X_val, self.y_val = self._prepare_features(val_df, fit=False)\n        self.X_test, self.y_test = self._prepare_features(test_df, fit=False)\n        \n        # Apply sample limits\n        self._limit_samples()\n        \n        # Clean up\n        del df, train_df, test_df, val_df\n\n    def _limit_samples(self):\n        \"\"\"Limit samples according to configuration\"\"\"\n        # Training data\n        if len(self.X_train) > self.max_train_samples:\n            indices = np.random.choice(len(self.X_train), self.max_train_samples, replace=False)\n            self.X_train = self.X_train[indices]\n            self.y_train = self.y_train[indices]\n        \n        # Validation data\n        if len(self.X_val) > self.max_val_samples:\n            indices = np.random.choice(len(self.X_val), self.max_val_samples, replace=False)\n            self.X_val = self.X_val[indices]\n            self.y_val = self.y_val[indices]\n        \n        # Test data\n        if len(self.X_test) > self.max_test_samples:\n            indices = np.random.choice(len(self.X_test), self.max_test_samples, replace=False)\n            self.X_test = self.X_test[indices]\n            self.y_test = self.y_test[indices]\n            \n        print(f\"Sample counts - Train: {len(self.X_train)}, Val: {len(self.X_val)}, Test: {len(self.X_test)}\")\n    \n    def _prepare_features(self, df, fit=False):\n        X = df.drop(['Label_Num'] + self.non_numeric_cols, axis=1)\n        y = df['Label_Num']\n        if fit:\n            X = self.scaler.fit_transform(X)\n        else:\n            X = self.scaler.transform(X)\n        return self.create_sequences(X, y)\n\n    def create_sequences(self, X, y):\n        sequences = []\n        labels = []\n        for i in range(len(X) - self.sequence_length):\n            sequences.append(X[i:i+self.sequence_length])\n            labels.append(y.iloc[i+self.sequence_length-1])\n        return np.array(sequences, dtype=np.float32), np.array(labels)  # Use float32\n    \n    def setup(self, stage=None):\n        self.train_dataset = TensorDataset(\n            torch.FloatTensor(self.X_train), \n            torch.LongTensor(self.y_train)\n        )\n        self.val_dataset = TensorDataset(\n            torch.FloatTensor(self.X_val), \n            torch.LongTensor(self.y_val)\n        )\n        self.test_dataset = TensorDataset(\n            torch.FloatTensor(self.X_test), \n            torch.LongTensor(self.y_test)\n        )\n        \n        # Clear memory\n        del self.X_train, self.y_train, self.X_val, self.y_val, self.X_test, self.y_test\n    \n    def train_dataloader(self):\n        if self.oversample:\n            # Get class counts from the dataset directly\n            _, y = self.train_dataset[:]\n            class_counts = torch.bincount(y)\n            weights = 1. / class_counts[y]\n            sampler = WeightedRandomSampler(weights, len(weights), replacement=True)\n        else:\n            sampler = RandomSampler(self.train_dataset)\n            \n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            sampler=sampler,\n            num_workers=self.num_workers,\n            persistent_workers=False,  # Disabled to save memory\n            pin_memory=True,\n            drop_last=True\n        )\n    \n    def val_dataloader(self):\n        return DataLoader(\n            self.val_dataset,\n            batch_size=self.batch_size,\n            shuffle=False,\n            num_workers=self.num_workers,\n            pin_memory=True,\n            drop_last=True\n        )\n    \n    def test_dataloader(self):\n        return DataLoader(\n            self.test_dataset,\n            batch_size=32,  # Reduced test batch size\n            shuffle=False,\n            num_workers=0,   # No multiprocessing for test\n            pin_memory=False,\n            drop_last=False\n        )\n\ndef init_wandb():\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"mohammad_wandb_secret\")\n    wandb.login(key=wandb_api_key)\n    \n    run = wandb.init(\n        project=config.wandb.project,\n        entity=config.wandb.entity,\n        tags=config.wandb.tags,\n        notes=config.wandb.notes,\n        config={\n            \"input_size\": None,\n            \"num_classes\": None,\n            \"sequence_length\": config.training.sequence_length,\n            \"train_samples\": config.training.max_train_samples,\n            \"val_samples\": config.training.max_val_samples,\n            \"test_samples\": config.training.max_test_samples,\n            \"model_config\": dict(config.model),\n            \"training_config\": dict(config.training)\n        }\n    )\n    \n    wandb_logger = WandbLogger(\n        experiment=run,\n        log_model=False  # Disabled to save space\n    )\n    \n    return wandb_logger, run\n\ndef main():\n    wandb_logger, run = init_wandb()\n    \n    data_module = NIDSDataModule(config)\n    data_module.prepare_data()\n    data_module.setup()\n    \n    sample_x, _ = next(iter(data_module.train_dataloader()))\n    input_size = sample_x.shape[2]\n    num_classes = len(data_module.classes)\n    \n    run.config.update({\n        \"input_size\": input_size,\n        \"num_classes\": num_classes\n    })\n    \n    model = GRUModel(input_size, num_classes, config)\n    \n    early_stopping = EarlyStopping(\n        monitor='val_loss',\n        patience=config.training.early_stopping_patience,\n        mode='min'\n    )\n    \n    checkpoint_callback = ModelCheckpoint(\n        monitor='val_acc',\n        mode='max',\n        save_top_k=1,\n        dirpath='checkpoints',\n        filename='best_model'\n    )\n \n    trainer = pl.Trainer(\n        precision=16,\n        logger=wandb_logger,\n        max_epochs=config.training.max_epochs,\n        callbacks=[early_stopping, checkpoint_callback],\n        deterministic=True,\n        gradient_clip_val=1.0,\n        enable_progress_bar=True,\n        log_every_n_steps=100,\n        accumulate_grad_batches=2  # Gradient accumulation\n    )\n    \n    trainer.fit(model, datamodule=data_module)\n    \n    # Test with memory optimizations\n    test_results = trainer.test(model, datamodule=data_module, verbose=False)\n    \n    # Collect predictions in batches\n    test_loader = data_module.test_dataloader()\n    all_preds = []\n    all_targets = []\n    \n    model.eval()\n    with torch.no_grad():\n        for batch in test_loader:\n            x, y = batch\n            y_hat = model(x)\n            preds = torch.argmax(y_hat, dim=1)\n            all_preds.append(preds.cpu())\n            all_targets.append(y.cpu())\n    \n    all_preds = torch.cat(all_preds).numpy()\n    all_targets = torch.cat(all_targets).numpy()\n    \n    # Calculate metrics\n    test_acc = accuracy_score(all_targets, all_preds)\n    test_f1 = f1_score(all_targets, all_preds, average='weighted')\n    \n    # Log final metrics\n    wandb.log({\n        'test_acc': test_acc,\n        'test_f1': test_f1,\n        'test_loss': test_results[0]['test_loss']\n    })\n    \n    # Create confusion matrix data\n    class_names = data_module.classes.tolist()\n    conf_mat = confusion_matrix(all_targets, all_preds)\n    \n    # Log as table to save memory\n    conf_mat_table = wandb.Table(columns=[\"Actual\", \"Predicted\", \"Count\"])\n    for i in range(len(class_names)):\n        for j in range(len(class_names)):\n            conf_mat_table.add_data(class_names[i], class_names[j], conf_mat[i, j])\n    \n    wandb.log({\"confusion_matrix\": conf_mat_table})\n    \n    # Classification Report\n    report = classification_report(\n        all_targets, all_preds, \n        target_names=class_names,\n        output_dict=True\n    )\n    \n    report_table = wandb.Table(columns=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\"])\n    for class_name in class_names:\n        report_table.add_data(\n            class_name,\n            report[class_name][\"precision\"],\n            report[class_name][\"recall\"],\n            report[class_name][\"f1-score\"],\n            report[class_name][\"support\"]\n        )\n    \n    wandb.log({\"classification_report\": report_table})\n    wandb.finish()\n\nif __name__ == \"__main__\":\n    # Set random seeds for reproducibility\n    torch.manual_seed(42)\n    np.random.seed(42)\n    \n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T09:45:41.470167Z","iopub.execute_input":"2025-06-21T09:45:41.470810Z","execution_failed":"2025-06-21T10:15:57.524Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohammad-fleity\u001b[0m (\u001b[33mmohammad-fleity-lebanese-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250621_094604-ngxnfpep</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot/runs/ngxnfpep' target=\"_blank\">royal-pine-61</a></strong> to <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot/runs/ngxnfpep' target=\"_blank\">https://wandb.ai/mohammad-fleity-lebanese-university/DL-NIDS-2--cic-ton-iot/runs/ngxnfpep</a>"},"metadata":{}},{"name":"stdout","text":"Sample counts - Train: 100000, Val: 20000, Test: 10000\nSample counts - Train: 100000, Val: 20000, Test: 10000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35d80112b2ac454ab052f0891867d5cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}